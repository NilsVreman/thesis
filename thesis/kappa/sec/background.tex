\chapter{Background}%
\label{ch:background}%

This chapter presents the necessary background and motivation for the remainder of the thesis.
We divide the chapter in two primary parts.
First, a discussion on the real-time theoretical aspects is provided.
An extended introduction to how real-time operating systems operates is presented, e.g., processor sharing, task states, scheduling strategies, etc.
However, the main focus is dedicated to the most commonly used task models and their respective advantages and disadvantages, with respect to deadline overruns.
Additionally, we provide a brief discussion on state-machine applicability to the aforementioned task models. 
Next, the relevant control theoretical background is presented based on the theory of real-time systems.
Two different system modelling approaches are introduced: switching systems and Markov jump linear systems.
Both models are particularly relevant for real-time systems where the control task can overrun its deadlines.
Specifically for these systems, we present and discuss different stability and performance analyses.

\section{Real-Time Systems}%
\label{sec:background:rts}%
%
% A short extension to the RTS (and RTOS) precise objective
We begin with an introduction to real-time system fundamentals. 
The breadth of the topic prevents a comprehensive review of the existing literature to fit within the scope of this thesis.
In fact, real-time systems are all information processing systems which reacts to external input within a predetermined deadline. 
This includes sensors, actuators, process control, machine vision, robotics, and health care systems, to acknowledge a fraction of all real-time systems.
Instead, we focus the attention to the elements which impact real-time control systems the most, i.e., CPU provisioning, memory management, periodic tasks, task models, scheduling policies, and execution models.
Since the RTOS is tightly interconnected with the hardware, it is natural to illustrate them jointly.
Next, we describe the underlying hardware and real-time architecture seen in Figure~\ref{fig:operating-system-abstraction}.
%
\begin{figure}[t]
    \centering
    \input{\figdir/operating-system-abstraction}%
    \caption{\fix{Need to arrange figure so it makes more sense.}}%
    \label{fig:operating-system-abstraction}%
\end{figure}

% CPU and cores
The \emph{central processing unit} (CPU, or simply \emph{processor}) is the electronic component responsible for executing the task functions.
Each function (or program) is translated into a list of instructions to be executed on the CPU.
These instructions belong to the machine's language used to tell the processor what type of operation to execute, e.g., load a specific memory registry or execute an arithmetic operation.
The time it takes for the CPU to execute one instruction, i.e., fetching the instruction from memory before decoding and executing it, is typically called an \emph{instruction cycle} (or simply \emph{cycle}); this is the basic unit used to measure CPU speed.
To execute the program instructions, the processor can contain one or more \emph{cores}, respectively denoting the processor as \emph{single-core} or \emph{multi-core}.
Each core is able to execute a list of program instructions.
Hence, the advantage of using multi-core processors (compared to single-core processors) is the increased number of instructions that can be executed in parallel. 
However, this gain comes at the cost of an elevated system complexity where the memory and application layout has to be adapted to the multi-core architecture~\cite{Brandenburg:2011}.

% Memory
Integrated with the processor is a \emph{cache} memory, i.e., a small but fast memory that is easy to access from the operational cores.
The cache memory stores recently accessed instructions and data to reduce the latency induced by fetching from \emph{main memory}, i.e., the main hardware storage.
Most modern CPUs have a layered cache memory hierarchy, where the smallest and fastest layer is denoted L1, the second smallest and fastest is denoted L2, and so on.
When the processor needs to access some data, it first examines whether the data exists in the cache and in that case fetch it from there; otherwise, it collects the data from the main memory.

If a task wants to access cached data (or instructions) that cannot be found, it is said to experience a \emph{cache miss}; similarly, a \emph{cache hit} occurs when the sought data is found in the cache.
Cache misses can arise if:
%
\begin{itemize}
    \item the size of the requested data is too large to fetch;
    \item the requested data is not yet loaded into the cache; or
    \item the data have been evicted from the cache, e.g., to make room for more recently retrieved data or from the cache being flushed due to security reasons.
\end{itemize}
%
Ideally, the number of cache misses that a task experiences are kept to a minimum, in particular since fetching data from the main memory can incur large timing overheads on the task execution.
Additionally, the longer a task executes, the less likely it is to contract cache misses.
Intuitively, the task will experience a few initial cache misses when the data is loaded into the cache, but thereafter the cache will be occupied by relevant data and the cache misses should decrease.
This is also known as \emph{cache warming}.
If the task continues to experience significant cache misses even after the cache warming phase, it is said to be \emph{thrashing} the cache, i.e., continuously experiencing cache misses.
Thrashing can severely impact both real-time performance, energy consumption, and even collapse the execution~\cite{Wadleigh:2000}.
In multi-core setups where different cores share a level-X cache, thrashing is a big concern; however, there exists strategies to mitigate frequent interference from different tasks sharing the same cache~\cite{Brandenburg:2011}.
To help mitigate cache eviction (both in single- and multi-core setups), \emph{cache partitioning} is typically employed.
Cache partitioning reserves specific memory addresses for specific tasks, whilst reserving others for shared data.
Thus, cache evictions are limited to the specific cache memory regions that are shared among multiple tasks; unless, the cache is flushed due to security reasons.

% GPIO 
To interface with the external environment, the hardware uses \emph{input/output peripherals} (I/O peripherals).
The peripherals are all external components connected to the hardware, e.g., sensors, actuators, or routers.
Depending on the hardware, the peripherals can either be connected to the circuit board responsible for joining the different components together or directly into the CPU.
If the link to the external environment is wireless, the I/O peripherals are not necessarily sensors or actuators, but rather radio antennas, Bluetooth transmitters, or Wi-Fi routers (depending on the wireless communication protocol) interacting with the sensors and actuators.
Typically, each peripheral is assigned to an \emph{I/O port} in the device, i.e., a unique number to know which physical pin to transmit and receive data through. 

% Microcontrollers and embedded systems
\nv{Not sure if this paragraph should be higher up (in connection to the I/O), or not?}
Although this thesis does not discern different hardware architectures from one another, it is appropriate to talk about \emph{microcontrollers} and \emph{embedded systems}, in particular because of their growing recognition. 
Despite being two different hardware architectures, the terms embedded system and microcontroller will carelessly be used interchangeably due to their natural similarities.
As introduced in Chapter~\ref{ch:intro}, microcontrollers (MCUs) are small computers with integrated processors, memory, and I/O peripherals.
Most embedded systems are based on microcontroller architectures, however, some embedded systems are based on one or more microprocessors with external memory and I/O peripherals.
Hence, microcontrollers are embedded systems and can be used to develop more complex embedded systems, but an embedded system is not necessarily a microcontroller.
\nv{Not sure if this paragraph should be higher up (in connection to the I/O), or not?}


Separately from the I/O port, the \emph{communication protocol} defines the rules used to pack and unpack the packets sent between the peripherals and the hardware over the \emph{communication channel}.
As an analogy, consider a postcard being sent between England and France; the port is where we choose to send the letter, i.e., the address to send it to and the postage stamp, while the protocol is the content of the message, i.e., the chosen communication format.
Communication protocols and their implementation details belong to a vast research topic which falls outside the scope of this thesis.
However, it is an important component of real-time networked control systems and is thus briefly introduced here.

% More into depth about the communication channel and what problems we might encounter here
Information transmitted over a network (wired or wireless) is generally represented as a set of bits (ones and zeroes) to be read in series or parallel; without loss of generality, we will only mention the serial case.
The communication protocol defines the rules determining how the transmitter should encode its data in order for the receiver to decode it using the same set of rules.
The rules are highly dependent on the communication protocol and its application domain.
We illustrate the idea of communication protocols with an example: consider the case where a transmitter wants to send the character \code{R} using the universal asynchronous receiver-transmitter (UART) protocol\footnote{Assuming ASCII encoding of the character, 8 data bits, no parity, and 1 stop bit, i.e., UART 8-N-1.}.
The rules defined by this protocol states that each data packet contains exactly 8 bits of information, is prepended with a start bit ($0$), and is appended with a stop bit ($1$).
Since the binary representation of \code{R} is $01010010$ (using ASCII encoding), the encoded character's packet representation to be sent over the network is then:
%
\begin{equation*}
    \underbracket{0}_{\text{Start bit}} \;\, \underbracket{01010010}_{\text{Data bits}} \;\, \underbracket{1}_{\text{End bit}}.
\end{equation*}
%
Transmitting a message, such as \code{RTS}, thus involve encoding each character individually before transmitting the encoded bit stream in sequence:
%
\begin{equation*}
    0\underbracket{01010010}_{\code{R}}1 \;\;
    0\underbracket{01010100}_{\code{T}}1 \;\;
    0\underbracket{01010011}_{\code{S}}1.
\end{equation*}

The network over which the packets are transmitted, typically consist of one or more routers forwarding the packets between different target locations.
To determine where to forward the packet to, the packet includes a \emph{network address}\footnote{The network address is an identifier to help recognise where to froward the packet to. Common examples of network addresses are IP addresses and MAC addresses.} that is read by the router before rerouting the packet according to a routing policy.
Additionally, each router contains a buffer to store incoming packets before processing them.
Processing the packets in the buffer is efficient, but it requires some non-negligible overhead, i.e., reading the network address and deciding where to forward the packet.
Thus, under normal conditions the receiver will experience packet latency and jitter, but if the network traffic is heavy the buffer space can quickly be exhausted.
In other words, if the packets arrive faster than what the router can process it becomes congested.

Multiple policies have been developed to control the congestion, e.g., tail drop~\addref{} and \nv{insert something}~\addref{}.
Common among all the congestion control strategies is that they employ intentional \emph{packet dropping}, i.e., if a packet arrives while the buffer space is exhausted, the congestion controller will remove either the arriving packet or one in the buffer (depending on the strategy).
In addition to the congestion controller dropping packets, there are intermittent packet drops due to, e.g., packets being misrouted~\addref{}, security threats~\addref{}, software bugs~\addref{}, or wireless communication~\addref{}.

Regardless of the packet drops' origins, they can have dire consequences for real-time control systems.
Losing packets on the network connecting the control hardware and the plant, is the same as loosing sensor measurements or control commands.
Generally, this will degrade the control performance~\addref{}, however, if enough packets are lost it could cause critical system failures or crashes~\addref{}.

% RTOS, threads
A real-time operating system is commonly employed to simplify the interface with the hardware while guaranteeing timeliness.
The RTOS is responsible for orchestrating the tasks' execution and allocating resources (e.g., memory and CPU time) to said tasks.
The terms ``task'', ``process'', and ``thread'' are frequently confused in many documents; to avoid this confusion we provide a definition in the context of real-time operating systems~\addref{}:
%
\begin{itemize}
    \item \emph{Process} -- A process is a computer program with its own stack, control block\footnote{A task's control block (TCB) includes descriptive information about the task, for instance, the identifier used by the scheduler, its priority, and the task's state (running, ready, blocked, suspended, etc.).}, and instruction set.

    \item \emph{Thread} -- A thread is an entity within a process that share the memory context with additional threads.

    \item \emph{Task} -- The term \emph{task} is used analogously with \emph{process}.
\end{itemize}
%
The notational confusion likely arose from the \emph{multithreading}, \emph{multiprocessing}, and \emph{multitasking} paradigms.
In a multiprocessing environment, multiple tasks can execute concurrently, each on a separate core; multithreading is a CPU feature for executing multiple threads concurrently on a single core; and, multitasking is when a single core is executing multiple tasks concurrently.

% Scheduler (how it allocates resources), threads, mechanisms, ISR
The RTOS typically employ a scheduler to orchestrate the tasks and to provide them with the appropriate resources.
The scheduler is responsible for switching tasks in and out, making sure that the correct task context (the task's resources) is brought into scope, handling interrupts, and ensuring fairness among the entities sharing the resources, e.g., interrupts, tasks, and kernel methods.
How the scheduler assigns resources is decided by the \emph{scheduling algorithm}.
Most scheduling algorithms adopt a \emph{preemptive} approach to assigning resources, i.e., the scheduler is run once every time slice (time quanta) to choose which task to switch in.
Classical examples of preemptive scheduling algorithms are:
%
\begin{enumerate*}[label=(\roman*)]
    \item fixed priority preemptive (FPP), where the task with the highest predetermined priority value is executed;
    \item earliest deadline first (EDF), where the task with the shortest time to its corresponding deadline is executed; and
    \item round-robin (RR), where the tasks are switched into scope in a circular order.
\end{enumerate*}
%
Depending on the choice of algorithm, the real-time system's execution pattern may vastly differ.
For instance, two different scheduling strategies applied to one set of real-time tasks, may or may not result in the system being \emph{schedulable}, i.e., all the temporal constraints are satisfied.
If the RTOS tasks are not schedulable there exists tasks overrunning their corresponding deadlines.

In order for the scheduler to know when to stop the currently running task in favour of switching in another, the RTOS clock triggers an \emph{interrupt} at every clock tick.
Interrupts can come from both hardware\footnote{Hardware interrupts come from events changing the state of the system, e.g., external signals triggering that they need attention from the RTOS, watchdog timers triggering an interrupt at set time intervals, or spurious interrupts (electrical anomalies)~\addref{}.} and software, but the RTOS ticks are triggered by the software clock in the RTOS.
Attached to the interrupt is generally an \emph{interrupt service routine} (ISR), i.e., a callback function to execute when the specific interrupt is triggered.
For instance, in the context of the scheduler's tick interrupt, the ISR may be responsible for switching out the currently active task before switching in a new task and its relevant context.
The ISR connected to the RTOS clock tick is also one of the major culprits behind \emph{release jitter} in RTOS, i.e., the time it takes to put the suspended task into the ready queue before picking a new candidate to execute varies between invocations.
Since the RTOS suspends the execution of the active task whenever an interrupt is triggered (even if it happens in the middle of a time slice), it is crucial that the ISR is quick to execute to avoid stalling the processor.
Hence, if the callback function takes too long to execute or if the ISR is triggered too often, it can cause significant time delays in the schedule execution.

Employing a scheduler for single-core processors follow the described paradigm, however, for multi-core systems additional design choices have to be made.
Fundamentally, two different approaches have been taken to scheduling to a multi-core processor: \emph{global} or \emph{partitioned} scheduling.
The former assumes one scheduler responsible for scheduling all the tasks in a global ready queue to the individual cores based on available and required resources.
On the other hand, partitioned scheduling (sometimes referred to as \emph{clustered} scheduling) involves dividing the tasks into partitions that are then mapped to separate cores with individual schedulers.
Despite having additional computational resources to work with, multi-core systems are subject to deadline overruns similarly to how single-core systems are.
Trivially, since partitioned multi-core scheduled systems can be perceived as a set of single-core scheduled systems, they can also experience the same computational overruns that a single-core system can.
For global multi-core schedulers, \nv{I need some help completing this sentence, I got lost in pfair schedulers and got confused}
\fix{Concluding sentence that confirms that this thesis is relevant even for multi-core systems.}


\subsubsection*{\note{Tasks}}%
% Tasks
All the real-time tasks considered in this thesis are \emph{recurrent}, i.e., they do not terminate during system operation.
The recurrent task model simplifies the \emph{a priori} analysis of the real-time workload's effect on the system execution.
A plethora of methods have been derived to model the recurrent task execution, ranging in complexity from the classic Liu and Layland model~\cite{Liu:1973} to directed acyclic graph (DAG) models~\cite{Saifullah:2014}.
Henceforth, the terms \emph{recurrent tasks} and \emph{tasks} will be used analogously.

The task model adopted in this thesis defines a task $\tau$ as a sequence of \emph{jobs} $j_t$, where each job is responsible for executing one full iteration of the task's function.
Here, $t$ counts the number of discrete time steps since the task was created.
Each task $\tau_i$ is characterised by the triplet $(e_i, d_i, p_i)$.
Here, for each job; $e_i > 0$ is the \emph{worst-case execution time} (WCET); $d_i > 0$ is the relative \emph{deadline}; and, $p_i \geq d_i$ is the minimum interarrival \emph{period}.
The RTOS scheduler releases a job $j_t$ at time $a_t$ (the job's \emph{release time}) and the job then completes its execution at time $f_t$ (the job's \emph{completion time}).
A recurrent task is \emph{periodic} if its jobs are released at equidistant time points, i.e., $a_t = t\cdot T$ where $T$ is fixed. 
In particular, control tasks are typically implemented as periodic tasks, hence one job is released in every period.
To make sure that the periodic task's jobs finish their execution before the subsequent job is released, it is common to adopt \emph{implicit deadlines}, i.e., that job $j_t$ completes its execution before the release time of job $j_{t+1}$; formally, it implies that $f_t \leq a_{t+1} = t\cdot T+T = a_t + T$ or simpler $d_i = p_i = T$.

Under ideal computational conditions, each job completes its execution before its corresponding deadline, i.e., $f_t \leq a_t + d_i$.
However, it can happen that the individual job has not yet finished executing when it reaches the end of its allotted time budget.
We then say that the job experiences a \emph{deadline overrun} (also referred to as \emph{deadline miss} or \emph{computational overrun}).
Respectively, if the job completes before its deadline, it \emph{meets} its deadline (experiences a \emph{deadline hit}).
%
\begin{definition}[Deadline Overrun]%
    \label{def:kappa:overrun}%
    The $t$-th job ($j_t$) of a task $\tau_i$ is said to experience a deadline overrun if
    \begin{equation}
        f_t > a_t + d_i.
    \end{equation}
\end{definition}
%
Computational overruns are present in generally all real-time domains from avionics and defence to consumer electronics~\cite{Akesson:2020}, thus highlighting the importance of analysing their impact on the systems' functional correctness.

% task models -> soft, hard, weakly-hard
Every real-time systems behave differently to deadline overruns.
Depending on the application, some systems crash while others experience a degraded efficiency.
Due to this individuality, most real-time systems were historically divided into two classes describing how the systems were affected by computational overruns.
%
\begin{itemize}
    \item \emph{Hard real-time systems} -- It is imperative that the deadlines are met in order to prevent critical system failure.

    \item \emph{Soft real-time systems} -- The perceived quality of the system is degraded with the number of overrun deadlines, but it is unlikely that it will impact system safety.
\end{itemize}
%
\question%
{Should I mention something about soft real-time models here as well (since we use it in Paper 2 and 5)?}%
{}
Despite these classes covering many real-time systems, they do not cover all cases.
Prominent for real-time control systems are the \emph{firm real-time systems}, characterised by being able to overrun a few, but not too many, deadlines before causing critical system failure.

Arguably the most recognised firm model is the \emph{weakly-hard} task model~\cite{Bernat:2001}.
The weakly-hard model was originally devised to provide formal guarantees to tasks that can tolerate occasional deadline overruns, e.g., control tasks where decreasing the sampling time would improve the overall performance whilst introducing intermittent computational overruns.
What defines a weakly-hard task is that the distribution of the deadline hits and misses during a window of $k$ jobs are precisely bounded.
In other words, in addition to the number of overrun deadlines that a task experiences in a window, the sequence in which they appear is also affecting the task execution.
We here compile the definitions of the weakly-hard models:
%
\begin{definition}[Weakly-Hard Task]%
\label{def:kappa:weakly-hard}%
    A weakly-hard task $\tau$ is a task that satisfies (at least) one of the following constraints:
    \begin{enumerate}[label=(\roman*)]
        \item \label{item:AnyHit} $\tau \vdash \anyhit{}$ (\tAH{}): in any window of $k$ consecutive jobs, the minimum number of deadline hits is $x$;
        \item \label{item:RowHit} $\tau \vdash \rowhit{}$ (\tRH{}): in any window of $k$ consecutive jobs, the minimum number of consecutive deadline hits is $x$;
        \item \label{item:AnyMiss} $\tau \vdash \anymiss{}$ (\tAM{}): in any window of $k$ consecutive jobs, the maximum number of deadline misses is $x$; and
        \item \label{item:RowMiss} $\tau \vdash \rowmiss{}$ (\tRM{}): in any window of $k$ consecutive jobs, the maximum number of consecutive deadline misses is $x$;
    \end{enumerate}
    for some values of $x\in \N^\geq$, $k \in \N^>$, where $x\leq k$.
\end{definition}
%
Here, the $\vdash$ symbol is used to indicate that all possible sequences of deadline hits and misses of $\tau$ satisfy the right hand side.

To formalise which sequences of deadline hits and misses that are permitted under a specific weakly-hard constraint an alphabet is introduced.
For historical reasons, the language used to characterise the deadline outcomes of a weakly-hard task is binary, i.e., it consisted solely of two unique character mappings to a deadline hit and a deadline miss\footnote{In \note{Paper~IV} we extend this notation to also encompass more appropriate languages in the real-time control systems setting.}.
Formally, the alphabet of outcomes is denoted $\Sigma = \left\{ 0, 1 \right\}$, where $0$ indicates a job overrunning its corresponding deadline and $1$ represents a job meeting its deadline.
With the use of the \emph{alphabet} and conventional language theoretical notation~\addref{}, a \emph{character} $c_t \in \Sigma$ is defined as the outcome of the $t$-th job.
Similarly, a \emph{word} $w$ is a sequence of $\abs{w}$ characters, i.e., $w = \seq{c_1, c_2, \ldots, c_{\abs{w}}}$.
Hence, a word is representing a sequence of deadline hits and misses.
The set of all all length $N$ words that can be constructed from the alphabet $\Sigma$ is denoted $\Sigma^N$.

Since all of the weakly-hard constraints act on the same language it is natural to ask whether they are relatable to one another, or not.
In~\cite{Bernat:2001}, the authors show that the constraints are in fact comparable using the sets containing all sequences satisfying the specific constraints'.
With a slight abuse of notation we will let $w \vdash \lambda$ represent the case when the word $w$ (outcome sequence) satisfies the weakly-hard constraint $\lambda$.
%
\begin{definition}[Satisfaction Set]
    The \emph{satisfaction set} $\sset{N}{\lambda}$ of an arbitrary weakly-hard constraint $\lambda$, is the set of all length $N \in \N^{>}$ words $w$ satisfying $\lambda$.
    Formally:
    \begin{equation}
        \sset{N}{\lambda} = \left\{ w \mid w \in \Sigma^N, w \vdash \lambda \right\}.
    \end{equation}
\end{definition}
%
To simplify notation, the set of infinite length words satisfying a constraint will be denoted as $\sset{\infty}{\lambda} \equiv \sset{}{\lambda}$.
Using the weakly-hard constraints' satisfaction sets it is then possible to define a partial ordering among the constraints, i.e., relate them to one another based on their difficulty to satisfy.
A weakly-hard constraint is typically \emph{harder to satisfy} if it is more restrictive in what sequences satisfy the constraint.
Consider for instance the constraint $\lambda_1 = \binom{1}{1}$, which requires that every job meets its corresponding deadline.
The constraint is extremely restrictive in what sequences it permits; in fact, the satisfaction set of this constraint only contains one sequence, $\sset{N}{\lambda_1} = \left\{ 1^N \right\}$.
On the other hand, the constraint $\lambda_2 = \binom{0}{1}$ requires no job deadlines to be met to be satisfied; thus, all sequences satisfy this constraint, $\sset{N}{\lambda_2} = \Sigma^N$.
Intuitively, since $\lambda_1$ is more restrictive than $\lambda_2$, we say that $\lambda_1$ \emph{dominates} $\lambda_2$.
We formalise the partial ordering in the following definition:
%
\begin{definition}[Constraint Dominance]%
    \label{def:kappa:dominance}%
    Given two arbitrary weakly-hard constraints $\lambda_1$ and $\lambda_2$, we say that $\lambda_1$ \emph{dominates} $\lambda_2$ (denoted $\lambda_1 \preceq \lambda_2$) if and only if all words satisfying $\lambda_1$ also satisfy $\lambda_2$.
    Formally:
    \begin{equation}
        \lambda_1 \preceq \lambda_2 \Leftrightarrow \sset{}{\lambda_1} \subseteq \sset{}{\lambda_2}
    \end{equation}
\end{definition}
%
Definition~\ref{def:kappa:dominance} confirms that $\lambda_1 = \binom{1}{1}$ dominates $\lambda_2 = \binom{0}{1}$, because $\sset{}{\lambda_1} \subseteq \sset{}{\lambda_2}$.
Many constraint dominance relations have been derived in literature\footnote{In \note{Paper III} we extend the known orderings with two theorems relating the \tAH{} and \tRH{} constraints, thus making it possible to relate \emph{all} the different weakly-hard constraints.}~\cite{Bernat:2001}.
Additionally, the partial ordering motivates the notion of \emph{constraint equivalence}.
Formally
%
\begin{equation}
    \lambda_1 \preceq \lambda_2 \land \lambda_2 \preceq \lambda_1 \Leftrightarrow \lambda_1 \equiv \lambda_2,
\end{equation}
%
where $\land$ is the logical conjunction operator.
The constraint equivalence is also expressible through the satisfaction sets, i.e., $\lambda_1 \equiv \lambda_2 \Leftrightarrow \sset{}{\lambda_1} = \sset{}{\lambda_2}$.

Despite not having gained a lot of traction in the research community, a real-time task can be subjected to \emph{multiple} weakly-hard constraints.
However, the nature of the weakly-hard constraints still require that every constraint is satisfied for a particular sequence.
Since one of the main mathematical advantages of the weakly-hard constraints is that they are fully representable by the closed set that is their respective satisfaction sets; if a task $\tau$ satisfies a set of $N$ constraints $\Lambda = \left\{ \lambda_1, \lambda_2, \ldots, \lambda_N \right\}$, the joint satisfaction set has to be the intersection of all the individual satisfaction sets.
Formally
%
\begin{equation}
    \sset{}{\Lambda} = \bigcap_{i=0}^N \, \sset{}{\lambda_i}
\end{equation}

% Scheduler deadline handling
% NOTE: This should be put after the discussion of the weakly-hard and soft models
\begin{figure}[t]
    \centering
    \input{\figdir/schedule-overrun}
    \caption{\fix{I know this figure sucks, but I wanted an inital placeholder}}
    \label{fig:schedule}
\end{figure}

The chosen task model is important when analysing the execution pattern of the real-time task, however, implementation details, such as the scheduler's functionality, are typically not included in the analysis.
In fact, it has been shown that the implementation's design choices affect both the performance and safety properties of the system~\cite{Cervin:2005}.
This thesis addresses the discrepancy between the real-time models, control theoretical models, and the implementation specifications by including details about the implementation in the real-time control system analysis.
For instance, consider the $t+3$-rd job in Figure~\ref{fig:schedule}; the behaviour of the real-time system is undefined, regardless of the chosen task model.
Furthermore, the function that job $j_{t+3}$ is supposed to carry out remains unfinished, resulting in an unknown behaviour if the overrun deadline is left unmanaged.
It is therefore crucial to include some details about the implementation in the system analysis.

In addition to orchestrating the tasks in the RTOS, the scheduler is also responsible for supervising the tasks overrunning their deadlines (denoted the \emph{overrun handling strategy}).
Different strategies have been developed to handle overruns in varying applications.
In~\cite{Caccamo:2002}, the authors propose a method for avoiding deadline overruns by postponing the deadline of the job that requires more processor time to complete.
An arbiter designed to drop certain jobs upon release (i.e., skipping them) is proposed in~\cite{Yoshimoto:2011}.
However, three of the simplest and most commonly employed overrun handling strategies are~\cite{Cervin:2004b}:
%
\begin{itemize}
    \item \tQ{} -- The naive approach involves letting the job overrunning its deadline to continue its execution whilst queueing up subsequent jobs.
        As soon as the executing job is finished, the first instance in the job queue is immediately released and activated.
        Instead of queuing all subsequent jobs, it is common to only queue the most recent job; this is typically denoted the \tQ{}\code{(1)} strategy.
        However, the \tQ{} strategies risk successive jobs being delayed enough to induce domino effects in the system.

    \item \tS{} -- Under the \tS{} strategy (sometimes referred to as the \code{Skip-Next} or \code{Continue} strategy), the job overrunning its deadline is allowed to continue executing until completion.
        Unlike the \tQ{} strategy, subsequent jobs are skipped (i.e., terminated before release) instead of being put into a job queue.
        Hence, the domino effects that can occur under \tQ{} are avoided; this does however come at the cost of skipping a full job even in the presence of infinitesimal overruns.

    \item \tK{} -- If a job overruns its deadline under the \tK{} strategy (sometimes referred to as the \code{Abort} strategy), the job is immediately terminated allowing the subsequent job to be released and activated on time.
        One of the main advantages with the \tK{} strategy comes from its binary outcome representation, i.e., either the job is completed or it is not.
        This fits well together with, for instance, the weakly-hard task models' language representation.
        On the other hand, one of the drawbacks with \tK{} are that many subsequent job deadlines can be overrun if the job iterations are not independent.
        Additionally, if the task function depend on an internal state, the part of the computation that was completed may need to be rolled back to a previous state via, e.g., memory checkpointing~\addref{}.
        Since such an operation requires additional overhead, it further increases the risk of missing the subsequent job's deadline.
\end{itemize}
%
A framework for switching between \tK{} and \tS{} to drop delayed packets in arbitrated networked control systems is presented in~\cite{Soudbakhsh:2018}.
The authors of~\cite{Pazzaglia:2018} discuss the performance of real-time control systems subject to the \tAM{} constraints with respect to both the \tK{} and \tS{} strategies; additionally, the authors discuss how the overrun handling strategy affect the freshness of the control signal, i.e., the age of the actuated control signal.
In~\cite{Ernst:2019} the authors extend an existing method for computing weakly-hard guarantees in multi-component systems where deadlines outcomes are considered binary events, i.e., adhering to the \tK{} strategy.


\subsection{Execution Modelling using State Machines}%
\label{sec:background:fsm}%
%
% How can we monitor/analyse the execution of real-time systems using graphs/finite state machines
Ever since Liu and Layland proposed their simplistic task model~\cite{Liu:1973}, more expressive models have been sought to properly characterise the execution of real-time tasks.
One of the more prominent methods to capture the task execution's expressiveness involved utilising directed acyclic graphs (DAGs)~\cite{Baruah:2003, Chakraborty:2005, Stigge:2011}.
In fact, both graphs and \emph{finite state machines} (FSM) are frequently used to monitor task execution and verify system safety~\cite{Kumar:2012, Dai:2020, Hertneck:2020}

The use of finite state machines (also referred to as \emph{finite state automata}) have moreover been applied to the research of deadline overrun modelling.
For a task's jobs, the computational overrun process have been modelled using finite state machines for both soft and firm real-time systems.
In~\cite{Horssen:2016} the authors model the \tAM{} weakly-hard constraint using an automaton where the transition events between states are represented by the job outcomes.
A \emph{Markov chain} (MC) model was used in~\cite{Ling:2003} to model the dropout rate of sensor packets in a soft networked control system.
\cite{Kwak:2001} utilise a Markov chain to select optimal time points for memory checkpointing in real-time systems where tasks may experience transient faults.
\nv{Mention UPPAAL}
We will henceforth sloppily refer to finite state machines as \emph{automata}\footnote{To keep notation consistent with \note{Papers III and IV}.}.

% Introduce notation for the FSM
Both automata and Markov chains are constructed from directed graphs, with the main difference that the automata transitions are deterministic in nature whilst the Markov chain requires a distribution of probabilities to model the transitions between states.
In this thesis, both the automata and MC denotes the underlying graph $\GG{} = (\VV{}, \EE{})$, where $\VV{}$ is the set of \emph{vertices} (or \emph{states}) and $\EE{}$ is the set of \emph{edges} (or \emph{transitions}).
However, the vertices and transitions symbolise different deadline overrun models for the automata and Markov chain.

The automaton model is used to represent the feasible sequences of deadline hits and misses for the weakly-hard constraints\footnote{\note{Papers III and IV}.}.
Here, the underlying graph depend on the chosen constraint, i.e., $\GG{\lambda} = (\VV{\lambda}, \EE{\lambda})$.
Each vertex $v_i \in \VV{\lambda}$ represent a word $w_i \in \sset{\abs{w_i}}{\lambda}$.
A transition $e_{i, j} = (v_i, v_j, c_{i,j}) \in \EE{\lambda}$ is a triplet describing the transition condition $c_{i,j} \in \Sigma$ to get from vertex $v_i$ to $v_j$.
In other words, if there exists a permissible sequence of deadline hits and misses $w_i$ which still satisfies the constraint $\lambda$ if followed by a deadline miss (i.e., $c_{i,j} = 0$), there exists a transition $e_{i,j} = (v_i, v_j, 0)$ between $v_i$ and $v_j$.

When modelling stochastic computational overruns\footnote{\note{Papers II and V}.}, the Markov chain models are naturally better-suited than the automaton model.
The MC is represented by a set of states $v_i \in \VV{}$ where the transitions between states is again a triplet $e_{i, j} = (v_i, v_j, p_{i, j}) \in \EE{}$.
Unlike the automata model, the transition here defines a transitional probability $p_{i, j}$ between two states $v_i$ and $v_j$, i.e., a transition from state $v_i$ to state $v_j$ occurs with probability $p_{i, j} \in [0, 1]$.
Trivially, the cumulative probability of leaving any state $v_i$ is $1$:
%
\begin{equation}
    %\sum_{j=0}^{\abs{\VV{}}}\, p_{i, j} = 1.
    \sum_{v_j\in\VV{}}\, p_{i, j} = 1.
\end{equation}
%
Note that $p_{i, i}$ is not necessarily $0$, meaning that with probability $p_{i, i}$ state $v_i$ remains the active state.
A Markov state is said to be \emph{absorbing} if it, once entered, is never left, i.e., has probability $p_{i, i} = 1$.
The transitional probability depends on the specific probability distribution.

\begin{figure}[t]
    \centering
    \input{\figdir/state-machine}
    \caption{\fix{todo}}
    \label{fig:kappa:state-machine}
\end{figure}
%
To demonstrate the similarities and differences between the automaton and Markov chain, an example is shown in Figure~\ref{fig:kappa:state-machine}.
Here, the automaton is constructed from a weakly-hard real-time task subject to the constraint $\anyhit{} = \binom{1}{2}$, i.e., at least one job has to meet its deadline in every window of two consecutive job activations.
Recall that the characters $0$ and $1$ respectively represent an overrun and a met deadline in the automaton's language.
Since the constraint does not tolerate two consecutive deadline overruns, if a deadline overrun occur from vertex $v_2$ the constraint would be violated and the fail-state is entered.
In comparison, the MC represent a soft real-time task where every job has an independent and identically distributed (i.i.d) probability $\rho$ of overrunning its deadline.
Notice that the transitions in both the automaton and in the Markov chain represent the outcome of exactly one job execution.
This is a deterministic transition (no probabilities involved) in the automaton, whilst it is stochastic for the MC.



\section{Control Systems}%
\label{sec:background:ctrl}%
%
\begin{figure}[t]
    \centering
    \input{\figdir/control-structure-abstraction}
    \caption{\fix{todo}}
    \label{fig:control-structure-abstraction}
\end{figure}
%
After abstracting away the real-time system's hardware layer and operating system, the remaining components are the mathematical models of the plant and controllers.
Figure~\ref{fig:control-structure-abstraction} highlights how these models relate to the relevant system architecture.
It is the purpose of computer-controlled systems theory to model and control continuous-time systems via digital components and discrete-time models~\cite{Astrom:1997}.
The mathematical models characterise the system's behaviour in time, typically via a dynamical system of differential (or difference) equations written on \emph{state-space} form:
%
\begin{equation}%
    \label{eq:state-space}%
    \left\{\begin{aligned}
        \dot{x}(t) &= f(x(t),\, u(t),\, t) \\
        y(t) &= g(x(t),\, u(t),\, t) \\
    \end{aligned}\right.
\end{equation}

Practically all real-world plants can be modelled using continuous-time, non-linear state-space equations on the same form as Equation~\eqref{eq:state-space}.
The physical properties of the plant (e.g., velocity, position, rotation, etc.) are collected in the \emph{state vector} $x(t) \in \R^{n_x}$, the exogenous signals affecting the plant are collected in the \emph{input vector} $u(t) \in \R^{n_u}$, and the measurable quantities are collected in the \emph{output vector} $y(t) \in \R^{n_y}$.
The dynamical behaviour of the system is described by the functions $f$ and $g$, which we say are \emph{time-variant} if they explicitly depend on the time variable $t$ and \emph{time-invariant} if they do not, i.e., if $f(x(t),\, u(t),\, t) = f(x(t),\, u(t))$ and $g(x(t),\, u(t),\, t) = g(x(t),\, u(t))$.
A state-space system is linear if the functions $f$ and $g$ can be expressed as linear combinations of their arguments, i.e., a linear state-space system can be written as:
%
\begin{equation}%
    \label{eq:linear-state-space}%
    \left\{\begin{aligned}
        \dot{x}(t) &= A(t)\,x(t) + B(t)\,u(t) \\
        y(t) &= C(t)\,x(t) + D(t)\,u(t) \\
    \end{aligned}\right.
\end{equation}
%
Here, the matrices $A(t) \in \R^{n_x \times n_x}$ and $B(t) \in \R^{n_x \times n_u}$ govern the state evolution of the system while $C(t) \in \R^{n_y \times n_x}$ and $D(t) \in \R^{n_y \times n_u}$ describe the output process.
If none of the matrices are time-dependent, the system is said to be linear time-invariant (LTI).

If the plant is continuous, it is typically \emph{discretised} to simplify the control analysis, synthesis, and implementation.
\nv{CONTINUE HERE}










\nv{mention something about data freshness}

\subsection{Control System Stability}%
\label{sec:background:stability}%
%

\subsection{Control System Performance}%
\label{sec:background:performance}%
%


\nv{%

%\section*{Embedded real-time control Systems}%
%
%\begin{itemize}
%    \item Refer to figure from Chapter~\ref{ch:intro} and then ``zoom'' in on the
%        different aspects treated in the different subsections.
%    \item Simple description of hardware components: Plant, Sensors, Network
%        (wired/wireless), control hardware, actuators.
%    \item Simple description of software components: network protocol, RTOS,
%        tasks, memory, interrupts, etc.
%    \item Maybe create a simple practical example (e.g., taxi of a plane) that
%        can be followed throughout this section.
%\end{itemize}
%
%
%\subsection*{Real-Time Model}%
%%
%\begin{itemize}

%    \item Describe advantages and disadvantages with each of the models
%    \item recall great example of real-time system in rust book
%\end{itemize}
%
%\subsubsection*{Modelling Execution using finite state-machine}%
%%
%\begin{itemize}
%    \item This section should maybe be moved?
%    \item Entire subsubsection requested by KE
%    \item More automata theory
%    \item "typically in control fsm have been used to design highlevel control
%        (e.g., taxi takeoff and landing), in principle (computer science)
%        automata have been used to represent more complicated things (for
%        instance regular languages). This is the basis of what the WeaklyHard.jl
%        does."
%    \item Include markov theory here. "In CS when the transition was
%        non-deterministic, i.e., probabilistic, then you have the concept of
%        Markov chains."
%\end{itemize}


\subsection*{Control System Model}%
%
\begin{itemize}
    \item Start from plant, non-linear model, linearisation
    \item Sensors and actuator models included here.
    \item Control model (non-linear, more common ones), relate to real-time
        tasks
    \item Switching systems! Markov Jump Linear Systems!
\end{itemize}


\subsubsection*{Stability Analysis}%
%
\begin{itemize}
    \item Binary: Stable or not
    \item Stability definitions: nominal, MS, MSS, JSR, Lyapunov
    \item differences (e.g., JSR vs. Lyapunov), applicability
\end{itemize}


\subsubsection*{Performance Analysis}%
%
\begin{itemize}
    \item Gradient: varying degree of performance
    \item Why Performance Analysis?
    \item Metrics
\end{itemize}

}%


% MENTION SOMETHING ABOUT {SYNCHRONOUS, ASYNCHRONOUS, PERIODIC, APERIODIC} TASKS

% MENTION TASK STATES (INTERNAL AND EXTERNAL)
