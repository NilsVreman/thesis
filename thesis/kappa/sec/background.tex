\chapter{Background}%
\label{ch:background}%

This chapter presents the necessary background and motivation for the remainder of the thesis.
We divide the chapter in two primary parts.
First, a discussion on the real-time theoretical aspects is provided.
An extended introduction to how real-time operating systems operates is presented, e.g., processor sharing, task states, scheduling strategies, etc.
However, the main focus is dedicated to the most commonly used task models and their respective advantages and disadvantages, with respect to deadline overruns.
Additionally, we provide a brief discussion on state-machine applicability to the aforementioned task models. 
Next, the relevant control theoretical background is presented based on the theory of real-time systems.
Two different system modelling approaches are introduced: switching systems and Markov jump linear systems.
Both models are particularly relevant for real-time systems where the control task can overrun its deadlines.
Specifically for these systems, we present and discuss different stability and performance analyses.

\section{Real-Time Systems}%
\label{sec:background:rts}%
%
% A short extension to the RTS (and RTOS) precise objective
We begin with an introduction to real-time system fundamentals. 
The breadth of the topic prevents a comprehensive review of the existing literature to fit within the scope of this thesis.
In fact, real-time systems are all information processing systems which reacts to external input within a predetermined deadline. 
This includes sensors, actuators, process control, machine vision, robotics, and health care systems, to acknowledge a fraction of all real-time systems.
Instead, we focus the attention to the elements which impact real-time control systems the most, i.e., RTOS fundamentals, periodic tasks, task models, scheduling policies, and execution models.
\question%
{
    Maybe the ``RTOS fundamentals'' should be ``CPU provisioning'' and ``memory management'' instead?
}{}
Since the RTOS is tightly interconnected with the hardware, it is natural to illustrate them jointly.
In Figure~\ref{fig:operating-system-abstraction}, the underlying hardware and real-time structure is expanded.
%
\begin{figure}[t]
    \centering
    \input{\figdir/operating-system-abstraction}%
    \caption{\fix{Need to arrange figure so it makes more sense.}}%
    \label{fig:operating-system-abstraction}%
\end{figure}

% CPU and cores
The \emph{central processing unit} (CPU, or simply \emph{processor}) is the electronic component responsible for executing the task functions.
Each function (or program) is translated into a list of instructions to be executed on the CPU.
These instructions belong to the machine's language used to tell the processor what type of operation to execute, e.g., load a specific memory registry or execute an arithmetic operation.
The time it takes for the CPU to execute one instruction, i.e., fetching the instruction from memory before decoding and executing it, is typically called an \emph{instruction cycle} (or simply \emph{cycle}); this is the basic unit used to measure CPU speed.
To execute the program instructions, the processor can contain one or more \emph{cores}, respectively denoting the processor as \emph{single-core} or \emph{multi-core}.
Each core is able to execute a list of program instructions.
Hence, the advantage of using multi-core processors (compared to single-core processors) is the increased number of instructions that can be executed in parallel. 
However, this gain comes at the cost of an elevated system complexity where the memory and application layout has to be adapted to the multi-core architecture~\cite{Brandenburg:2011}.

% Memory
Integrated with the processor is a \emph{cache} memory, i.e., a small but fast memory that is easy to access from the operational cores.
The cache memory stores recently accessed instructions and data to reduce the latency induced by fetching from \emph{main memory}, i.e., the main hardware storage.
Most modern CPUs have a layered cache memory hierarchy, where the smallest and fastest layer is denoted L1, the second smallest and fastest is denoted L2, and so on.
When the processor needs to access some data, it first examines whether the data exists in the cache and in that case fetch it from there; otherwise, it collects the data from the main memory.

If a task wants to access cached data (or instructions) that cannot be found, it is said to experience a \emph{cache miss}; similarly, a \emph{cache hit} occurs when the sought data is found in the cache.
Cache misses can arise if:
%
\begin{itemize}
    \item the size of the requested data is too large to fetch;
    \item the requested data is not yet loaded into the cache; or
    \item the data have been evicted from the cache, e.g., to make room for more recently retrieved data or from the cache being flushed due to security reasons.
\end{itemize}
%
Ideally, the number of cache misses that a task experiences are kept to a minimum, in particular since fetching data from the main memory can incur large timing overheads on the task execution.
Additionally, the longer a task executes, the less likely it is to contract cache misses.
Intuitively, the task will experience a few initial cache misses when the data is loaded into the cache, but thereafter the cache will be occupied by relevant data and the cache misses should decrease.
This is also known as \emph{cache warming}.
If the task continues to experience significant cache misses even after the cache warming phase, it is said to be \emph{thrashing} the cache, i.e., continuously experiencing cache misses.
Thrashing can severely impact both real-time performance, energy consumption, and even collapse the execution~\cite{Wadleigh:2000}.
In multi-core setups where different cores share a level-X cache, thrashing is a big concern; however, there exists strategies to mitigate frequent interference from different tasks sharing the same cache~\cite{Brandenburg:2011}.
To help mitigate cache eviction (both in single- and multi-core setups), \emph{cache partitioning} is typically employed.
Cache partitioning reserves specific memory addresses for specific tasks, whilst reserving others for shared data.
Thus, cache evictions are limited to the specific cache memory regions that are shared among multiple tasks; unless, the cache is flushed due to security reasons.

% GPIO 
To interface with the external environment, the hardware uses \emph{input/output peripherals} (I/O peripherals).
The peripherals are all external components connected to the hardware, e.g., sensors, actuators, or routers.
Depending on the hardware, the peripherals can either be connected to the circuit board responsible for joining the different components together or directly into the CPU.
If the link to the external environment is wireless, the I/O peripherals are not necessarily sensors or actuators, but rather radio antennas, Bluetooth transmitters, or Wi-Fi routers (depending on the wireless communication protocol) interacting with the sensors and actuators.
Typically, each peripheral is assigned to an \emph{I/O port} in the device, i.e., a unique number to know which physical pin to transmit and receive data through. 
Depending on the hardware, the port can either be accessed directly from a task or via direct memory access (DMA).
When a task accesses a port, it fully consumes the processor until the data I/O is complete.
Instead, DMA allows the CPU to initialise the data transfer which will progress as a background task until the transfer is finished.
This frees up CPU cycles to be used by other tasks at the cost of complicating the cache architecture.
\question%
{Should the last 4 lines be removed?}
{}

Separately from the I/O port, the \emph{communication protocol} defines the rules used to pack and unpack the packets sent between the peripherals and the hardware over the \emph{communication channel}.
As an analogy, consider a postcard being sent between England and France; the port is where we choose to send the letter, i.e., the address to send it to and the postage stamp, while the protocol is the content of the message, i.e., the chosen communication format.
Communication protocols and their implementation details belong to a vast research topic which falls outside the scope of this thesis.
However, it is an important component of real-time networked control systems and is thus briefly introduced here.

% More into depth about the communication channel and what problems we might encounter here
Information transmitted over a network (wired or wireless) is generally represented as a set of bits (ones and zeroes) to be read in series or parallel; without loss of generality, we will only mention the serial case.
The communication protocol defines the rules determining how the transmitter should encode its data in order for the receiver to decode it using the same set of rules.
The rules are highly dependent on the communication protocol and its application domain.
We illustrate the idea of communication protocols with an example: consider the case where a transmitter wants to send the character \code{R} using the universal asynchronous receiver-transmitter (UART) protocol\footnote{Assuming ASCII encoding of the character, 8 data bits, no parity, and 1 stop bit, i.e., UART 8-N-1.}.
The rules defined by this protocol states that each data packet contains exactly 8 bits of information, is prepended with a start bit ($0$), and is appended with a stop bit ($1$).
Since the binary representation of \code{R} is $01010010$ (using ASCII encoding), the encoded character's packet representation to be sent over the network is then:
%
\begin{equation*}
    \underbracket{0}_{\text{Start bit}} \;\, \underbracket{01010010}_{\text{Data bits}} \;\, \underbracket{1}_{\text{End bit}}.
\end{equation*}
%
Transmitting a message, such as \code{RTS}, thus involve encoding each character individually before transmitting the encoded bit stream in sequence:
%
\begin{equation*}
    0\underbracket{01010010}_{\code{R}}1 \;\;
    0\underbracket{01010100}_{\code{T}}1 \;\;
    0\underbracket{01010011}_{\code{S}}1.
\end{equation*}

The network over which the packets are transmitted, typically consist of one or more routers forwarding the packets between different target locations.
To determine where to forward the packet to, the packet includes a \emph{network address}\footnote{The network address is an identifier to help recognise where to froward the packet to. Common examples of network addresses are IP addresses and MAC addresses.} that is read by the router before rerouting the packet according to a routing policy.
Additionally, each router contains a buffer to store incoming packets before processing them.
Processing the packets in the buffer is efficient, but it requires some non-negligible overhead, i.e., reading the network address and deciding where to forward the packet.
Thus, under normal conditions the receiver will experience packet latency and jitter, but if the network traffic is heavy the buffer space can quickly be exhausted.
In other words, if the packets arrive faster than what the router can process it becomes congested.

Multiple policies have been developed to control the congestion, e.g., tail drop~\addref{} and \nv{insert something}~\addref{}.
Common among all the congestion control strategies is that they employ intentional \emph{packet dropping}, i.e., if a packet arrives while the buffer space is exhausted, the congestion controller will remove either the arriving packet or one in the buffer (depending on the strategy).
In addition to the congestion controller dropping packets, there are intermittent packet drops due to, e.g., packets being misrouted~\addref{}, security threats~\addref{}, software bugs~\addref{}, or wireless communication~\addref{}.
\question%
{Should I mention something about TCP and UDP here?}
{}

Regardless of the packet drops' origins, they can have dire consequences for real-time control systems.
Losing packets on the network connecting the control hardware and the plant, is the same as loosing sensor measurements or control commands.
Generally, this will degrade the control performance~\addref{}, however, if enough packets are lost it could cause critical system failures or crashes~\addref{}.
\question%
{Add something more here or should I discuss it further in the control section?}
{}

% Microcontrollers and embedded systems
\nv{Not sure if this paragraph should be higher up (in connection to the I/O), or not?}
Although this thesis does not discern different hardware architectures from one another, it is appropriate to talk about \emph{microcontrollers} and \emph{embedded systems}, in particular because of their growing recognition. 
Despite being two different hardware architectures, the terms embedded system and microcontroller will carelessly be used interchangeably due to their natural similarities.
As introduced in Chapter~\ref{ch:intro}, microcontrollers (MCUs) are small computers with integrated processors, memory, and I/O peripherals.
On the other hand, most embedded systems are based on microcontroller architectures, however, some embedded systems are based on one or more microprocessors with external memory and I/O peripherals.
Hence, microcontrollers are embedded systems and can be used to develop more complex embedded systems, but an embedded system is not necessarily a microcontroller.

% RTOS, threads


% Tasks (Task stats, and states)

% Scheduler (how it allocates resources), threads, mechanisms, deadline handling, many paragraphs here probably


\nv{Maybe create a figure with different task states?}

\subsection{Execution Modelling using State Machines}%
\label{sec:background:fsm}%
%


\section{Control Systems}%
\label{sec:background:ctrl}%
%

\subsection{Control System Stability}%
\label{sec:background:stability}%
%

\subsection{Control System Performance}%
\label{sec:background:performance}%
%


\nv{%

\section*{Embedded real-time control Systems}%
%
\begin{itemize}
    \item Refer to figure from Chapter~\ref{ch:intro} and then ``zoom'' in on the
        different aspects treated in the different subsections.
    \item Simple description of hardware components: Plant, Sensors, Network
        (wired/wireless), control hardware, actuators.
    \item Simple description of software components: network protocol, RTOS,
        tasks, memory, interrupts, etc.
    \item Maybe create a simple practical example (e.g., taxi of a plane) that
        can be followed throughout this section.
\end{itemize}


\subsection*{Real-Time Model}%
%
\begin{itemize}
    \item Start with a historical perspective
    \item Hard, Soft, Weakly-Hard, More expressive (\cite{Stigge:2011}), other?
    \item Describe advantages and disadvantages with each of the models
    \item recall great example of real-time system in rust book
\end{itemize}

\subsubsection*{Modelling Execution using finite state-machine}%
%
\begin{itemize}
    \item This section should maybe be moved?
    \item Entire subsubsection requested by KE
    \item More automata theory
    \item "typically in control fsm have been used to design highlevel control
        (e.g., taxi takeoff and landing), in principle (computer science)
        automata have been used to represent more complicated things (for
        instance regular languages). This is the basis of what the WeaklyHard.jl
        does."
    \item Include markov theory here. "In CS when the transition was
        non-deterministic, i.e., probabilistic, then you have the concept of
        Markov chains."
\end{itemize}


\subsection*{Control System Model}%
%
\begin{itemize}
    \item Start from plant, non-linear model, linearisation
    \item Sensors and actuator models included here.
    \item Control model (non-linear, more common ones), relate to real-time
        tasks
    \item Switching systems! Markov Jump Linear Systems!
\end{itemize}


\subsubsection*{Stability Analysis}%
%
\begin{itemize}
    \item Binary: Stable or not
    \item Stability definitions: nominal, MS, MSS, JSR, Lyapunov
    \item differences (e.g., JSR vs. Lyapunov), applicability
\end{itemize}


\subsubsection*{Performance Analysis}%
%
\begin{itemize}
    \item Gradient: varying degree of performance
    \item Why Performance Analysis?
    \item Metrics
\end{itemize}

}%


% MENTION SOMETHING ABOUT {SYNCHRONOUS, ASYNCHRONOUS, PERIODIC, APERIODIC} TASKS

% MENTION TASK STATES (INTERNAL AND EXTERNAL)
