\chapter{Background}%
\label{ch:background}%

This chapter presents the necessary background and motivation for the remainder of the thesis.
We divide the chapter in two primary parts.
First, a discussion on the real-time theoretical aspects is provided.
An extended introduction to how real-time operating systems operates is presented, e.g., processor sharing, task states, scheduling strategies, etc.
However, the main focus is dedicated to the most commonly used task models and their respective advantages and disadvantages, with respect to deadline overruns.
Additionally, we provide a brief discussion on state-machine applicability to the aforementioned task models. 
Next, the relevant control theoretical background is presented based on the theory of real-time systems.
Two different system modelling approaches are introduced: switching systems and Markov jump linear systems.
Both models are particularly relevant for real-time systems where the control task can overrun its deadlines.
Specifically for these systems, we present and discuss different stability and performance analyses.

\section{Real-Time Systems}%
\label{sec:background:rts}%
%
% A short extension to the RTS (and RTOS) precise objective
We begin with an introduction to real-time system fundamentals. 
The breadth of the topic prevents a comprehensive review of the existing literature to fit within the scope of this thesis.
In fact, real-time systems are all information processing systems which reacts to external input within a predetermined deadline. 
This includes sensors, actuators, process control, machine vision, robotics, and health care systems, to acknowledge a fraction of all real-time systems.
Instead, we focus the attention to the elements which impact real-time control systems the most, i.e., CPU provisioning, memory management, periodic tasks, task models, scheduling policies, and execution models.
Since the RTOS is tightly interconnected with the hardware, it is natural to illustrate them jointly.
Next, we describe the underlying hardware and real-time architecture seen in Figure~\ref{fig:operating-system-abstraction}.
%
\begin{figure}[t]
    \centering
    \input{\figdir/operating-system-abstraction}%
    \caption{\fix{Need to arrange figure so it makes more sense.}}%
    \label{fig:operating-system-abstraction}%
\end{figure}

% CPU and cores
The \emph{central processing unit} (CPU, or simply \emph{processor}) is the electronic component responsible for executing the task functions.
Each function (or program) is translated into a list of instructions to be executed on the CPU.
These instructions belong to the machine's language used to tell the processor what type of operation to execute, e.g., load a specific memory registry or execute an arithmetic operation.
The time it takes for the CPU to execute one instruction, i.e., fetching the instruction from memory before decoding and executing it, is typically called an \emph{instruction cycle} (or simply \emph{cycle}); this is the basic unit used to measure CPU speed.
To execute the program instructions, the processor can contain one or more \emph{cores}, respectively denoting the processor as \emph{single-core} or \emph{multi-core}.
Each core is able to execute a list of program instructions.
Hence, the advantage of using multi-core processors (compared to single-core processors) is the increased number of instructions that can be executed in parallel. 
However, this gain comes at the cost of an elevated system complexity where the memory and application layout has to be adapted to the multi-core architecture~\cite{Brandenburg:2011}.

% Memory
Integrated with the processor is a \emph{cache} memory, i.e., a small but fast memory that is easy to access from the operational cores.
The cache memory stores recently accessed instructions and data to reduce the latency induced by fetching from \emph{main memory}, i.e., the main hardware storage.
Most modern CPUs have a layered cache memory hierarchy, where the smallest and fastest layer is denoted L1, the second smallest and fastest is denoted L2, and so on.
When the processor needs to access some data, it first examines whether the data exists in the cache and in that case fetch it from there; otherwise, it collects the data from the main memory.

If a task wants to access cached data (or instructions) that cannot be found, it is said to experience a \emph{cache miss}; similarly, a \emph{cache hit} occurs when the sought data is found in the cache.
Cache misses can arise if:
%
\begin{itemize}
    \item the size of the requested data is too large to fetch;
    \item the requested data is not yet loaded into the cache; or
    \item the data have been evicted from the cache, e.g., to make room for more recently retrieved data or from the cache being flushed due to security reasons.
\end{itemize}
%
Ideally, the number of cache misses that a task experiences are kept to a minimum, in particular since fetching data from the main memory can incur large timing overheads on the task execution.
Additionally, the longer a task executes, the less likely it is to contract cache misses.
Intuitively, the task will experience a few initial cache misses when the data is loaded into the cache, but thereafter the cache will be occupied by relevant data and the cache misses should decrease.
This is also known as \emph{cache warming}.
If the task continues to experience significant cache misses even after the cache warming phase, it is said to be \emph{thrashing} the cache, i.e., continuously experiencing cache misses.
Thrashing can severely impact both real-time performance, energy consumption, and even collapse the execution~\cite{Wadleigh:2000}.
In multi-core setups where different cores share a level-X cache, thrashing is a big concern; however, there exists strategies to mitigate frequent interference from different tasks sharing the same cache~\cite{Brandenburg:2011}.
To help mitigate cache eviction (both in single- and multi-core setups), \emph{cache partitioning} is typically employed.
Cache partitioning reserves specific memory addresses for specific tasks, whilst reserving others for shared data.
Thus, cache evictions are limited to the specific cache memory regions that are shared among multiple tasks; unless, the cache is flushed due to security reasons.

% GPIO 
To interface with the external environment, the hardware uses \emph{input/output peripherals} (I/O peripherals).
The peripherals are all external components connected to the hardware, e.g., sensors, actuators, or routers.
Depending on the hardware, the peripherals can either be connected to the circuit board responsible for joining the different components together or directly into the CPU.
If the link to the external environment is wireless, the I/O peripherals are not necessarily sensors or actuators, but rather radio antennas, Bluetooth transmitters, or Wi-Fi routers (depending on the wireless communication protocol) interacting with the sensors and actuators.
Typically, each peripheral is assigned to an \emph{I/O port} in the device, i.e., a unique number to know which physical pin to transmit and receive data through. 

% Microcontrollers and embedded systems
\nv{Not sure if this paragraph should be higher up (in connection to the I/O), or not?}
Although this thesis does not discern different hardware architectures from one another, it is appropriate to talk about \emph{microcontrollers} and \emph{embedded systems}, in particular because of their growing recognition. 
Despite being two different hardware architectures, the terms embedded system and microcontroller will carelessly be used interchangeably due to their natural similarities.
As introduced in Chapter~\ref{ch:intro}, microcontrollers (MCUs) are small computers with integrated processors, memory, and I/O peripherals.
Most embedded systems are based on microcontroller architectures, however, some embedded systems are based on one or more microprocessors with external memory and I/O peripherals.
Hence, microcontrollers are embedded systems and can be used to develop more complex embedded systems, but an embedded system is not necessarily a microcontroller.
\nv{Not sure if this paragraph should be higher up (in connection to the I/O), or not?}


Separately from the I/O port, the \emph{communication protocol} defines the rules used to pack and unpack the packets sent between the peripherals and the hardware over the \emph{communication channel}.
As an analogy, consider a postcard being sent between England and France; the port is where we choose to send the letter, i.e., the address to send it to and the postage stamp, while the protocol is the content of the message, i.e., the chosen communication format.
Communication protocols and their implementation details belong to a vast research topic which falls outside the scope of this thesis.
However, it is an important component of real-time networked control systems and is thus briefly introduced here.

% More into depth about the communication channel and what problems we might encounter here
Information transmitted over a network (wired or wireless) is generally represented as a set of bits (ones and zeroes) to be read in series or parallel; without loss of generality, we will only mention the serial case.
The communication protocol defines the rules determining how the transmitter should encode its data in order for the receiver to decode it using the same set of rules.
The rules are highly dependent on the communication protocol and its application domain.
We illustrate the idea of communication protocols with an example: consider the case where a transmitter wants to send the character \code{R} using the universal asynchronous receiver-transmitter (UART) protocol\footnote{Assuming ASCII encoding of the character, 8 data bits, no parity, and 1 stop bit, i.e., UART 8-N-1.}.
The rules defined by this protocol states that each data packet contains exactly 8 bits of information, is prepended with a start bit ($0$), and is appended with a stop bit ($1$).
Since the binary representation of \code{R} is $01010010$ (using ASCII encoding), the encoded character's packet representation to be sent over the network is then:
%
\begin{equation*}
    \underbracket{0}_{\text{Start bit}} \;\, \underbracket{01010010}_{\text{Data bits}} \;\, \underbracket{1}_{\text{End bit}}.
\end{equation*}
%
Transmitting a message, such as \code{RTS}, thus involve encoding each character individually before transmitting the encoded bit stream in sequence:
%
\begin{equation*}
    0\underbracket{01010010}_{\code{R}}1 \;\;
    0\underbracket{01010100}_{\code{T}}1 \;\;
    0\underbracket{01010011}_{\code{S}}1.
\end{equation*}

The network over which the packets are transmitted, typically consist of one or more routers forwarding the packets between different target locations.
To determine where to forward the packet to, the packet includes a \emph{network address}\footnote{The network address is an identifier to help recognise where to froward the packet to. Common examples of network addresses are IP addresses and MAC addresses.} that is read by the router before rerouting the packet according to a routing policy.
Additionally, each router contains a buffer to store incoming packets before processing them.
Processing the packets in the buffer is efficient, but it requires some non-negligible overhead, i.e., reading the network address and deciding where to forward the packet.
Thus, under normal conditions the receiver will experience packet latency and jitter, but if the network traffic is heavy the buffer space can quickly be exhausted.
In other words, if the packets arrive faster than what the router can process it becomes congested.

Multiple policies have been developed to control the congestion, e.g., tail drop~\addref{} and \nv{insert something}~\addref{}.
Common among all the congestion control strategies is that they employ intentional \emph{packet dropping}, i.e., if a packet arrives while the buffer space is exhausted, the congestion controller will remove either the arriving packet or one in the buffer (depending on the strategy).
In addition to the congestion controller dropping packets, there are intermittent packet drops due to, e.g., packets being misrouted~\addref{}, security threats~\addref{}, software bugs~\addref{}, or wireless communication~\addref{}.

Regardless of the packet drops' origins, they can have dire consequences for real-time control systems.
Losing packets on the network connecting the control hardware and the plant, is the same as loosing sensor measurements or control commands.
Generally, this will degrade the control performance~\addref{}, however, if enough packets are lost it could cause critical system failures or crashes~\addref{}.

% RTOS, threads
A real-time operating system is commonly employed to simplify the interface with the hardware while guaranteeing timeliness.
The RTOS is responsible for orchestrating the tasks' execution and allocating resources (e.g., memory and CPU time) to said tasks.
The terms ``task'', ``process'', and ``thread'' are frequently confused in many documents; to avoid this confusion we provide a definition in the context of real-time operating systems~\addref{}:
%
\begin{itemize}
    \item \emph{Process} -- A process is a computer program with its own stack, control block\footnote{A task's control block (TCB) includes descriptive information about the task, for instance, the identifier used by the scheduler, its priority, and the task's state (running, ready, blocked, suspended, etc.).}, and instruction set.

    \item \emph{Thread} -- A thread is an entity within a process that share the memory context with additional threads.

    \item \emph{Task} -- The term \emph{task} is used analogously with \emph{process}.
\end{itemize}
%
The notational confusion likely arose from the \emph{multithreading}, \emph{multiprocessing}, and \emph{multitasking} paradigms.
In a multiprocessing environment, multiple tasks can execute concurrently, each on a separate core; multithreading is a CPU feature for executing multiple threads concurrently on a single core; and, multitasking is when a single core is executing multiple tasks concurrently.

% Scheduler (how it allocates resources), threads, mechanisms, ISR
The RTOS typically employ a scheduler to orchestrate the tasks and to provide them with the appropriate resources.
The scheduler is responsible for switching tasks in and out, making sure that the correct task context (the task's resources) is brought into scope, handling interrupts, and ensuring fairness among the entities sharing the resources, e.g., interrupts, tasks, and kernel methods.
How the scheduler assigns resources is decided by the \emph{scheduling algorithm}.
Most scheduling algorithms adopt a \emph{preemptive} approach to assigning resources, i.e., the scheduler is run once every time slice (time quanta) to choose which task to switch in.
Classical examples of preemptive scheduling algorithms are:
%
\begin{enumerate*}[label=(\roman*)]
    \item fixed priority preemptive (FPP), where the task with the highest predetermined priority value is executed;
    \item earliest deadline first (EDF), where the task with the shortest time to its corresponding deadline is executed; and
    \item round-robin (RR), where the tasks are switched into scope in a circular order.
\end{enumerate*}
%
Depending on the choice of algorithm, the real-time system's execution pattern may vastly differ.
For instance, two different scheduling strategies applied to one set of real-time tasks, may or may not result in the system being \emph{schedulable}, i.e., all the temporal constraints are satisfied.
If the RTOS tasks are not schedulable there exists tasks overrunning their corresponding deadlines.

In order for the scheduler to know when to stop the currently running task in favour of switching in another, the RTOS clock triggers an \emph{interrupt} at every clock tick.
Interrupts can come from both hardware\footnote{Hardware interrupts come from events changing the state of the system, e.g., external signals triggering that they need attention from the RTOS, watchdog timers triggering an interrupt at set time intervals, or spurious interrupts (electrical anomalies)~\addref{}.} and software, but the RTOS ticks are triggered by the software clock in the RTOS.
Attached to the interrupt is generally an \emph{interrupt service routine} (ISR), i.e., a callback function to execute when the specific interrupt is triggered.
For instance, in the context of the scheduler's tick interrupt, the ISR may be responsible for switching out the currently active task before switching in a new task and its relevant context.
The ISR connected to the RTOS clock tick is also one of the major culprits behind \emph{release jitter} in RTOS, i.e., the time it takes to put the suspended task into the ready queue before picking a new candidate to execute varies between invocations.
Since the RTOS suspends the execution of the active task whenever an interrupt is triggered (even if it happens in the middle of a time slice), it is crucial that the ISR is quick to execute to avoid stalling the processor.
Hence, if the callback function takes too long to execute or if the ISR is triggered too often, it can cause significant time delays in the schedule execution.

\question%
{What should I mention about scheduling for multi-core systems?}%
{}
For single-core systems this is not a trivial problem to solve, but for multi-core processors the complexity is elevated even further. 


\subsubsection*{Tasks}%
\note{Dislike the header; Like the separation}

% Tasks
All the real-time tasks considered in this thesis are \emph{recurrent}, i.e., they do not terminate during system operation.
The recurrent task model simplifies the \emph{a priori} analysis of the real-time workload's effect on the system execution.
A plethora of methods have been derived to model the recurrent task execution, ranging in complexity from the classic Liu and Layland model~\cite{Liu:1973} to directed acyclic graph (DAG) models~\cite{Saifullah:2014}.
Henceforth, the terms \emph{recurrent tasks} and \emph{tasks} will be used analogously.

The task model adopted in this thesis defines a task $\tau$ as a sequence of \emph{jobs} $j_t$, where each job is responsible for executing one full iteration of the task's function.
Here, $t$ counts the number of discrete time steps since the task was created.
Each task $\tau_i$ is characterised by the triplet $(e_i, d_i, p_i)$.
Here, for each job; $e_i > 0$ is the \emph{worst-case execution time} (WCET); $d_i > 0$ is the relative \emph{deadline}; and, $p_i \geq d_i$ is the minimum interarrival \emph{period}.
The RTOS scheduler releases a job $j_t$ at time $a_t$ (the job's \emph{release time}) and the job then completes its execution at time $f_t$ (the job's \emph{completion time}).
A recurrent task is \emph{periodic} if its jobs are released at equidistant time points, i.e., $a_t = t\cdot T$ where $T$ is fixed. 
In particular, control tasks are typically implemented as periodic tasks, hence one job is released in every period.
To make sure that the periodic task's jobs finish their execution before the subsequent job is released, it is common to adopt \emph{implicit deadlines}, i.e., that job $j_t$ completes its execution before the release time of job $j_{t+1}$; formally, it implies that $f_t \leq a_{t+1} = t\cdot T+T = a_t + T$ or simpler $d_i = p_i = T$.

Under ideal computational conditions, each job completes its execution before its corresponding deadline, i.e., $f_t \leq a_t + d_i$.
However, it can happen that the individual job has not yet finished executing when it reaches the end of its allotted time budget.
We then say that the job experiences a \emph{deadline overrun} (also referred to as \emph{deadline miss} or \emph{computational overrun}).
Respectively, if the job completes before its deadline, it \emph{meets} its deadline (experiences a \emph{deadline hit}).
%
\begin{definition}[Deadline Overrun]%
    \label{def:kappa:overrun}%
    The $t$-th job ($j_t$) of a task $\tau_i$ is said to experience a deadline overrun if
    \begin{equation}
        f_t > a_t + d_i.
    \end{equation}
\end{definition}
%
Computational overruns are present in generally all real-time domains from avionics and defence to consumer electronics~\cite{Akesson:2020}, thus highlighting the importance of analysing their impact on the systems' functional correctness.

% task models -> soft, hard, weakly-hard
Every real-time systems behave differently to deadline overruns.
Depending on the application, some systems crash while others experience a degraded efficiency.
Due to this individuality, most real-time systems were historically divided into two classes describing how the systems were affected by computational overruns.
%
\begin{itemize}
    \item \emph{Hard real-time systems} -- It is imperative that the deadlines are met in order to prevent critical system failure.

    \item \emph{Soft real-time systems} -- The perceived quality of the system is degraded with the number of overrun deadlines, but it is unlikely that it will impact system safety.
\end{itemize}
%
Despite these classes covering many real-time systems, they do not cover all cases.
Prominent for real-time control systems are the \emph{firm real-time systems}, characterised by being able to overrun a few, but not too many, deadlines before causing critical system failure.

Arguably the most recognised firm model is the \emph{weakly-hard} task model~\cite{Bernat:2001}.
The weakly-hard model was originally devised to provide formal guarantees to tasks that can tolerate occasional deadline overruns, e.g., control tasks where decreasing the sampling time would improve the overall performance whilst introducing intermittent computational overruns.
What defines a weakly-hard task is that the distribution of the deadline hits and misses during a window of $k$ jobs are precisely bounded.
In other words, in addition to the number of overrun deadlines that a task experiences in a window, the sequence in which they appear is also affecting the task execution.
We here compile the definitions of the weakly-hard models:
%
\begin{definition}[Weakly-Hard Task]%
\label{def:kappa:weakly-hard}%
    A weakly-hard task $\tau$ is a task that satisfies (at least) one of the following constraints:
    \begin{enumerate}[label=(\roman*)]
        \item \label{item:AnyHit} $\tau \vdash \anyhit{}$ (\tAH{}): in any window of $k$ consecutive jobs, the minimum number of deadline hits is $x$;
        \item \label{item:RowHit} $\tau \vdash \rowhit{}$ (\tRH{}): in any window of $k$ consecutive jobs, the minimum number of consecutive deadline hits is $x$;
        \item \label{item:AnyMiss} $\tau \vdash \anymiss{}$ (\tAM{}): in any window of $k$ consecutive jobs, the maximum number of deadline misses is $x$; and
        \item \label{item:RowMiss} $\tau \vdash \rowmiss{}$ (\tRM{}): in any window of $k$ consecutive jobs, the maximum number of consecutive deadline misses is $x$;
    \end{enumerate}
    for some values of $x\in \N^\geq$, $k \in \N^>$, where $x\leq k$.
\end{definition}
%
Here, the $\vdash$ symbol is used to indicate that all possible sequences of deadline hits and misses of $\tau$ satisfy the right hand side.

To formalise which sequences of deadline hits and misses that are permitted under a specific weakly-hard constraint an alphabet is introduced.
For historical reasons, the language used to characterise the deadline outcomes of a weakly-hard task consisted of mapping a deadline hit to one value and a deadline miss to another\footnote{In \note{Paper~IV} we extend this notation to also encompass more appropriate languages in the real-time control systems setting.}.
Formally, the alphabet of outcomes is denoted $\Sigma = \left\{ 0, 1 \right\}$, where $0$ indicates a job overrunning its corresponding deadline and $1$ represents a job meeting its deadline.
With the use of the \emph{alphabet} and conventional language theoretical notation~\addref{}, a \emph{character} $c_t \in \Sigma$ is defined as the outcome of the $t$-th job.
Similarly, a \emph{word} $w$ is a sequence of $\abs{w}$ characters, i.e., $w = \seq{c_1, c_2, \ldots, c_{\abs{w}}}$.
Hence, a word is representing a sequence of deadline hits and misses.
The set of all all length $N$ words that can be constructed from the alphabet $\Sigma$ is denoted $\Sigma^N$.

Since all of the weakly-hard constraints act on the same language it is natural to ask whether they are relatable to one another, or not.
In~\cite{Bernat:2001}, the authors show that the constraints are in fact comparable using the sets containing all sequences satisfying the specific constraints'.
With a slight abuse of notation we will let $w \vdash \lambda$ represent the case when the word (outcome sequence) $w$ satisfies the weakly-hard constraint $\lambda$.
%
\begin{definition}[Satisfaction Set]
    The \emph{satisfaction set} $\sset{N}{\lambda}$ of an arbitrary weakly-hard constraint $\lambda$, is the set of all length $N \in \N^{>}$ words $w$ satisfying $\lambda$.
    Formally:
    \begin{equation}
        \sset{N}{\lambda} = \left\{ w \mid w \in \Sigma^N, w \vdash \lambda \right\}.
    \end{equation}
\end{definition}
%
To simplify notation, the set of infinite length words satisfying a constraint will be denoted as $\sset{\infty}{\lambda} \equiv \sset{}{\lambda}$.
Using the weakly-hard constraints' satisfaction sets it is then possible to define a partial ordering.
%
\begin{definition}[Constraint Dominance]
    Given two arbitrary weakly-hard constraints $\lambda_1$ and $\lambda_2$, we say that $\lambda_1$ \emph{dominates} $\lambda_2$ (denoted $\lambda_1 \preceq \lambda_2$) if and only if all words satisfying $\lambda_1$ also satisfy $\lambda_2$.
    Formally:
    \begin{equation}
        \lambda_1 \preceq \lambda_2 \Leftrightarrow \sset{}{\lambda_1} \subseteq \sset{}{\lambda_2}
    \end{equation}
\end{definition}
%
An arbitrary weakly-hard constraint will be denoted by $\lambda$ and a set of $N$ weakly-hard constraints will be denoted $\Lambda = \left\{ \lambda_1, \lambda_2, \ldots, \lambda_N \right\}$
%
\nv{HERE: Continue with satisfaction set, languages, and dominations.}


The set of $N$ tasks executing in the RTOS is generally denoted the \emph{task set}, i.e., $\taskset = \left\{ \tau_1, \tau_2, \ldots, \tau_N \right\}$.
\nv{Write something more here about orchestration and lead it automatically into the paragraph about deadline handling strategies}

% Scheduler deadline handling
\nv{HERE: Deadline handling strategies}


\subsection{Execution Modelling using State Machines}%
\label{sec:background:fsm}%
%


\section{Control Systems}%
\label{sec:background:ctrl}%
%

\subsection{Control System Stability}%
\label{sec:background:stability}%
%

\subsection{Control System Performance}%
\label{sec:background:performance}%
%


\nv{%

\section*{Embedded real-time control Systems}%
%
\begin{itemize}
    \item Refer to figure from Chapter~\ref{ch:intro} and then ``zoom'' in on the
        different aspects treated in the different subsections.
    \item Simple description of hardware components: Plant, Sensors, Network
        (wired/wireless), control hardware, actuators.
    \item Simple description of software components: network protocol, RTOS,
        tasks, memory, interrupts, etc.
    \item Maybe create a simple practical example (e.g., taxi of a plane) that
        can be followed throughout this section.
\end{itemize}


\subsection*{Real-Time Model}%
%
\begin{itemize}
    \item Start with a historical perspective
    \item Hard, Soft, Weakly-Hard, More expressive (\cite{Stigge:2011}), other?
    \item Describe advantages and disadvantages with each of the models
    \item recall great example of real-time system in rust book
\end{itemize}

\subsubsection*{Modelling Execution using finite state-machine}%
%
\begin{itemize}
    \item This section should maybe be moved?
    \item Entire subsubsection requested by KE
    \item More automata theory
    \item "typically in control fsm have been used to design highlevel control
        (e.g., taxi takeoff and landing), in principle (computer science)
        automata have been used to represent more complicated things (for
        instance regular languages). This is the basis of what the WeaklyHard.jl
        does."
    \item Include markov theory here. "In CS when the transition was
        non-deterministic, i.e., probabilistic, then you have the concept of
        Markov chains."
\end{itemize}


\subsection*{Control System Model}%
%
\begin{itemize}
    \item Start from plant, non-linear model, linearisation
    \item Sensors and actuator models included here.
    \item Control model (non-linear, more common ones), relate to real-time
        tasks
    \item Switching systems! Markov Jump Linear Systems!
\end{itemize}


\subsubsection*{Stability Analysis}%
%
\begin{itemize}
    \item Binary: Stable or not
    \item Stability definitions: nominal, MS, MSS, JSR, Lyapunov
    \item differences (e.g., JSR vs. Lyapunov), applicability
\end{itemize}


\subsubsection*{Performance Analysis}%
%
\begin{itemize}
    \item Gradient: varying degree of performance
    \item Why Performance Analysis?
    \item Metrics
\end{itemize}

}%


% MENTION SOMETHING ABOUT {SYNCHRONOUS, ASYNCHRONOUS, PERIODIC, APERIODIC} TASKS

% MENTION TASK STATES (INTERNAL AND EXTERNAL)
