\chapter{Background}%
\label{ch:background}%

This chapter presents the necessary background and motivation for the remainder of the thesis.
We divide the chapter in two primary parts.
First, a discussion on the real-time theoretical aspects is provided.
An extended introduction to how real-time operating systems operates is presented, e.g., processor sharing, task states, scheduling strategies, etc.
However, the main focus is dedicated to the most commonly used task models and their respective advantages and disadvantages, with respect to deadline overruns.
Additionally, we provide a brief discussion on state-machine applicability to the aforementioned task models. 
Next, the relevant control theoretical background is presented based on the theory of real-time systems.
Two different system modelling approaches are introduced: switching systems and Markov jump linear systems.
Both models are particularly relevant for real-time systems where the control task can overrun its deadlines.
Specifically for these systems, we present and discuss different stability and performance analyses.

\section{Real-Time Systems}%
\label{sec:background:rts}%
%
% A short extension to the RTS (and RTOS) precise objective
We begin with an introduction to real-time system fundamentals. 
The breadth of the topic prevents a comprehensive review of the existing literature to fit within the scope of this thesis.
In fact, real-time systems are all information processing systems which reacts to external input within a predetermined deadline. 
This includes sensors, actuators, process control, machine vision, robotics, and health care systems, to acknowledge a fraction of all real-time systems.
Instead, we focus the attention to the elements which impact real-time control systems the most, i.e., CPU provisioning, memory management, periodic tasks, task models, scheduling policies, and execution models.
Since the RTOS is tightly interconnected with the hardware, it is natural to illustrate them jointly.
Next, we describe the underlying hardware and real-time architecture seen in Figure~\ref{fig:operating-system-abstraction}.
%
\begin{figure}[t]
    \centering
    \input{\figdir/operating-system-abstraction}%
    \caption{A more detailed view of the digital elements of the control system introduced in Figure~\ref{fig:high-level-abstraction}. Processor, memory, and hardware interfaces are represented as well as the scheduler responsible for determining which task(s) that are currently executing on the hardware platform.}%
    \label{fig:operating-system-abstraction}%
\end{figure}

Although this thesis does not discern different hardware architectures from one another, it is appropriate to talk about \emph{microcontrollers} and \emph{embedded systems}, in particular because of their growing recognition. 
Despite being two different hardware architectures, the terms embedded system and microcontroller will carelessly be used interchangeably due to their natural similarities.
As introduced in Chapter~\ref{ch:intro}, microcontrollers (MCUs) are small computers with integrated processors, memory, and I/O peripherals.
Most embedded systems are based on microcontroller architectures, however, some embedded systems are based on one or more microprocessors with external memory and I/O peripherals.
Hence, microcontrollers are embedded systems and can be used to develop more complex embedded systems, but an embedded system is not necessarily a microcontroller.

% CPU and cores
The basis of every hardware architecture is a \emph{central processing unit} (CPU, or simply \emph{processor}).
This is the electronic component responsible for executing the task functions.
Each function (or program) is translated into a list of instructions to be executed on the CPU.
These instructions belong to the machine's language used to tell the processor what type of operation to execute, e.g., load a specific memory register or execute an arithmetic operation.
The time it takes for the CPU to execute one instruction, i.e., fetching the instruction from memory before decoding and executing it, is typically called an \emph{instruction cycle} (or simply \emph{cycle}); this is the basic unit used to measure CPU speed.
To execute the program instructions, the processor can contain one or more \emph{cores}, respectively denoting the processor as \emph{single-core} or \emph{multi-core}.
Each core is able to execute a list of program instructions.
Hence, the advantage of using multi-core processors (compared to single-core processors) is the increased number of instructions that can be executed in parallel. 
However, this gain comes at the cost of an elevated system complexity where the memory and application layout has to be adapted to the multi-core architecture~\cite{Brandenburg:2011}.

% Memory
Integrated with the processor is a \emph{cache} memory, i.e., a small but fast memory that is easy to access from the operational cores.
The cache memory stores recently accessed instructions and data to reduce the latency induced by fetching from \emph{main memory}, i.e., the main hardware storage.
Most modern CPUs have a layered cache memory hierarchy, where the smallest and fastest layer is denoted L1, the second smallest and fastest is denoted L2, and so on.
When the processor needs to access some data, it first examines whether the data exists in the cache and in that case fetch it from there; otherwise, it collects the data from the main memory.

If a task wants to access cached data (or instructions) that cannot be found, it is said to experience a \emph{cache miss}; similarly, a \emph{cache hit} occurs when the sought data is found in the cache.
Cache misses can arise if:
%
\begin{itemize}
    \item the size of the requested data is too large to fetch;
    \item the requested data is not yet loaded into the cache; or
    \item the data has been evicted from the cache, e.g., to make room for more recently retrieved data, or because the cache has been flushed due to security reasons.
\end{itemize}
%
Ideally, the number of cache misses that a task experiences are kept to a minimum, in particular since fetching data from the main memory can incur large timing overheads on the task execution.
Additionally, the longer a task executes, the less likely it is to contract cache misses.
Intuitively, the task will experience a few initial cache misses when the data is loaded into the cache, but thereafter the cache will be occupied by relevant data and the cache misses should decrease.
This is also known as \emph{cache warming}.
If the task continues to experience significant cache misses even after the cache warming phase, it is said to be \emph{thrashing} the cache, i.e., continuously experiencing cache misses.
Thrashing can severely impact both real-time performance, energy consumption, and even collapse the execution~\cite{Wadleigh:2000}.
In multi-core setups where different cores share a layer-X cache, thrashing is a big concern; however, there exists strategies to mitigate frequent interference from different tasks sharing the same cache~\cite{Brandenburg:2011}.
To help mitigate cache eviction (both in single- and multi-core setups), \emph{cache partitioning} is typically employed.
Cache partitioning reserves specific memory addresses for specific tasks, whilst reserving others for shared data.
Thus, cache evictions are limited to the specific cache memory regions that are shared among multiple tasks; unless, the cache is flushed due to security reasons.

% GPIO 
To interface with the external environment, the hardware uses \emph{input/output peripherals} (I/O peripherals).
The peripherals are all external components connected to the hardware, e.g., sensors, actuators, or routers.
Depending on the hardware, the peripherals can either be connected to the circuit board responsible for joining the different components together or directly into the CPU.
If the link to the external environment is wireless, the I/O peripherals are not necessarily sensors or actuators, but rather radio antennas, Bluetooth transmitters, or Wi-Fi routers (depending on the wireless communication protocol) interacting with the sensors and actuators.
Typically, each peripheral is assigned to an \emph{I/O port} in the device, i.e., a unique number to know which physical pin to transmit and receive data through. 

Separately from the I/O port, the \emph{communication protocol} defines the rules used to pack and unpack the packets sent between the peripherals and the hardware over the \emph{communication channel}.
As an analogy, consider a postcard being sent between England and France; the port is where we choose to send the letter, i.e., the address to send it to and the postage stamp, while the protocol is the content of the message, i.e., the chosen communication format.
Communication protocols and their implementation details belong to a vast research topic which falls outside the scope of this thesis.
However, it is an important component of real-time networked control systems and is thus briefly introduced here.

% More into depth about the communication channel and what problems we might encounter here
Information transmitted over a network (wired or wireless) is generally represented as a set of bits (ones and zeroes) to be read in series or parallel; without loss of generality, we will only mention the serial case.
The communication protocol defines the rules determining how the transmitter should encode its data in order for the receiver to decode it using the same set of rules.
The rules are highly dependent on the communication protocol and its application domain.
We illustrate the idea of communication protocols with an example: consider the case where a transmitter wants to send the character \code{R} using the universal asynchronous receiver-transmitter (UART) protocol\footnote{Assuming ASCII encoding of the character, 8 data bits, no parity, and 1 stop bit, i.e., UART 8-N-1.}.
The rules defined by this protocol states that each data packet contains exactly 8 bits of information, is prepended with a start bit ($0$), and is appended with a stop bit ($1$).
Since the binary representation of \code{R} is $01010010$ (using ASCII encoding), the encoded character's packet representation to be sent over the network is then
%
\begin{equation*}
    \underbracket{0}_{\text{Start bit}} \;\, \underbracket{01010010}_{\text{Data bits}} \;\, \underbracket{1}_{\text{End bit}}.
\end{equation*}
%
Transmitting a message, such as \code{RTS}, thus involve encoding each character individually before transmitting the encoded bit stream in sequence,
%
\begin{equation*}
    0\underbracket{01010010}_{\code{R}}1 \;\;
    0\underbracket{01010100}_{\code{T}}1 \;\;
    0\underbracket{01010011}_{\code{S}}1.
\end{equation*}

The network over which the packets are transmitted, typically consist of one or more routers forwarding the packets between different target locations.
To determine where to forward the packet to, the packet includes a \emph{network address}\footnote{The network address is an identifier to help recognise where to forward the packet to. Common examples of network addresses are IP addresses and MAC addresses.} that is read by the router before rerouting the packet according to a routing policy.
Additionally, each router contains a buffer to store incoming packets before processing them.
Processing the packets in the buffer is efficient, but it requires some non-negligible overhead, i.e., reading the network address and deciding where to forward the packet.
Thus, under normal conditions the receiver will experience packet latency and jitter, but if the network traffic is heavy the buffer space can quickly be exhausted.
In other words, if the packets arrive faster than what the router can process it becomes congested.

Multiple policies have been developed to control the congestion, such as tail drop~\cite{Comer:2013}.
Common among all the congestion control strategies is that they employ intentional \emph{packet dropping}, i.e., if a packet arrives while the buffer space is exhausted, the congestion controller will remove either the arriving packet or one in the buffer (depending on the strategy).
In addition to the congestion controller dropping packets, there are intermittent packet drops due to, e.g., packets being misrouted~\cite{Bradley:1998}, security threats~\cite{Hansman:2005}, software bugs~\cite{Mai:2011}, or wireless communication~\cite{Zhu:2021}.

Regardless of the packet drops' origins, they can have dire consequences for real-time control systems.
Losing packets on the network connecting the control hardware and the plant, is the same as losing sensor measurements or control commands.
Generally, this will degrade the control performance~\cite{Nilsson:1998b}, however, if enough packets are lost it could cause critical system failures or crashes~\cite{Xiong:2007}.

% RTOS, threads
A real-time operating system is commonly employed to simplify the interface with the hardware while guaranteeing timeliness.
The RTOS is responsible for orchestrating the tasks' execution and allocating resources (e.g., memory and CPU time) to said tasks.
The terms ``task'', ``process'', and ``thread'' are frequently confused in many documents; to avoid this confusion we provide a definition in the context of real-time operating systems:
%
\begin{itemize}
    \item \emph{Process} -- A process is a computer program with its own stack, control block\footnote{A task's control block (TCB) includes descriptive information about the task, for instance, the identifier used by the scheduler, its priority, and the task's state (running, ready, blocked, suspended, etc.).}, and instruction set.

    \item \emph{Thread} -- A thread is an entity within a process that share the memory context with additional threads.

    \item \emph{Task} -- The term \emph{task} is used analogously with \emph{process}.
\end{itemize}
%
The notational confusion likely arose from the \emph{multithreading}, \emph{multiprocessing}, and \emph{multitasking} paradigms.
In a multiprocessing environment, multiple tasks can execute concurrently, each on a separate core; multithreading is a CPU feature for executing multiple threads concurrently on a single core; and, multitasking is when a single core is executing multiple tasks concurrently.

% Scheduler (how it allocates resources), threads, mechanisms, ISR
The RTOS typically employ a scheduler to orchestrate the tasks and to provide them with the appropriate resources.
The scheduler is responsible for switching tasks in and out, making sure that the correct task context (the task's resources) is brought into scope, handling interrupts, and ensuring fairness among the entities sharing the resources, e.g., interrupts, tasks, and kernel methods.
How the scheduler assigns resources is decided by the \emph{scheduling algorithm}.
Most scheduling algorithms adopt a \emph{preemptive} approach to assigning resources, i.e., the scheduler is run once every time slice (time quanta) to choose which task to switch in.
Classical examples of preemptive scheduling algorithms are:
%
\begin{enumerate*}[label=(\roman*)]
    \item fixed priority preemptive (FPP), where the task with the highest predetermined priority value is executed;
    \item earliest deadline first (EDF), where the task with the shortest time to its corresponding deadline is executed; and
    \item round-robin (RR), where the tasks are switched into scope in a circular order.
\end{enumerate*}
%
Depending on the choice of algorithm, the real-time system's execution pattern may vastly differ.
For instance, two different scheduling strategies applied to one set of real-time tasks, may or may not result in the system being \emph{schedulable}, i.e., all the temporal constraints are satisfied.
If the RTOS tasks are not schedulable there exists tasks overrunning their corresponding deadlines.

In order for the scheduler to know when to stop the currently running task in favour of switching in another, the RTOS clock triggers an \emph{interrupt} at every clock tick.
Interrupts can come from both hardware\footnote{Hardware interrupts come from events changing the state of the system, e.g., external signals triggering that they need attention from the RTOS, watchdog timers triggering an interrupt at set time intervals, or spurious interrupts (electrical anomalies)~\cite{Drepper:2003}.} and software, but the RTOS ticks are triggered by the software clock in the RTOS.
Attached to the interrupt is generally an \emph{interrupt service routine} (ISR), i.e., a callback function to execute when the specific interrupt is triggered.
For instance, in the context of the scheduler's tick interrupt, the ISR may be responsible for switching out the currently active task before switching in a new task and its relevant context.
The ISR connected to the RTOS clock tick is also one of the major culprits behind \emph{release jitter} in RTOS, i.e., the time it takes to put the suspended task into the ready queue before picking a new candidate to execute varies between invocations.
Since the RTOS suspends the execution of the active task whenever an interrupt is triggered (even if it happens in the middle of a time slice), it is crucial that the ISR is quick to execute to avoid stalling the processor.
Hence, if the callback function takes too long to execute or if the ISR is triggered too often, it can cause significant time delays in the schedule execution.

Employing a scheduler for single-core processors follow the described paradigm, however, for multi-core systems additional design choices have to be made.
Fundamentally, two different approaches have been taken to scheduling to a multi-core processor: \emph{global} or \emph{partitioned} scheduling.
The former assumes one scheduler responsible for scheduling all the tasks in a global ready queue to the individual cores based on available and required resources.
On the other hand, partitioned scheduling (sometimes referred to as \emph{clustered} scheduling) involves dividing the tasks into partitions that are then mapped to separate cores with individual schedulers.
Despite having additional computational resources to work with, multi-core systems are subject to deadline overruns similarly to single-core systems, when the capacity is exceeded or due to other events such as deadlocks and locking of shared resources.


\subsubsection*{Tasks}%
% Tasks
All the real-time tasks considered in this thesis are \emph{recurrent}, i.e., they do not terminate during system operation.
The recurrent task model simplifies the \emph{a priori} analysis of the real-time workload's effect on the system execution.
A plethora of methods have been derived to model the recurrent task execution, ranging in complexity from the classic Liu and Layland model~\cite{Liu:1973} to directed acyclic graph (DAG) models~\cite{Saifullah:2014}.
Henceforth, the terms \emph{recurrent tasks} and \emph{tasks} will be used analogously.

The task model adopted in this thesis defines a task $\tau$ as a sequence of \emph{jobs} $j_k$, where each job is responsible for executing one full iteration of the task's function.
Here, $k$ counts the number of discrete time steps since the task was created.
Each task $\tau_i$ is characterised by the triplet $(e_i, d_i, p_i)$.
Here, for each job; $e_i > 0$ is the \emph{worst-case execution time} (WCET); $d_i > 0$ is the relative \emph{deadline}; and, $p_i \geq d_i$ is the minimum interarrival \emph{period}.
The RTOS scheduler releases a job $j_k$ at time $a_k$ (the job's \emph{release time}) and the job then completes its execution at time $f_k$ (the job's \emph{completion time}).
A recurrent task is \emph{periodic} if its jobs are released at equidistant time points, i.e., $a_k = k\cdot \Ts$ where $\Ts$ is fixed. 
In particular, control tasks are typically implemented as periodic tasks, hence one job is released in every period.
To make sure that the periodic task's jobs finish their execution before the subsequent job is released, it is common to adopt \emph{implicit deadlines}, i.e., that job $j_k$ completes its execution before the release time of job $j_{k+1}$; formally, it implies that $f_k \leq a_{k+1} = k\cdot \Ts+\Ts = a_k + \Ts$ or simpler $d_i = p_i = \Ts$.

Under ideal computational conditions, each job completes its execution before its corresponding deadline, i.e., $f_k \leq a_k + d_i$.
However, it can happen that the individual job has not yet finished executing when it reaches the end of its allotted time budget.
We then say that the job experiences a \emph{deadline overrun} (also referred to as \emph{deadline miss} or \emph{computational overrun}).
Respectively, if the job completes before its deadline, it \emph{meets} its deadline (experiences a \emph{deadline hit}).
%
\begin{definition}[Deadline Overrun]%
    \label{def:kappa:overrun}%
    The $k$-th job ($j_k$) of a task $\tau_i$ is said to experience a deadline overrun if
    \begin{equation*}
        f_k > a_k + d_i.
    \end{equation*}
\end{definition}
%
Computational overruns are present in generally all real-time domains from avionics and defence to consumer electronics~\cite{Akesson:2020}, thus highlighting the importance of analysing their impact on the systems' functional correctness.

% task models -> soft, hard, weakly-hard
Every real-time system behave differently in the presence of deadline overruns.
Depending on the application, some systems crash while others experience a degraded efficiency.
Due to this individuality, most real-time systems were historically divided into two classes describing how the systems were affected by computational overruns.
%
\begin{itemize}
    \item \emph{Hard real-time systems} -- It is imperative that the deadlines are met in order to prevent critical system failure.

    \item \emph{Soft real-time systems} -- The perceived quality of the system is degraded with the number of overrun deadlines, but it is unlikely that it will impact system safety.
\end{itemize}
%
Soft real-time systems typically do not crash catastrophically when they experience a finite number of deadline overruns.
To analyse these systems, their deadline overruns are typically modelled using stochastic processes.
For instance, the simplest models assume that jobs' deadline outcomes are independent and identically distributed random variables, i.e., that the outcome of each job depend only on the probability that this specific job overruns its deadline.
Intuitively, this assumption is too simplistic for real systems and instead stochastic models for the task execution were developed based on, for instance, Markov chains~\cite{Liu:2005, Friebe:2022, Abeni:2017, Lincoln:2002} and task chains~\cite{Manolache:2004, Liu:2010}, where the transitions between states are probabilistic.
Despite the hard and soft classes covering many real-time systems, they do not cover all cases.
In particular, embedded systems can be described using the \emph{firm real-time systems} model, characterised by being able to overrun a few, but not too many, deadlines before causing critical system failure.

Arguably the most recognised firm model is the \emph{weakly-hard} task model~\cite{Bernat:2001}.
These models were originally devised to provide formal guarantees to tasks that can tolerate occasional deadline overruns, e.g., control tasks where decreasing the sampling time would improve the overall performance whilst introducing intermittent computational overruns.
What defines a weakly-hard task is that the distribution of deadline hits and misses during a window of $k$ jobs is precisely bounded\footnote{From the context it will always be clear when $k$ is used to respectively denote the window length of a weakly-hard constraint and to count the discrete number of job iterations of a recurrent task.}.
In other words, in addition to the number of overrun deadlines that a task experiences in a window, the sequence in which they appear is also affecting the task execution.
We here compile the definitions of the weakly-hard models:
%
\begin{definition}[Weakly-Hard Task]%
\label{def:kappa:weakly-hard}%
    A weakly-hard task $\tau$ is a task that satisfies (at least) one of the following constraints:
    \begin{enumerate}[label=(\roman*)]
        \item \label{item:AnyHit} $\tau \vdash \anyhit{}$ (\tAH{}): in any window of $k$ consecutive jobs, the minimum number of deadline hits is $x$;
        \item \label{item:RowHit} $\tau \vdash \rowhit{}$ (\tRH{}): in any window of $k$ consecutive jobs, the minimum number of consecutive deadline hits is $x$;
        \item \label{item:AnyMiss} $\tau \vdash \anymiss{}$ (\tAM{}): in any window of $k$ consecutive jobs, the maximum number of deadline misses is $x$; and
        \item \label{item:RowMiss} $\tau \vdash \rowmiss{}$ (\tRM{}): in any window of $k$ consecutive jobs, the maximum number of consecutive deadline misses is $x$;
    \end{enumerate}
    for some values of $x\in \N_{\geq}$, $k \in \N_{>}$, where $x\leq k$.
\end{definition}
%
Here, the $\vdash$ symbol is used to indicate that all possible sequences of deadline hits and misses of $\tau$ satisfy the right hand side.

To formalise which sequences of deadline hits and misses that are permitted under a specific weakly-hard constraint an alphabet is introduced.
For historical reasons, the language used to characterise the deadline outcomes of a weakly-hard task is binary, i.e., it consisted solely of two unique character mappings to a deadline hit and a deadline miss\footnote{In Paper~IV we extend this notation to also encompass more appropriate languages in the real-time control systems setting.}.
Formally, the \emph{alphabet} of outcomes is denoted $\Sigma = \left\{ 0, 1 \right\}$, where $0$ indicates a job overrunning its corresponding deadline and $1$ represents a job meeting its deadline.
With the use of the alphabet and conventional language theoretical notation~\cite{Hopcroft:2006}, a \emph{character} $c_k \in \Sigma$ is defined as the outcome of the $k$-th job.
Similarly, a \emph{word} $w$ is a sequence of $\abs{w}$ characters, i.e., $w = \seq{c_1, c_2, \ldots, c_{\abs{w}}}$.
Hence, a word is representing a sequence of deadline hits and misses.
The set of all all length $N$ words that can be constructed from the alphabet $\Sigma$ is denoted $\Sigma^N$.

Since all of the weakly-hard constraints act on the same language it is natural to ask whether they are relatable to one another, or not.
In~\cite{Bernat:2001}, the authors show that the constraints are in fact comparable using the sets containing all sequences satisfying the specific constraints.
With a slight abuse of notation we will let $w \vdash \lambda$ represent the case when a word $w$ (outcome sequence) satisfies the weakly-hard constraint $\lambda$.
%
\begin{definition}[Satisfaction Set]
    The \emph{satisfaction set} $\sset{N}{\lambda}$ of an arbitrary weakly-hard constraint $\lambda$, is the set of all length $N \in \N_{>}$ words $w$ satisfying $\lambda$.
    Formally,
    \begin{equation*}
        \sset{N}{\lambda} = \left\{ w \mid w \in \Sigma^N,\, w \vdash \lambda \right\}.
    \end{equation*}
\end{definition}
%
To simplify notation, the set of infinite length words satisfying a constraint will be denoted as $\sset{\infty}{\lambda} \equiv \sset{}{\lambda}$.
Using the satisfaction sets it is then possible to define a partial ordering among the constraints, i.e., relate them to one another based on their difficulty to satisfy.
A weakly-hard constraint is \emph{harder to satisfy} if it is more restrictive in which sequences satisfy the constraint.
Consider for instance the constraint $\lambda_1 = \binom{1}{1}$, which requires that every job meets its corresponding deadline.
The constraint is extremely restrictive in what sequences it permits; in fact, the satisfaction set of this constraint only contains one sequence, $\sset{N}{\lambda_1} = \left\{ 1^N \right\}$.
On the other hand, the constraint $\lambda_2 = \binom{0}{1}$ requires no job deadlines to be met to be satisfied; thus, all sequences satisfy this constraint, $\sset{N}{\lambda_2} = \Sigma^N$.
Intuitively, since $\lambda_1$ is more restrictive than $\lambda_2$, we say that $\lambda_1$ \emph{dominates} $\lambda_2$.
We formalise the partial ordering in the following definition:
%
\begin{definition}[Constraint Dominance]%
    \label{def:kappa:dominance}%
    Given two arbitrary weakly-hard constraints $\lambda_1$ and $\lambda_2$, we say that $\lambda_1$ \emph{dominates} $\lambda_2$ (denoted $\lambda_1 \preceq \lambda_2$) if and only if all words satisfying $\lambda_1$ also satisfy $\lambda_2$.
    Formally,
    \begin{equation*}
        \lambda_1 \preceq \lambda_2 \Leftrightarrow \sset{}{\lambda_1} \subseteq \sset{}{\lambda_2}.
    \end{equation*}
\end{definition}
%
Definition~\ref{def:kappa:dominance} confirms that $\lambda_1 = \binom{1}{1}$ dominates $\lambda_2 = \binom{0}{1}$, because $\sset{}{\lambda_1} \subseteq \sset{}{\lambda_2}$.
Many constraint dominance relations have been derived in literature\footnote{In Paper III we extend the known orderings with two theorems relating the \tAH{} and \tRH{} constraints, thus making it possible to relate \emph{all} the different weakly-hard constraints.}~\cite{Bernat:2001}.
Additionally, the partial ordering motivates the notion of \emph{constraint equivalence}
%
\begin{equation*}
    \lambda_1 \preceq \lambda_2 \land \lambda_2 \preceq \lambda_1 \Leftrightarrow \lambda_1 \equiv \lambda_2,
\end{equation*}
%
where $\land$ is the logical conjunction operator.
The constraint equivalence is also expressible through the satisfaction sets, i.e., $\lambda_1 \equiv \lambda_2 \Leftrightarrow \sset{}{\lambda_1} = \sset{}{\lambda_2}$.

Despite not having gained a lot of traction in the research community, a real-time task can be subjected to \emph{multiple} weakly-hard constraints.
However, the nature of the weakly-hard constraints still require that every constraint is satisfied for a particular sequence.
Since one of the main mathematical advantages of the weakly-hard constraints is that they are fully representable by the closed set that is their respective satisfaction sets; if a task $\tau$ satisfies a set of $N$ constraints $\Lambda = \left\{ \lambda_1, \lambda_2, \ldots, \lambda_N \right\}$, the joint satisfaction set has to be the intersection of all the individual satisfaction sets
%
\begin{equation*}
    \sset{}{\Lambda} = \bigcap_{i=0}^N \, \sset{}{\lambda_i}.
\end{equation*}

% Scheduler deadline handling
% NOTE: This should be put after the discussion of the weakly-hard and soft models
\begin{figure}[t]
    \centering
    \input{\figdir/schedule-overrun}
    \caption{Example of the execution trace belonging to a task $\tau$, where four jobs are depicted.
    The $k+3$-rd job reaches its corresponding deadline (the activation of the next job) without having completed its execution.}
    \label{fig:schedule}
\end{figure}

The chosen task model is important when analysing the execution pattern of the real-time task, but implementation details, such as the scheduler's functionality, are typically not included in the analysis.
In fact, it has been shown that the implementation's design choices significantly affect both the performance and safety properties of the system~\cite{Cervin:2005}.
This thesis addresses the discrepancy between the real-time models, control theoretical models, and the implementation specifications by including details about the implementation in the real-time control system analysis.
For instance, consider the $k+3$-rd job in Figure~\ref{fig:schedule}; the behaviour of the real-time system is undefined, regardless of the chosen task model.
The function that job $j_{k+3}$ is supposed to carry out remains unfinished, resulting in an unknown behaviour if the overrun deadline is left unmanaged.
It is therefore crucial to include some details about the implementation in the system analysis.

In addition to orchestrating the tasks in the RTOS, the scheduler is also responsible for supervising the tasks overrunning their deadlines (denoted the \emph{overrun handling strategy}).
Different strategies have been developed to handle overruns in varying applications.
In~\cite{Caccamo:2002}, the authors propose a method for avoiding deadline overruns by postponing the deadline of the job that requires more processor time to complete.
An arbiter designed to drop certain jobs upon release (i.e., skipping them) is proposed in~\cite{Yoshimoto:2011}.
However, as described in~\cite{Cervin:2004b}, three of the simplest and most commonly employed overrun handling strategies are:
%
\begin{itemize}
    \item \tQ{} -- The naive approach involves letting the job overrunning its deadline to continue its execution whilst queueing up subsequent jobs.
        As soon as the executing job is finished, the first instance in the job queue is immediately released and activated.
        Instead of queuing all subsequent jobs, it is common to only queue the most recent job; this is typically denoted the \tQ{}\code{(1)} strategy.
        However, the \tQ{} strategies risk successive jobs being delayed enough to induce domino effects in the system.

    \item \tS{} -- Under the \tS{} strategy (sometimes referred to as the \code{Skip-Next} or \code{Continue} strategy), the job overrunning its deadline is allowed to continue executing until completion.
        Unlike the \tQ{} strategy, subsequent jobs are skipped (i.e., terminated before release) instead of being put into a job queue.
        Hence, the domino effects that can occur under \tQ{} are avoided; this does however come at the cost of skipping a full job even in the presence of infinitesimal overruns.

    \item \tK{} -- If a job overruns its deadline under the \tK{} strategy (sometimes referred to as the \code{Abort} strategy), the job is immediately terminated allowing the subsequent job to be released and activated on time.
        One of the main advantages with the \tK{} strategy comes from its binary outcome representation, i.e., either the job is completed or it is not.
        This fits well together with, for instance, the weakly-hard task models' language representation.
        On the other hand, one of the drawbacks with \tK{} are that many subsequent job deadlines can be overrun if the job iterations are not independent.
        Additionally, if the task function depend on an internal state, the part of the computation that was completed may need to be rolled back to a previous state via, e.g., memory checkpointing~\cite{Vogt:2015}.
        Since such an operation requires additional overhead, it further increases the risk of missing the subsequent job's deadline.
\end{itemize}
%
A framework for switching between \tK{} and \tS{} to drop delayed packets in arbitrated networked control systems is presented in~\cite{Soudbakhsh:2018}.
The authors of~\cite{Pazzaglia:2018} discuss the performance of real-time control systems subject to the \tAM{} constraints with respect to both the \tK{} and \tS{} strategies; additionally, the authors discuss how the overrun handling strategy affect the freshness of the control signal, i.e., the age of the actuated control signal.
In~\cite{Ernst:2019} the authors extend an existing method for computing weakly-hard guarantees in multi-component systems where deadlines outcomes are considered binary events, i.e., adhering to the \tK{} strategy.

Separately from the scheduler's overrun strategy, it is important to analyse what happens when the actuators do not receive a fresh control command due to, for instance, a deadline overrun or a packet loss on the network.
The actuators typically employ an \emph{actuator mode} to decide which command to enact on the plant if no new control command has been received.
The research community has proposed (among many other) smart actuator degradation policies~\cite{Ma:2018} and adaptive compensation schemes~\cite{Xing:2017} for actuators subject to intermittent faults.
However, two of the simplest and most frequently employed policies are the \tZ{} and \tH{} actuator modes~\cite{Schenato:2009}.
If no new control signal is received by the actuator, the \tH{} actuation mode holds the last received control command until a new directive is received.
On the contrary, the \tZ{} actuation mode stops acting on the plant if no new control command is received.\footnote{Paper~I shows that the choice of actuation mode can significantly affect both stability and performance properties of the physical system, when the controller is subject to computational overruns.}
Choosing a proper actuator mode is non-trivial and is in general highly dependent on the plant and controller dynamics~\cite{Schenato:2009}.
For controllers with integrating dynamics, \tH{} is a good choice pressuming that the controller command is keeping the system close to its desired execution point.
On the contrary, the \tZ{} mode may be safer for plants with noisy or unstable dynamics, because avoiding to affect the system can be better than to act on outdated control commands.



\subsection{Execution Modelling using State Machines}%
\label{sec:background:fsm}%
%
% How can we monitor/analyse the execution of real-time systems using graphs/finite state machines
Ever since Liu and Layland proposed their simplistic task model~\cite{Liu:1973}, more expressive models have been sought to properly characterise the execution of real-time tasks.
One of the more prominent methods to capture the task execution's expressiveness involved utilising directed acyclic graphs (DAGs)~\cite{Baruah:2003, Chakraborty:2005, Stigge:2011}.
In fact, both graphs and \emph{finite state machines} (FSM) are frequently used to monitor task execution and verify system safety~\cite{Kumar:2012, Dai:2020, Hertneck:2020}

The use of finite state machines (also referred to as \emph{finite state automata}) have moreover been applied to the research of deadline overrun modelling.
For a task's jobs, the computational overrun process have been modelled using finite state machines for both soft and firm real-time systems.
In~\cite{Horssen:2016} the authors model the \tAM{} weakly-hard constraint using an automaton where the transition events between states are represented by the job outcomes.
A \emph{Markov chain} (MC) model was used in~\cite{Ling:2003} to model the dropout rate of sensor packets in a soft networked control system.
\cite{Kwak:2001} utilise a Markov chain to select optimal time points for memory checkpointing in real-time systems where tasks may experience transient faults.
We will henceforth sloppily refer to finite state machines as \emph{automata}\footnote{To keep notation consistent with Papers III and IV.}.

% Introduce notation for the FSM
Both automata and Markov chains are constructed from directed graphs, with the main difference that the automata transitions are deterministic by nature whilst the Markov chain requires a distribution of probabilities to model the transitions between states.
In this thesis, both the automata and MC denotes the underlying graph $\GG{} = (\VV{}, \EE{})$, where $\VV{}$ is the set of \emph{vertices} (or \emph{states}) and $\EE{}$ is the set of \emph{edges} (or \emph{transitions}).
However, the vertices and transitions symbolise different deadline overrun models for the automata and Markov chain.

The automaton model is used to represent the feasible sequences of deadline hits and misses for the weakly-hard constraints\footnote{The automaton model is derived in Paper III and utilised in Paper IV.}.
Here, the underlying graph depend on the chosen constraint, i.e., $\GG{\lambda} = (\VV{\lambda}, \EE{\lambda})$.
Each vertex $v_i \in \VV{\lambda}$ represent a word $w_i \in \sset{\abs{w_i}}{\lambda}$.
A transition $e_{i, j} = (v_i, v_j, c_{i,j}) \in \EE{\lambda}$ is a triplet describing the transition condition $c_{i,j} \in \Sigma$ to get from vertex $v_i$ to $v_j$.
In other words, if there exists a permissible sequence of deadline hits and misses $w_i$ which still satisfies the constraint $\lambda$ if followed by a deadline miss (i.e., $c_{i,j} = 0$), there exists a transition $e_{i,j} = (v_i, v_j, 0)$ between $v_i$ and $v_j$.

When modelling stochastic computational overruns\footnote{The Markov chain is utilised in Papers~II and V.}, the Markov chain models are naturally better-suited than the automaton model.
The MC is represented by a set of states $v_i \in \VV{}$ where the transitions between states is again a triplet $e_{i, j} = (v_i, v_j, p_{i, j}) \in \EE{}$.
Unlike the automata model, the transition here defines a transitional probability $p_{i, j}$ between two states $v_i$ and $v_j$, i.e., a transition from state $v_i$ to state $v_j$ occurs with probability $p_{i, j} \in [0, 1]$.
Trivially, the cumulative probability of leaving any state $v_i$ is $1$
%
\begin{equation*}
    \sum_{v_j\in\VV{}}\, p_{i, j} = 1.
\end{equation*}
%
Note that $p_{i, i}$ is not necessarily $0$, meaning that with probability $p_{i, i}$ state $v_i$ remains the active state.
A Markov state is said to be \emph{absorbing} if it, once entered, is never left, i.e., has probability $p_{i, i} = 1$.
The transitional probability depends on the specific probability distribution.

\begin{figure}[t]
    \centering
    \input{\figdir/state-machine}
    \caption{Graph representation example of an automaton (left) and Markov chain (right).
    Left: the automaton models the firm real-time constraint that at most one deadline overrun can exist in every window of two consecutive jobs.
    Right: the Markov chain models the soft real-time constraint in which the outcome of each job is an iid process with overrun probability $p$.}
    \label{fig:kappa:state-machine}
\end{figure}
%
To demonstrate the similarities and differences between the automaton and Markov chain, an example is shown in Figure~\ref{fig:kappa:state-machine}.
Here, the automaton is constructed from a weakly-hard real-time task subject to the constraint $\anyhit{} = \binom{1}{2}$, i.e., at least one job has to meet its deadline in every window of two consecutive job activations.
Recall that the characters $0$ and $1$ respectively represent an overrun and a met deadline in the automaton's language.
Since the constraint does not tolerate two consecutive deadline overruns, if a deadline overrun occur from vertex $v_2$ the constraint would be violated and the fail-state is entered.
In comparison, the MC represent a soft real-time task where every job has an independent and identically distributed (i.i.d) probability $p$ of overrunning its deadline.
Notice that the transitions in both the automaton and in the Markov chain represent the outcome of exactly one job execution.
This is a deterministic transition (no probabilities involved) in the automaton, whilst it is stochastic for the MC.



\section{Control Systems}%
\label{sec:background:ctrl}%
%
\begin{figure}[t]
    \centering
    \input{\figdir/control-structure-abstraction}
    \caption{An example of the code snippet executed by the control task together with the mathematical representation of the control algorithm (left) and the mathematical model of the plant dynamics (right).}
    \label{fig:control-structure-abstraction}
\end{figure}
%
By omitting the implementation details, such as the real-time system's hardware layer and operating system, the remaining components are the mathematical models of the plant and controllers.
Figure~\ref{fig:control-structure-abstraction} highlights how these models relate to the relevant system architecture.
It is the purpose of computer-controlled systems theory to model and control continuous-time systems via digital components and discrete-time models~\cite{Astrom:1997}.
The mathematical models characterise the system's behaviour in time, typically via a dynamical system of differential (or difference) equations written on \emph{state-space} form
%
\begin{equation}%
    \label{eq:state-space}%
    \left\{\begin{aligned}
        \dot{x}(t) &= f(x(t),\, u(t),\, t) \\
        y(t) &= g(x(t),\, u(t),\, t). \\
    \end{aligned}\right.
\end{equation}

Practically all real-world plants can be modelled using continuous-time, non-linear state-space equations on the same form as Equation~\eqref{eq:state-space}.
The physical properties of the plant (e.g., velocity, position, rotation, etc.) are collected in the \emph{state vector} $x(t) \in \R^{n_x}$, the exogenous signals affecting the plant are collected in the \emph{input vector} $u(t) \in \R^{n_u}$, and the measurable quantities are collected in the \emph{output vector} $y(t) \in \R^{n_y}$.
The dynamical behaviour of the system is described by the functions $f$ and $g$, which we say are \emph{time-variant} if they explicitly depend on the time variable $k$ and \emph{time-invariant} if they do not, i.e., if $f(x(t),\, u(t),\, t) = f(x(t),\, u(t))$ and $g(x(t),\, u(t),\, t) = g(x(t),\, u(t))$.
A state-space system is linear if the functions $f$ and $g$ can be expressed as linear combinations of their arguments, i.e., a linear state-space system can be written as
%
\begin{equation}%
    \label{eq:linear-state-space}%
    \left\{\begin{aligned}
        \dot{x}(t) &= \Ap_c(t)\,x(t) + \Bp_c(t)\,u(t) \\
        y(t) &= \Cp_c(t)\,x(t) + \Dp_c(t)\,u(t). \\
    \end{aligned}\right.
\end{equation}
%
The matrices $\Ap_c(t) \in \R^{n_x \times n_x}$ and $\Bp_c(t) \in \R^{n_x \times n_u}$ govern the state evolution of the plant while $\Cp_c(t) \in \R^{n_y \times n_x}$ and $\Dp_c(t) \in \R^{n_y \times n_u}$ describe its output process.
The plant is said to be linear time-invariant (LTI) if none of the matrices in Equation~\eqref{eq:linear-state-space} are time-dependent, e.g., $\Ap_c(t) = \Ap_c$.

If the plant is continuous, it is typically \emph{discretised} to simplify the analysis, synthesis, and implementation of the controllers.
Transforming the continuous-time plant model into its discrete-time equivalent is non-trivial and generally depend on how the continuous system is sampled, i.e., how the analog-to-digital converters (ADC) and digital-to-analog converters (DAC) are designed.
The DAC is located near the actuators and translates the digital value received from the controller to an analog actuator command.
Similarly, the ADC transforms the analog sensor values to digital signals to be sent to the controlling hardware.
One of the most commonly applied sampling techniques is the zero-order hold (ZOH) circuit, where the DAC holds the last converted analog signal constant until a new discrete value is received and converted.
The sensors are generally configured to sample the plant at the discrete time instants when the control signal to the plant changes.
In other words, if the DAC receives and converts discrete values at time instants $\left\{\, t_k \,\right\}_k$, the sensors sample the system in the same time points.
For periodically sampled systems, the sampling instants are equidistant with a sampling period of $\Ts$, i.e., $t_k = k\cdot \Ts$.
The resulting discrete-time, LTI model of the plant $\plant$ is then
%
\begin{equation}%
    \label{eq:discrete-lti-plant}%
    \plant \; : \; \left\{\begin{aligned}
        x_{k+1} &= \Ap\,x_k + \Bp\,u_k + \Wp\, w_k\\
        y_k &= \Cp\,x_k + \Dp\,u_k, \\
    \end{aligned}\right.
\end{equation}
%
where the variable subscripts counts the discrete time samples since system startup, i.e., $x_k = x(k\cdot \Ts)$ and $x_{k+1} = x(k\cdot \Ts + \Ts)$.
Note that the process' stochastic disturbance process $w_k \in \R^{n_w}$ and dynamic matrix $\Wp \in \R^{n_x \times n_w}$ have been added.
The disturbance process $w_k$ is typically a stochastic process with (assumed) known statistical properties, modelling the unknown plant dynamics and exogenous signals.

Similarly to how the plant is described using a dynamical system on state-space form, controllers are typically also defined on the same format (denoted the controller's \emph{control law}).
Section~\ref{sec:background:rts} described how the control algorithm is implemented as a function governed by a task executing in the RTOS; modern controllers are hence unlikely to be continuous.
Therefore, the discrete, time-invariant state-space representation of a general control law is
%
\begin{equation}%
    \label{eq:discrete-state-space}%
    \left\{\begin{aligned}
        z_{k+1} &= p(z_k,\, y_k) \\
        u_k &= q(z_k,\, y_k). \\
    \end{aligned}\right.
\end{equation}
%
Here, the \emph{control state vector} $z_k \in \R^{n_z}$ represent the controller's dynamics, the \emph{control signals} $u_k \in \R^{n_u}$ are also the inputs to the plant, and the functions $p$ and $q$ govern the dynamical behaviour of the controller.
Similar to the plant, if $p$ and $q$ can be expressed as linear combinations of their arguments, i.e., $p(z_k,\, y_k) = \Ac\,z_k+\Bc\,y_k$ and $q(z_k,\, y_k) = \Cc\,z_k+\Dc\,y_k$, then the controller $\ctrler$ is linear
%
\begin{equation}%
    \label{eq:discrete-lti-ctrler}%
    \ctrler \; : \; \left\{\begin{aligned}
        z_{k+1} &= \Ac\,z_k + \Bc\,y_k \\
        u_k &= \Cc\,z_k + \Dc\,y_k. \\
    \end{aligned}\right.
\end{equation}
%
We say that a controller $\ctrler$ is \emph{stateless} (or \emph{static}) if it has no dependence on the internal state $z$, i.e., $n_z = 0$.
Furthermore, a \emph{stateful} (or \emph{dynamic}) controller $\ctrler$ is any controller that depend on its internal state $z$, i.e., $n_z > 0$.

To improve timing predictability whilst also reducing input-output jitter one-step delay controllers are frequently employed\footnote{If the controller adopts a one-step delay approach, the order of the last two statements of the control algorithm in Figure~\ref{fig:control-structure-abstraction} has to be swapped, suggesting that the second equation of the system of equations~\eqref{eq:discrete-lti-ctrler} instead should be read as $u_{k+1} = \Cc\,z_k+\Dc\,y_k$. However, the control law's reformulation is typically made implicitly by augmenting the controller's state with the control signal.}.
The one-step delay is generally taken into consideration during the control design process, in other words, the control algorithm is designed for the control signal to be computed during the $t$-th control period and to be set out to the actuators at the release of the $k+1$-st control job.
In the real-time literature, one-step delay are commonly referred to as \emph{logical execution time} (LET) controllers~\cite{Ernst:2018, Gemlau:2021, Kirsch:2012}.
The LET paradigm simplifies the schedulability analysis, improves timing predictability, and removes time-varying computational delays (e.g., release jitter, context switching overhead, and interrupts), but it comes at the cost of introducing a time-delay in the actuation.

The vast treatise on synthesis and analysis of both linear and non-linear control laws that exists is likely too large to review in any thesis; however, some of the most known algorithms are outlined here.
Arguably the most famous control structure is the proportional-integral-derivative (PID) controller, in which the error between the sensor measurements and the desired plant states are used to compute the new control signal~\cite{Astrom:2006}.
The PID controller is linear and stateful, assuming that either the I or D part is included.
From the domain of optimal control, the linear-quadratic regulator (LQR) was derived to analytically minimise a quadratic cost function penalising large control signals and plant state deviations.
The linear and stateless LQR algorithm has been the subject to a large research effort~\cite{Goswami:2012, Linsenmayer:2018}, likely due to its elegant mathematical properties.
Additionally, the LQR is a fundamental part of solving the linear-quadratic-Gaussian (LQG) problem~\cite{Astrom:1997}.
Like LQR, model predictive control (MPC) originate from optimal control~\cite{Allgower:1999}; however, they differ in the cost function's structure.
When LQR optimises the control structure over a full time window, MPC provides a receding time horizon solution while also supporting constraints on the trajectory, control state, and control signal.

Control theory is dedicated to developing models and algorithms (such as PID, LQR, LQG, and MPC) for designing $p$ and $q$ such that the control system's state follows a desired trajectory while minimising undesirable effects such as noise, disturbances, and large actuator commands.
In Sections~\ref{sec:background:stability} and~\ref{sec:background:performance}, the control law's objectives will be expanded upon, particularly discussing \emph{stability}  and \emph{performance}.
However, to discuss stability and performance in real-time control systems, it is crucial to first properly introduce \emph{feedback}.
The sensor signals that are received at the controller are processed by the control algorithm before the computed control signal is transmitted back to the actuators.
Routing signals back into the plant like this is called \emph{feedback control} (or \emph{closed-loop control}).
Typically, the closed-loop system (i.e., all components involved in the closed-loop control) is also described by a dynamical equation, acquired by combining Equations~\eqref{eq:discrete-lti-plant} and~\eqref{eq:discrete-lti-ctrler}
%
\begin{equation}%
    \label{eq:clsys}%
    \tilde{x}_{k+1} = \clmat\, \tilde{x}_k + \clinp_w\, w_k,
\end{equation}
%
where $\tilde{x}_k$ is the closed-loop system's state vector, $\clmat$ is the closed-loop system matrix responsible for encoding the closed-loop system's dynamics, and
%
\begin{equation*}%
    \clinp_w = \begin{bmatrix}
        \Wp \\
        0
    \end{bmatrix}
\end{equation*}
%
is the dynamic matrix of the exogenous stochastic variable $w_k$.

Thus far, the system execution has been assumed to be faultless.
However, we are interested in analysing the real-time control system when the control tasks are subject to computational overruns.
Despite only having system components that are discrete-time, linear, and time-invariant, the closed-loop system dynamics become time-variant when introducing computational faults.
In place of Equation~\eqref{eq:discrete-lti-ctrler}, the controller $\ctrler$ is then characterised by
%
\begin{equation}%
    \label{eq:tv-ctrler}%
    \ctrler \; : \; \left\{\begin{aligned}
        z_{k+1} &= \Ac_{\theta_k}\,z_k + \Bc_{\theta_k}\,y_k \\
        u_k &= \Cc_{\theta_k}\,z_k + \Dc_{\theta_k}\,y_k, \\
    \end{aligned}\right.
\end{equation}
%
where $\theta_k$ is the process governing the overruns, e.g., a weakly-hard sequence or Markov process.
For instance, given a sequence $w$ of job outcomes adhering to the weakly-hard \tRH{} constraint $\genfrac{<}{>}{0pt}{}{2}{5}$, e.g., $w = 110011011$; indexing the $k$-th character in the word $w$ as $c_k$, the outcome process is $\theta_k = c_k$.
The controller dynamics then depend on whether the deadline was met ($1$) or overrun ($0$) as well as the scheduler's overrun policy, i.e., whether \tK{}, \tS{}, \tQ{}, or some other deadline overrun strategy was employed.

The changing control equations do in turn also change the closed-loop dynamics from Equation~\eqref{eq:clsys}
%
\begin{equation}%
    \label{eq:tv-clsys}%
    \tilde{x}_{k+1} = \clmat_{\theta_k}\, \tilde{x}_k + \clinp_w\, w_k.
\end{equation}
%
If $\theta_k$ is governed by a Markov process, then Equation~\eqref{eq:tv-clsys} is denoted a \emph{Markov jump linear system} (MJLS)~\cite{Feng:1992}.
Instead, if the process changing the closed-loop dynamics is deterministic, it is labeled a \emph{switching system}~\cite{Liberzon:2003}.
We acknowledge that this simplified description of both Markov jump linear systems and switching systems is extremely coarse.
In the upcoming sections, we will introduce notation for both MJLS and switching systems such that the stability and performance analyses are unambiguous.



\subsection{Control System Stability}%
\label{sec:background:stability}%
%
One of the fundamental objectives of all control synthesis is to \emph{stabilise} the system.
Stability theory is a broad domain and there exists many both weak and strong definitions of stability.
For dynamical systems (such as control systems), a common notion of stability is that bounded perturbations in the initial conditions of a differential (or difference) equation results in bounded perturbations in the solution\footnote{Note that this implies that the disturbances do not affect the linear system's stability.}.
In other words, if a system is \emph{stable} for a bounded set of initial condition then the solution will not grow unbounded.
Conversely, if a system is \emph{unstable} the solution grows unbounded.
As an example, if the taxiing system was unstable, under certain conditions the wheel velocity would rapidly increase until the wheel engine shut down or broke down (a highly undesirable behaviour).

In practice, these vague notions of stability are formalised in rigorous mathematical definitions.
Two of the classic definitions of stability for \emph{linear} dynamical systems are the Routh-Hurwitz stability criterion for continuous systems~\cite{Astrom:2008} and the Schur stability criterion for discrete systems~\cite{Astrom:1997}.
Very simplified, the criteria could be summarised by:
%
\begin{itemize}
    \item A continuous-time linear system is said to be \emph{Hurwitz stable} if all its characteristic polynomial's roots lie in the open left half-plane.
    \item A discrete-time linear system is said to be \emph{Schur stable} if all its characteristic polynomial's roots lie inside the open unit disk.
\end{itemize}
%
In practice, these conditions directly correspond to verifying that all the system's eigenvalues are either strictly in the left half plane (continuous systems) or strictly inside the unit disc (discrete systems).
The Hurwitz stability criterion for a continuous-time LTI system with system matrix $\clmat^c$ and the Schur stability criterion for a discrete-time LTI system with system matrix $\clmat$ is then
%
\begin{equation*}%
    \begin{matrix*}[l]
        \text{Hurwitz: } & \max{ \left\{ \mathrm{Re}\funof{\eig{i}{\clmat^c}} \right\}} < 0, \\
        \text{Schur: } & \max{ \left\{ \abs{\eig{i}{\clmat}} \right\}} < 1,
    \end{matrix*}
\end{equation*}
%
where $\eig{i}{\clmat}$ retrieves the $i$-th eigenvalue of $\clmat$ and $\mathrm{Re}\funof{\cdot}$ extracts the real-part of a complex number (element-wise when applied to a vector).
It is worth noting that the Schur stability criterion is dependent on the \emph{spectral radius} of the system, i.e., the maximum asymptotic growth rate of the system.
Denoting with $\rho\funof{\clmat}$ the spectral radius of a square matrix $\clmat \in \R^{n \times n}$, it is then defined as\footnote{The second equality is a well-established relation known as Gelfand's formula. The equality holds true regardless of the chosen matrix norm.}
%
\begin{equation}%
    \label{eq:spectral-radius}%
    \rho\funof{\clmat} = \max \left\{ \abs{\eig{1}{\clmat}}, \abs{\eig{2}{\clmat}}, \ldots, \abs{\eig{n}{\clmat}} \right\} = \lim_{k\rightarrow\infty} \norm{\clmat^k}^{\sfrac{1}{k}}.
\end{equation}

\subsubsection*{Deterministic Switching Stability}%
%
The classical notions of linear systems stability do not hold when the closed-loop dynamics is time-variant.
Instead, more powerful mathematical tools are needed, dependent on the definition of the switching process $\theta_k$, i.e., whether the system is subject to \emph{deterministic} or \emph{stochastic} switching.
By deterministic switching we mean systems where the switching process $\theta_k$ is a deterministic worst-case constraint, such as the weakly-hard constraints; in other words, a switching system where stability is independent of the probability of transitioning between different execution modes (vertices in the graph representation).
On the other hand, stochastic switching refer to systems where $\theta_k$ is governed by an entirely stochastic process, e.g., a Markov chain, and the switching stability is hence highly dependent on the transition probabilities.
In this thesis, emphasis has been to analyse switching stability from the deterministic framework, using methods introduced next.

% Lyapunov stability
A useful tool for determining stability of most dynamical systems is \emph{Lyapunov's second method} -- more frequently referred to as \emph{Lyapunov's stability criterion}.
Essentially, the stability criterion is based on finding a function with particular properties (denoted the \emph{Lyapunov function}) for the system under consideration; if a Lyapunov function exists, the system is globally asymptotically stable~\cite{Astrom:1997}.
The Lyapunov function is an energy function which is zero at the unique point of equilibrium, positive everywhere else, and decreases along all trajectories of the system.
For instance, consider an oscillating pendulum; it will always move toward the position with the lowest energy, finally reaching its resting state when it is hanging freely downwards, i.e., where the potential energy of the pendulum has reached its equilibrium.
Unfortunately, finding a suitable Lyapunov function is generally a very complex problem -- in particular when the system experiences switching dynamics due to overruns.

% Switching stability
Fortunately, the \emph{joint spectral radius} (JSR)~\cite{Rota:1960} has in recent years become the subject of intense research due to its role in (among others) discrete-time switching systems stability analysis.
From its name, it might be clear that the joint spectral radius is a generalisation of the spectral radius from Equation~\eqref{eq:spectral-radius}.
In fact, the JSR generalises the spectral radius to a set of matrices $\Aa = \left\{ \clmat_1, \clmat_2, \ldots, \clmat_N \right\}$ (denoted the \emph{switching set}), i.e., it represents the maximum asymptotic growth rate of the arbitrary switching between matrices in the set.
For the switched dynamical system in Equation~\eqref{eq:tv-clsys}, each matrix $\clmat_i \in \Aa$ is a unique matrix describing one switching mode of $\theta_k$ and the JSR is then the maximal asymptotic growth rate of the closed-loop states $\tilde{x}_k$ under arbitrary switching of the matrices $\clmat_i \in \Aa$.
Trivially, if the switching set only contains one matrix, i.e., $\Aa = \left\{ \clmat \right\}$, then the joint spectral radius and the spectral radius are equivalent.

To simplify notation, we borrow the following definition from~\cite{Jungers:2009}
%
\begin{equation*}
    \Aa^k \triangleq \left\{ \clmat_1 \cdots \clmat_k \,\mid\, \clmat_i \in \Aa \right\}.
\end{equation*}
%
Thus, $\Aa^k$ is the set of \emph{all} matrix multiplications involving $k$ matrices from the set $\Aa$.
Using the introduced notation, the following definition provides a formal definition of the JSR~\cite{Jungers:2009}.
%
\begin{definition}[Joint Spectral Radius]%
    \label{def:jsr}%
    Given a bounded set of matrices $\Aa$, the \emph{joint spectral radius} (JSR) of $\Aa$ is defined as
    \begin{equation*}
        \rho\funof{\Aa} = \underset{k\rightarrow\infty}{\lim{}\sup{}} \left\{ \norm{\clmat}^{\sfrac{1}{k}} \,\mid\, \clmat \in \Aa^k \right\}.
    \end{equation*}
\end{definition}

%Intuitively, since an LTI system is Schur stable if the spectral radius is less than one, it is easy to argue that a switching system is stable if the joint spectral radius is less than. \mm{Is it really so intuitive and easy to argue?}
Since $\rho\funof{\Aa}$ correspond to the maximum asymptotic growth rate of the arbitrary switching between matrices in the set $\Aa$, if the worst-case sequence of matrices $\overbar{\clmat} \in \Aa^k$ has a bounded growth rate $\norm{\overbar{\clmat}} < 1$, it implies that the switching system $\tilde{x}_k = \overbar{\clmat}\cdot \tilde{x}_0$ is bounded by the triangle inequality, i.e., $\abs{\tilde{x}_k} \leq \norm{\overbar{\clmat}}\abs{\tilde{x}_0} \rightarrow 0$.
Stability of a switched dynamical system via the joint spectral radius is outlined in the following theorem (for more details and a formal proof we refer the interested reader to~\cite{Jungers:2009}).
%
\begin{theorem}[Switching Stability]%
    \label{thm:switching-stability}%
    For any bounded set of matrices $\Aa$, the corresponding switched dynamical system is stable if and only if $\rho\funof{\Aa} < 1$.
\end{theorem}
%
Furthermore, Theorem~\ref{thm:switching-stability} raises an interesting question about the converse case: which is the switching sequence $\clmat_1 \cdots \clmat_k \in \Aa^k$ resulting in $\rho\funof{\Aa} \geq 1$?
To the best of this thesis' author's knowledge, there are currently no results outlining \emph{which} sequence that destabilises the switched dynamical system.

The JSR is a powerful tool since it makes it possible to determine whether a switching system is asymptotically stable ($\rho\funof{\Aa} < 1$) or not ($\rho\funof{\Aa} \geq 1$).
However, the problem of computing the exact JSR value, and thus also testing whether or not the system is stable, is in practice undecidable~\cite{Blondel:2000}.
To overcome this problem, the research community have in the last decade set out to develop and improve approximation methods, bounding the JSR from above and below~\cite{Jungers:2009}.
Multiple different methods have been developed to approximate the JSR, such methods include (but are not limited to): branch and bound~\cite{Gripenberg:1996}, polytope~\cite{Protasov:1996}, ellipsoidal norms~\cite{Blondel:2005, John:2014}, and sum-of-squares (SOS) relaxation methods~\cite{Parrilo:2008, Wang:2021a, Wang:2021b}.

An interesting relation to the Lyapunov stability is the fact that searching for a common ellipsoidal norm is equivalent to finding a \emph{common quadratic Lyapunov function}.
However, in~\cite{Ando:1998} the authors describe a constructive method for generating asymptotically stable switching systems for which no common quadratic Lyapunov function exist.
Instead, the authors of~\cite{Parrilo:2008} prove that if the solution to the SOS problem in Equation~\eqref{eq:sos} provides a pair $(p\funof{x}, \gamma)$ such that
%
\begin{enumerate*}[label=(\roman*)]
    \item the polynomial $p\funof{x}$ is homogeneous and in the interior of the SOS cone and
    \item $\gamma < 1$,
\end{enumerate*}
%
then $p\funof{x}$ is positive definite and decreases along all trajectories, meaning that $p\funof{x}$ is a Lyapunov function for the switching system defined by $\Aa$.
%
\begin{equation}%
    \label{eq:sos}%
    \begin{aligned}
        \rho_{\mathrm{SOS},2d}\funof{\Aa} =& \inf_{p\funof{x} \in \R_{2d}\left[x\right], \gamma}  \gamma \\
        \textrm{s.t.}\quad&
        \left\{\begin{matrix*}[l]
                p\funof{x} &\text{is SOS}\\
                \gamma^{2d} p\funof{x} - p\funof{\clmat_i x} &\text{is SOS},\; \clmat_i \in \Aa.
        \end{matrix*}\right.
    \end{aligned}
\end{equation}
%
Here $\R_{2d} \left[ x \right]$ is the set of homogeneous polynomials of degree $2d$.
Additionally, the authors show how the upper bound on the JSR achieved by the SOS approach in Equation~\eqref{eq:sos} is never weaker than the one obtained with a common quadratic Lyapunov function.

Trivially, since $\rho^{LB}\funof{\Aa} \leq \rho\funof{\Aa} \leq \rho^{UB}\funof{\Aa}$, if an approximation method provides an upper bound $\rho^{UB}\funof{\Aa}$ that is below one for the switched dynamical system, it is confirmed to be stable; conversely, if the lower bound $\rho^{LB}\funof{\Aa}$ is above one, the system is guaranteed unstable.
The active research in this field continues to enhance the bounds as well as algorithmic complexities, thus constantly improving the applicability of the JSR notion.

\subsubsection*{Stochastic Switching Stability}%
%
% MSS, MS
If the overrun process in Equation~\eqref{eq:tv-clsys} is governed by a Markov model, it is possible to utilise well-known results for Markov jump linear systems (MJLS)~\cite{Lincoln:2002, Feng:1992, Nilsson:1998}.
While the switching system stability analysis (e.g., JSR) provides a deterministic stability certificate, i.e., the switching system stability analysis provides guarantees that the dynamical system will be stable under \emph{all} configurations of the different modes, the MJLS stability analysis provides a stochastic certificate.
This certificate does in turn provide probabilistic guarantees that the MJLS will \emph{almost surely}\footnote{In probability theory, the notion of \emph{almost surely} means that an event will happen with probability $1$.} converge to its equilibrium point~\cite{Feng:1992}.

Since MJLS are stochastic by nature, the existing stability analyses typically investigate how the expected value of the system's states evolve in time.
Arguably the most well-known stability analysis methods are \emph{stochastic stability} (SS) and different variants of \emph{mean-square stability} (MSS)~\cite{Feng:1992}.
Paraphrased from~\cite{Costa:2005}, we say that the dynamical system in~\eqref{eq:tv-clsys}, governed by a Markov process $\theta_k$, is mean-square stable if
%
\begin{equation*}%
    \lim_{k\rightarrow \infty} \E{\norm{\tilde{x}_k}^2} = \varepsilon < \infty.
\end{equation*}
%
In practice, testing for mean-square stability implies verifying that the spectral radius of the operator $\Psi$, i.e., the operator defining the Markov jump linear system's covariance, is less than one.
We denote an $N$-state Markov chain's transition probability matrix with $\Pi = \left\{ p_{i,j} \right\} \in \R^{N \times N}$; this matrix specifies the probability of transitioning from the $i$-th to the $j$-th Markov state.
Additionally, $I_{n}$ is defined as the size $n \times n $ identity matrix and $\otimes$ denotes the Kronecker product.
The following theorem then holds~\cite{Costa:2005}.
%
\begin{theorem}[Mean-Square Stability]%
    \label{thm:mss}%
    The dynamical system in Equation~\eqref{eq:tv-clsys}, governed by an $N$-state Markov chain with transition probability matrix $\Pi \in \R^{N \times N}$, is MSS if
    %
    \begin{equation*}%
        \rho\funof{\Psi} < 1,
    \end{equation*}
    %
    where
    %
    \begin{equation*}%
        \Psi = \left( \Pi^\T{} \otimes I_{n^2} \right) \cdot \blkdiag{ \left\{ \clmat_i \otimes \clmat_i \, \mid \, i \in 1, 2, \ldots, N \right\} }.
    \end{equation*}
\end{theorem}
%
In practice, Theorem~\ref{thm:mss} simplifies the MSS testing and implies almost sure stability of the expected dynamics.
Finding the spectral radius of a matrix is intuitively much faster than, for instance, the joint spectral radius (which has already been discussed to be undecidable).
This is one of the advantages with the stochastic certificate received by the MSS in comparison to the deterministic one received by computing the JSR.



\subsection{Control System Performance}%
\label{sec:background:performance}%
%
% Talk about performance on a high level: Why performance analysis? What is its relation to stability analysis?
Stability analysis, such as the one discussed in Section~\ref{sec:background:stability} is undeniably a black and white concept, where the system is either stable or it is not.
A control system can for instance neither be 90\% nor 10\% stable.
This stands in contrast to the notion of \emph{control system performance} which is a more ambivalent and fluctuating concept.
Performance as a metric to be measured exists in many differing domains, however, we will unequivocally use the notion of \emph{performance} in place of \emph{control system performance}, unless otherwise stated.

Evaluating the performance of a control system involves first defining what is important for the application in focus.
As mentioned in Section~\ref{sec:background:ctrl}, the objective of controller synthesis is to design the functions $p$ and $q$ from Equation~\eqref{eq:discrete-state-space} such that some predetermined requirements are met.
Aside from the obvious stability necessity, the requirements typically involve (but are not limited to):
%
\begin{enumerate*}[label=(\roman*)]
    \item accurately following a desired system trajectory,
    \item quick convergence to said trajectory,
    \item efficiently rejecting process disturbances,
    \item minimising the control effort without significantly affecting other objectives, and
    \item estimating or predicting the system state by eliminating inaccuracies and measurement errors.
\end{enumerate*}
%
Ideally, any controller's ultimate goal is to track the desired system trajectory perfectly whilst also rejecting all disturbances and keeping the control effort to a minimum.
Unfortunately, there exists a trade-off between the controller's \emph{robustness} and \emph{tracking performance}, i.e., how well it handles disturbances and how well it handles setpoint changes.
The trade-off expresses itself implicitly as a limit on how efficiently the desired setpoint can be tracked if disturbances and unmodeled dynamics are also to be effectively rejected.
Conclusively, how the different control objectives are valued is dependent on the particular application.

To judge how well the synthesised controller performs, a \emph{performance analysis} is frequently employed.
The analysis method uses a \emph{performance metric} to qualitatively measure the performance of the controller, i.e., the performance metric produces a descriptive value for the specific controller.
Using the performance measure, it is then possible to compare the quality of one controller to another.
The analysis is typically simulation-based, meaning that the performance is evaluated on a virtual replica (or model) of the system.
Intuitively, performance analysis methods provide a coarse estimation of how well the controller will perform when connected to the actual system.
To improve the results, it is thus necessary to run the controller also on the physical hardware.
However, this can become costly in case there are undetected problems with either the control or real-time system design.
The system is therefore first evaluated in extensive simulations and tests~\cite{Mandrioli:2022}.

% Describe our performance analysis
Innumerable different methods for analysing control system performance have been developed.
This thesis facilitates a performance analysis of real-time control systems subject to computational overruns, where the performance is evaluated on the closed-loop system using a stochastic quadratic cost function
%
\begin{equation}%
    \label{eq:cost}%
    J_k = \E{\tilde{x}_k^\T{}\, Q\, \tilde{x}_k}.
\end{equation}
%
Here $J_k$ is the closed-loop state's cost increment in the $k$-th period and $Q$ is a positive semidefinite weight matrix penalising large closed-loop state deviations.
The model is based on well-known theory for linear stochastic systems~\cite{Astrom:1970, Astrom:1997, Cervin:2019}.

We assume that the desired reference point for the closed-loop state $\tilde{x}_k$ is zero and that the disturbance $w_k$ is an independent, discrete-time, zero mean, Gaussian white noise process with covariance $\E{w_k\,w_k^\T{}} = R_1$.
The assumptions may seem overly restrictive, but we emphasise that more elaborate and advanced disturbance processes (and reference trajectories) can be realised by augmenting the closed-loop system with additional states describing the exogenous dynamics.\footnote{In both Paper I and II we augment the system state to model respectively brown noise (integrated white noise) and reference trajectories.}
With these assumptions, together with the initial condition on the closed-loop state's mean value and covariance being respectively $\E{\tilde{x}_0} = m_0$ and $\E{\tilde{x}_0\,\tilde{x}_0^\T{}} = R_0$, it is now possible to describe how to evaluate Equation~\eqref{eq:cost}.

The expression in Equation~\eqref{eq:cost} is difficult to compute in practice.
Instead, it is easier to first transform the cost $J_k$ into an equivalent form before evaluating it.
We define the closed-loop system's mean value and covariance functions as $m_k$ and $P_k$ respectively
%
\begin{equation}%
    \label{eq:cl-def}%
    m_k = \E{\tilde{x}_k}, \quad P_k = \cov{\tilde{x}_k} = \E{\tilde{x}_k\, \tilde{x}_k^\T{}} - \E{\tilde{x}_k}\,\E{\tilde{x}_k}^\T{}.
\end{equation}
%
Note that because $\E{w_k} = 0$, the mean value of the state adhere to the following dynamical equation
%
\begin{equation*}%
    m_{k+1} = \E{\clmat\,\tilde{x}_k + \clinp_w\,w_k} = \clmat\, m_k.
\end{equation*}
%
From~\eqref{eq:cl-def} it then follows that~\eqref{eq:cost} can equivalently be written as~\cite{Bates:2011}
%
\begin{equation}%
    \label{eq:tr-cost}%
    \begin{aligned}
        J_k &= \E{\tilde{x}_k^\T{}\, Q\, \tilde{x}_k} \\
        &= \trace{\E{\tilde{x}_k^\T{}\, Q\, \tilde{x}_k}} \\
        &= \E{\trace{\tilde{x}_k^\T{}\, Q\, \tilde{x}_k}} \\
        &= \E{\trace{Q\, \tilde{x}_k\, \tilde{x}_k^\T{}}} \\
        &= \trace{Q\, \E{\tilde{x}_k\,\tilde{x}_k^\T{}}} \\
        &= \trace{Q\, \left( P_k + m_k\,m_k^\T{} \right)} \\
        &= \trace{Q\, P_k} + m_k^\T{}\, Q\, m_k.
    \end{aligned}
\end{equation}
%
Here, $\trace{\cdot}$ computes the trace of its argument.
If the condition $\E{\tilde{x}_0} = m_0 = 0$ is enforced, the expression can be reduced to solving $J_k = \trace{Q\, P_k}$.

Solving Equation~\eqref{eq:tr-cost} still requires knowledge about how the closed-loop state's covariance $P_k$ propagates in time.
Recalling that $\tilde{x}_k$ and $w_k$ are independent, evolving the covariance results in the following dynamical equation
%
\begin{equation}%
    \label{eq:cov}%
    \begin{aligned}
        P_{k+1} &= \cov{\tilde{x}_{k+1}} \\
        &= \E{\tilde{x}_{k+1}\,\tilde{x}_{k+1}^\T{}} - \E{\tilde{x}_{k+1}}\,\E{\tilde{x}_{k+1}}^\T{} \\
        &= \E{\tilde{x}_{k+1}\,\tilde{x}_{k+1}^\T{}} - m_{k+1}\,m_{k+1}^\T{} \\
        &= \E{\tilde{x}_{k+1}\,\tilde{x}_{k+1}^\T{}} - \clmat\, m_{k}\,m_{k}^\T{}\,\clmat^\T{} \\
        &= \E{ \left( \clmat\,\tilde{x}_k + \clinp_w\, w_k \right)\left( \clmat\,\tilde{x}_k + \clinp_w\, w_k \right)^\T{} } - \clmat\, m_{k}\,m_{k}^\T{}\,\clmat^\T{} \\
        &= \clmat\,\E{ \tilde{x}_k\,\tilde{x}_k^\T{} }\,\clmat^\T{} + \clinp_w\,\E{ w_k\,w_k^\T{} }\,\clinp_w^\T{} - \clmat\, m_{k}\,m_{k}^\T{}\,\clmat^\T{} \\
        &= \clmat\,\left( \cov{\tilde{x}_k} + m_k\,m_k^\T{} \right)\,\clmat^\T{} + \clinp_w\,\E{ w_k\,w_k^\T{} }\,\clinp_w^\T{} - \clmat\, m_k\,m_{k}^\T{}\,\clmat^\T{} \\
        &= \clmat\,\cov{\tilde{x}_k}\,\clmat^\T{} + \clinp_w\,\E{ w_k\,w_k^\T{} }\,\clinp_w^\T{} \\
        &= \clmat\,P_k\,\clmat^\T{} + \clinp_w\,R_1\,\clinp_w^\T{}.
    \end{aligned}
\end{equation}
%
Trivially, Equations~\eqref{eq:tr-cost} and~\eqref{eq:cov} establish the foundation for the performance analysis on a nominally executing dynamical system, such as the one in~\eqref{eq:clsys}.
When the control task starts experiencing deadline overruns, the switching dynamics alters the performance analysis accordingly, i.e., swapping $\clmat$ for $\clmat_{\theta_k}$ in Equation~\eqref{eq:cov}.\footnote{In Papers I and II we make additional alterations to the performance analysis to properly facilitate it under the specific system conditions.}

% Mention some tools to analyse performance
The research community developed multiple different tools to simplify the performance analysis of real-time embedded (or networked) control systems~\cite{Ohlin:2006}.
For instance, \code{JitterBug}~\cite{Lincoln:2002} and \code{JitterTime}~\cite{Cervin:2019} are tools to precisely evaluate the quadratic cost function considered in this thesis~\eqref{eq:tr-cost} for mixed continuous-/discrete-time system components.\footnote{The \code{JitterBug} toolbox models the execution time of different components as stochastic variables (in a Markov process) with known probabilistic properties.
This allows the entire system to be modeled as a MJLS where the stationary performance can be computed by solving a set of linear equations.
The \code{JitterTime} toolbox for Matlab and Julia~(\url{https://github.com/X-N-C/JitterTime.jl}) was developed to provide similar functionality, with more freedom for the user to design the different components' timing models.
These timing models make it possible to analyse the quadratic cost function~\eqref{eq:tr-cost} when, for instance, the control jobs overrun their corresponding deadlines or the communication between different components is lost.
Additionally, different complex scheduling policies and deadline overrun handling strategies, e.g., the \tK{}, \tS{}, or \tQ{} strategies, can freely be implemented.
One of the main disadvantages with \code{JitterTime} compared to \code{JitterBug} is that infinite horizon or stochastic timing scenarios might require a Monte Carlo or scenario theory based approach to provide accurate performance results.}
