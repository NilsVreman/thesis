We consider a controllable and fully observable \emph{discrete-time} sampled linear time invariant system, expressed as
\begin{equation}%
    \label{eq:plant}%
    \plant:\quad
    \begin{cases}
        x_{k+1} &= \Ap\,x_k + \Bp\,u_k \\
        y_{k} &= \Cp\,x_k + \Dp\,u_k,
    \end{cases}
\end{equation}
where $x_k \in \R^{n_x}$, $u_k \in \R^{n_u}$ and $y_k \in \R^{n_y}$ are the plant state, the control signal and the plant output, sampled at time $k\cdot \Ts$, $\Ts$ is the sampling period, and $k \in \N_{\geq}$.
The plant is controlled by a stabilising, LTI, one-step delay discrete-time controller
\begin{equation}%
    \label{eq:controller}%
    \ctrler:\quad
    \begin{cases}
        z_{k+1} = \Ac\,z_k + \Bc\,\left(r_k - y_k\right) \\
        u_{k+1} = \Cc\,z_k + \Dc\,\left(r_k - y_k\right),
    \end{cases}
\end{equation}
where $z_k \in \R^{n_z}$ is the controller's internal state and $r_k \in \R^{n_y}$ is the setpoint.
Without loss of generality, we consider $r_k = 0$.


\subsection{Real-time tasks that may miss deadlines}
\label{ssec:whalgebra}

The controller in~\eqref{eq:controller} is implemented as a real-time task $\tau$, and designed to be executed periodically with period $\Ts$ in a real-time embedded platform.
Under nominal conditions the task releases an instance (called \emph{job}) in each period, that should be completed before the release of the next instance.
We denote the sequence of activation instants for $\tau$ with $(a_k)_{k \in \N_{\geq}}$, such that, in nominal conditions, $a_{k+1} = a_k+\Ts$, the sequence of completion instants $(f_k)_{k \in \N_{\geq}}$, and the sequence of job deadlines with $(d_k)_{k \in \N_{\geq}}$, such that $d_k = a_k + \Ts$ (also called \emph{implicit} deadline).
This requirement can be either satisfied or not, leading respectively to deadline hits and misses.

\begin{definition}[Deadline hit and miss]%
    \label{def:hit}%
    The $k$-th job of a periodic task $\tau$ with period $\Ts$ hits its deadline when $f_k \leq d_k$ and misses its deadline when $f_k > d_k$.
\end{definition}

We refer to both deadline hits and misses using the term \emph{outcome} of a job.
Intuitively, each job's outcome is dependent on the characteristics of the remaining tasks executing in the real-time system and the chosen scheduling algorithm.
Given a taskset and a (worst-case) schedule, it is possible to bound the worst-case behaviour of the job outcomes~\cite{Bernat:2001, Ernst:2015}.
This bound is generally denoted using the \emph{weakly-hard model}~\cite{Bernat:2001}.
Following such model, a task $\tau$ may satisfy any combination of these weakly-hard constraints, defined as follows.
\begin{enumerate}[label=(\roman*)]
    \item \label{item:mk} $\tau \vdash \lcssanymiss{}$: in any window of $\ell$ consecutive jobs, at most $x$ deadlines are missed;
    \item \label{item:hk} $\tau \vdash \lcssanyhit{}$: in any window of $\ell$ consecutive jobs, at least $x$ deadlines are hit;
    \item \label{item:cons} $\tau \vdash \lcssrowmiss{}$: in any window of $\ell$ consecutive jobs, at most $x$ \emph{consecutive} deadlines are missed; and
    \item $\tau \vdash \lcssrowhit{}$: in any window of $\ell$ consecutive jobs, at least $x$ \emph{consecutive} deadlines are hit.
\end{enumerate}
In all such cases, $x\in \N_{\geq}$, $\ell \in \N_{>}$, and $x\leq \ell$.
A generic weakly-hard constraint is hereafter denoted with the symbol $\lambda$, while a set of $L$ constraints will be referred to as $\Lambda = \{ \lambda_1, \lambda_2, \dots, \lambda_L \}$.

We define a \emph{word} $w= \seq{c_1, c_2, \dots, c_N}$ as a sequence of $N$ consecutive outcomes, where each outcome $c_k$ is a character in the alphabet $\Sigma = \{\cM, \cH\}$.
We use $w \vdash \lambda$ to denote that $w$ satisfies the constraint $\lambda$. 
Stating that $\tau \vdash \lambda$ means that all the possible sequences of outcomes that $\tau$ can experience satisfy the corresponding constraint $\lambda$.
The set of such sequences naturally results from the definition of $\lambda$, and is formally defined as the \emph{satisfaction set} as follows~\cite{Bernat:2001}.

\begin{definition}[Satisfaction set $\mathcal{S}_N\left(\lambda\right)$]%
    \label{def:satisfaction}%
    We denote with $\sset{N}{\lambda}$ the set of words of length $N \geq 1$ that satisfy a constraint $\lambda$.
    Formally, $\sset{N}{\lambda} = \{ w \in \Sigma^N \mid w \vdash \lambda \}$.
\end{definition}
Taking the limit to infinity, the set $\sset{}{\lambda}$ contains all the words of infinite length that satisfy $\lambda$.
The notion of \emph{domination} between constraints~\cite{Bernat:2001} then follows.
\begin{definition}[Constraint domination]%
    \label{def:domination}%
    Constraint $\lambda_i$ \emph{dominates} $\lambda_j$ (formally, $\lambda_i \preceq \lambda_j$) if $\sset{}{\lambda_i} \subseteq \sset{}{\lambda_j}$. 
\end{definition}


\subsection{Control tasks that may miss deadlines}
\label{sec:back_deadline_miss}
When a control task $\tau$ is implemented on an embedded platform with limited computational power, alongside other applications, it is not uncommon for it to experience deadline misses, even in case of simple control designs (PID, LQG, etc)~\cite{akesson2020empirical,pazzaglia2021adaptive}.
Computational overruns may be caused by, e.g., bursts of interrupts, cache misses, variable execution times of ancillary functions, or other complex interactions.
If such events are rare or temporary, choosing a longer period for the controller to avoid them may result in worse performance and stability margins for nominal conditions~\cite{Pazzaglia:2019}.

Characterising the stability and performance of such controllers requires knowing what happens when a control deadline is missed~\cite{Pazzaglia:2019,Maggio:2020,Vreman:2021}.
In particular, we need a \emph{deadline miss handling strategy} to decide the fate of the job that missed the deadline (and possibly the next ones), and an \emph{actuator mode} to deal with the loss of a new control signal, for example by \tH{}ing the previous value constant or \tZ{}ing it~\cite{schenato09}.
A few handling strategies for periodic controllers have been proposed in literature, the most interesting being \tK{} and \tS{}~\cite{Cervin:2005,Pazzaglia:2019,Maggio:2020}.

\begin{definition}[\tK{} strategy]%
    \label{def:kill}%
    Under the \tK{} strategy, a job that misses its deadline is terminated immediately. 
    Formally, for the $k$-th job of $\tau$ either $f_k\leq d_k$ or $f_k=\infty$.
\end{definition}

\begin{definition}[\tS{} strategy]%
    \label{def:skip}%
    Under the \tS{} strategy, a job that misses its deadline is allowed to continue during the following period.
    Formally, if the $k$-th job of $\tau$ misses its deadline $d_k$, a new deadline $d^{+}_k = d_k + \Ts$ is set for the job, and $a_{k+1} = d^{+}_k$.
\end{definition}


\subsection{Stability analysis techniques based on JSR}
\label{sec:existing}

In~\cite{Maggio:2020}, the authors identify a set of subsequences of hit and missed deadlines, which can be arbitrarily combined to obtain all possible sequences in $\sset{}{\lcssrowmiss{}}$.
The stability analysis of the resulting arbitrary switching system is then obtained by leveraging the \emph{Joint Spectral Radius} (JSR)~\cite{rota}.

Given $m \in \N_{>}$ and a set of matrices $\Aa = \{\clmat_1, \ldots, \clmat_m \} \subseteq \R^{n\times n}$, under the hypothesis of arbitrary switching over any sequence $s=\seq{a_1,a_2,\dots}$ of indices of matrices in $\Aa$, the JSR of $\Aa$ is defined by:
\begin{equation}%
    \label{jsr}
    \rho\funof{\Aa} = \lim_{N\rightarrow\infty} \max_{s \in \{1,\ldots,m\}^N} \norm{\clmat_{a_N} \cdots \clmat_{a_2}\clmat_{a_1}}^{\frac{1}{N}}.
\end{equation}
The number $\rho\left(\Aa\right)$ characterizes the maximal asymptotic growth rate of matrix products from $\Aa$ (thus $\rho(\Aa) < 1$ means that the system is asymptotically stable), and is independent of the norm $\norm{\cdot}$ used in \eqref{jsr}.
Existing practical tools such as the JSR Matlab toolbox~\cite{vankeerberghen2014jsr} include multiple algorithms to compute both upper and lower bounds on $\rho\left(\Aa\right)$.

When the switching sequences between the dynamics of $\Aa$ are not arbitrary, but constrained by a graph $\GG{}$, the so called \emph{constrained joint spectral radius} (CJSR)~\cite{dai2012gelfand} can be applied.
Introducing $S_N\funof{\GG{}}$ as the set of all possible switching sequences $s$ of length $N$ that satisfy the constraints of a graph $\GG{}$, the CJSR of $\Aa$ is defined by
\begin{equation}%
    \label{cjsr}%
    \rho\funof{\Aa,\GG{}} = \lim_{N\rightarrow\infty} \max_{s \in S_N\funof{\GG{}}} \norm{\clmat_{a_N} \cdots \clmat_{a_2}\clmat_{a_1}}^{\frac{1}{N}}.
\end{equation}
In general, computing or approximating the CJSR is harder than using the JSR. 
In~\cite{philippe2016stability}, the authors propose a multinorm-based method to approximate with arbitrary accuracy the CJSR.
Other works~\cite{kozyakin2014berger,xu2020approximation} propose the creation of an arbitrary switching system such that its JSR is equal to the CJSR of the original system, based on a Kronecker lifting method.
This will be also our approach, as detailed later.

In~\cite{parrilo}, the authors propose an efficient approach to compute upper bounds of the JSR based on positive polynomials which can be decomposed as \emph{sums of squares} (SOS). 
Finding the coefficients of a polynomial being SOS simplifies to solving an SDP~\cite{lasserre2001global}.
%
To reduce time and space complexity, a \emph{sparse} variant has been proposed in \cite{sparsejsr} exploiting the sparsity of the input matrices, based on the \emph{term sparsity} SOS (TSSOS) framework~\cite{tssos}.
By contrast, the procedure in~\cite{parrilo} will be denoted hereafter as \emph{dense}.
While providing a more conservative result, the sparse upper bound can be obtained significantly faster if the matrices from $\Aa$ are sparse~\cite{sparsejsr}, e.g., the matrices we analyse in Section~\ref{sec:stability}.
%This is relevant for our analysis, as the matrices that we analyse (introduced in Section~\ref{sec:stability}) are sparse.
