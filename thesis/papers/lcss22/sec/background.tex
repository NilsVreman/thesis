\removed{In this section we introduce the necessary background and the notation used in the following.}
\removed{We also provide a thorough analysis of related research contributions.}%

\subsection{Control Systems}
\label{ssec:systemmodel}
In this paper, we consider an arbitrary \emph{discrete-time} sampled linear time invariant (LTI) system~\cite{Astrom:1997}, expressed as follows:
%
\begin{equation}
    \label{eq:plant}
    P:\quad
    \begin{cases}
        x_{t+1} &= A_p\,x_t + B_p\,u_t \\
        y_{t} &= C_p\,x_t + D_p\,u_t.
    \end{cases}
\end{equation}
%
Here $\new{t\in\N\setminus\left\{ 0 \right\}}$ is a positive integer value.
The system is sampled with sampling period $T$.
The vector $x_t\in\R^n$ contains the plant's internal states, $u_t\in\R^r$ contains the signals controlling the plant, and $y_t\in\R^q$ is the vector of measurement values acquired from the plant at time $t\cdot T$. Finally $A_p$, $B_p$, $C_p$ and $D_p$ are real-valued matrices of appropriate size defining the dynamics of the plant.
In line with standard assumptions, we assume the system in~\eqref{eq:plant} to be controllable and the state to be fully observable.

The plant is controlled by a stabilising, LTI, one-step delay,%
\removed{\footnote{\removed{One-step delay controllers are controllers in which a control signal computed in the $k$-th interval is actuated at the beginning of the $k+1$-th period. In the real-time systems jargon, this is enforced by adopting the so-called the Logical Execution Time paradigm~\cite{Kirsch:2012, Ernst:2018}.}}%
From a real-time perspective, it improves the timing predictability, and from a control perspective, it reduces input-output jitter and allows to neglect time-varying computational delays.}
discrete-time controller producing $u_t$ and defined as:
%
\begin{equation}
    \label{eq:controller}
    C:\quad
    \begin{cases}
        z_{t+1} &= A_c\,z_t + B_c\,e_t \\
        u_{t+1} &= C_c\,z_t + D_c\,e_t.
    \end{cases}
\end{equation}
%
Here, $z_t\in\R^s$ is the controller's internal state and $e_t\in\R^q$, $e_t= r_t - y_t$ is the tracking error of the controller \new{(where $r_t$ is the reference signal to track)}.
\removed{For simplicity, and}Without loss of generality, we will be treating the regulator problem\removed{; ergo, no reference tracking} ($r_t = 0$).

\subsection{Real-time tasks that may miss deadlines}
\label{ssec:whalgebra}

The controller in~\eqref{eq:controller} is implemented as a real-time task $\tau$, and designed to be executed periodically with period $T$ in a real-time embedded platform.
Under nominal conditions the task releases an instance (called \emph{job}) in each period, that should be completed
before the release of the next instance. We denote the sequence of activation
instants for $\tau$ with $(a_i)_{i \in \N}$, such that, in nominal conditions, $a_{i+1} = a_i+T$,  the sequence of
completion instants $(f_i)_{i \in \N}$, and the sequence of job
deadlines with $(d_i)_{i \in \N}$, such that $d_i = a_i +
T$ (also called \emph{implicit} deadline).  This requirement can be either
satisfied or not, leading respectively to deadline hits and misses.

\begin{definition}[Deadline hit]%
\label{def:hit}%
    The $i$-th job of a periodic task $\tau$ with period $T$ \removed{(and implicit deadline)}hits its deadline when $f_i \leq d_i$.
\end{definition}

\begin{definition}[Deadline miss]%
\label{def:miss}%
    The $i$-th job of a periodic task $\tau$ with period $T$ \removed{(and implicit deadline)}misses its deadline when $f_i > d_i$.
\end{definition}

We refer to both deadline hits and misses using the
term \emph{outcome} of a job.  In order to provide some guarantees on
how the computation is distributed in the application and therefore how
misses and hits are interleaved, \emph{weakly-hard task models} have
been introduced in the past~\cite{Bernat:2001}. \removed{Weakly-hard models
impose constraints on the number of deadline hits and misses that can
be experienced in a window of $k\geq 1$ consecutive activations.
In particular, the following four models have been proposed in literature~\cite{Bernat:2001}.}

\begin{definition}[Weakly-hard task models]%
\label{def:weakly-hard}%
    A task $\tau$ may satisfy any combination of these four basic weakly
    hard constraints:
    \begin{enumerate}[label=(\roman*)]
        \item \label{item:mk} $\tau \vdash\overbar{\binom{m}{k}}$: at most $m$ deadlines are missed, in any window of $k$ consecutive jobs;
        \item \label{item:hk} $\tau \vdash\binom{h}{\!\:\!\:k\!\:\!\:}$: at least $h$ deadlines are hit, in any window of $k$ consecutive jobs;
        \item \label{item:cons} $\tau \vdash\overbar{\genfrac{<}{>}{0pt}{}{m}{\!\:\!\:k\!\:\!\:}}$: at most $m$ \emph{consecutive} deadlines are missed, in any window of $k$ consecutive jobs; and
        \item $\tau \vdash\genfrac{<}{>}{0pt}{}{h}{\!\:\!\:k\!\:\!\:}$: at least $h$ \emph{consecutive} deadlines are hit, in any window of $k$ consecutive jobs,
    \end{enumerate}
    \removed{with $m,h,k\in \N$, $m\leq k$, $h \leq k$ and $k\geq 1$.}\new{with $m,h\in \N$, $k \in \N\setminus \left\{ 0 \right\}$, $m\leq k$, and $h\leq k$.}
\end{definition}

The model expressed in \ref{item:hk} was first introduced in~\cite{Hamdaoui:1995}, but its complementary version \ref{item:mk} %
\new{%
-- often referred to as the $(m,k)$ model -- has gained the most research traction~\cite{pazzaglia2020generalized, Hammadeh:2019, Natarajan:2019}.
For the model in~\ref{item:cons} it has been proven that the window size becomes irrelevant~\cite{Bernat:2001}, hence it is commonly referred to as $\overbar{\left<m\right>}$ (a notation also adopted in this paper). A useful result presented in~\cite{Maggio:2020} is that $\overbar{\left<m\right>} = \overbar{\binom{m}{m+1}}$.
}%
\removed{%
has gained most research traction recently, and is often referred to as the $(m,k)$ model.
In particular, schedulability analyses have been developed for the $(m,k)$ model~\cite{pazzaglia2020generalized, Hammadeh:2019}.
Furthermore, a weakly-hard sensitivity analysis is implemented in a toolchain that allows to derive the strongest satisfied $(m,k)$ constraint from C code~\cite{Natarajan:2019}.
The model in~\ref{item:cons} constrains the maximum number of consecutive deadline misses.
As proven in~\cite{Bernat:2001}, the window size becomes irrelevant in this model,\footnote{A useful result presented in~\cite{Maggio:2020} is that $\overbar{\left<m\right>} = \left( m, m+1 \right)$.} hence it is commonly referred to as $\overbar{\left<m\right>}$ (a notation also adopted in this paper).
}%


These models convey different information about the deadline miss properties of the task, and can also be used jointly, stating that a task $\tau$ satisfies a \emph{set} of constraints (possibly of different types).
\removed{%
In particular, the possibility of defining sets of constraints, for example by obtaining the worst-case number of deadline misses over different time windows, opens the door to a more detailed (and possibly less pessimistic) timing description of the tasks. 
This latter aspect has been however generally}\new{However, this has generally been} overlooked by the research literature, which often considers timing models defined by a single weakly-hard constraint.
\removed{Due to the relative importance and prevalence of $(m,k)$ and $\overbar{\left<m\right>}$ models in the recent literature, we will use such models in our examples. 
Nonetheless, we remark that the analysis presented in the remainder of the paper is invariant to the choice of weakly-hard model. 
To simplify the notation,}%
In the following we will refer to a generic weakly-hard constraint using the symbol $\lambda$, while a set of $L$ weakly hard constraints will be referred as $\Lambda = \{ \lambda_1, \lambda_2, \dots, \lambda_L \}$.

\removed{Stating that $\tau \vdash \lambda$ means that all the possible sequences of outcomes (hits and misses) that $\tau$ can experience satisfy the corresponding constraint $\lambda$. To this end,}We define a string $\astring= \{\alpha_1, \alpha_2, \dots, \alpha_N\}$ \removed{of length $N$}\new{representing a sequence of $N$ consecutive outcomes}.
Each element $\alpha_i$ in the string is a \removed{symbol}\new{character} that belongs to the alphabet of job outcomes $\Sigma$, \new{defined as follows}.
\removed{The most common choices are to use $\cM$ and $\cH$ (or $0$ and $1$) to indicate respectively a miss and a hit.}

\begin{definition}[Alphabet $\Sigma$ of job outcomes\removed{~for real-time tasks}]%
    \label{def:basic-alphabet}%
    The alphabet $\Sigma$ includes all \removed{the}possible outcomes of a \removed{real-time}job.
    Commonly, $\Sigma = \{ \cM, \cH \}$ \removed{where $\cM$ indicates a deadline miss and $\cH$ indicates a deadline hit.}\new{where $\cM$ and $\cH$ denotes respectively a deadline miss or hit.}
\end{definition}

We use $\astring \vdash \lambda$ to denote that the string $\astring$
satisfies the constraint $\lambda$. 
\new{Stating that $\tau \vdash \lambda$ means that all the possible sequences of outcomes (hits and misses) that $\tau$ can experience satisfy the corresponding constraint $\lambda$.} 
The set of \removed{all possible sequences that satisfy
a given constraint $\lambda$}\new{such sequences} naturally results from the definition of
$\lambda$, and is formally defined as \new{the \emph{satisfaction set} as} follows~\cite{Bernat:2001}.

\begin{definition}[Satisfaction set $\sset{N}{\lambda}$\removed{~of a weakly-hard constraint $\lambda$}]%
    \label{def:satisfaction}%
    We denote with $\sset{N}{\lambda}$ the set of sequences of length $N \geq 1$ that satisfy a constraint $\lambda$.
    Formally, $\sset{N}{\lambda} = \{ \astring \in \Sigma^N \mid \astring \vdash \lambda \}$.
\end{definition}
Taking the limit to infinity, the set $\sset{\infty}{\lambda}$ contains all the sequences of infinite length that satisfy \removed{the constraint}$\lambda$.
We will henceforth denote $\sset{}{\lambda} \equiv \sset{\infty}{\lambda}$.
%
By leveraging the definition of satisfaction sets, it is possible to
determine how two constraints $\lambda_i$ and $\lambda_j$ relate to
one another~\cite{Bernat:2001, Wu:2020}. In particular, we can define
a \emph{domination} notion between constraints~\cite{Bernat:2001}.

\begin{definition}[Constraint domination]%
    \label{def:domination}%
    \removed{%
        Given two constraints $\lambda_i$ and $\lambda_j$, we say that $\lambda_i$ is harder to satisfy than (or dominates) $\lambda_j$ (denoted by $\lambda_i \prec \lambda_j$) if all sequences that satisfy $\lambda_i$ also satisfy $\lambda_j$, i.e., when $\sset{}{\lambda_i} \subset \sset{}{\lambda_j}$. 
    Similarly, we say that $\lambda_i \preceq \lambda_j$ if $\sset{}{ \lambda_i } \subseteq \sset{}{ \lambda_j}$.
   }\new{%
    Given two constraints $\lambda_i$ and $\lambda_j$, we say that $\lambda_i$ \emph{dominates} $\lambda_j$ if all sequences satisfying $\lambda_i$ also satisfy $\lambda_j$.
    Formally, $\lambda_i \preceq \lambda_j \Leftrightarrow \sset{}{ \lambda_i } \subseteq \sset{}{ \lambda_j }$. 
   } %
\end{definition}
The notion of constraint domination introduces a partial ordering between constraints~\cite{Bernat:2001}. \removed{Despite constraint ordering being an active research area, 
(with new relations found recently, e.g., in~\cite{Wu:2020}), the abstraction of properties of the represented physical process by leveraging constraint dominance relationships is still unexplored.} In Section~\ref{sec:analytic_results}, we relate the real-time concept of constraint dominance to the control theoretical notion of stability.


\subsection{Control tasks that may miss deadlines}
\label{sec:back_deadline_miss}
%While the considerations above apply to generic tasks,
This paper considers the special case of \removed{control tasks}\new{$\tau$ being a \emph{control task} (e.g., PID, LQG, etc)} subject to weakly-hard constraints.
\removed{, meaning that the task $\tau$ implements a control algorithm (e.g., a Proportional Integral and Derivative controller, a Linear Quadratic Regulator, etc).}%
\new{%
Particularly, this model applies to control tasks implemented on embedded platforms with limited computational power, alongside other concurrent applications~\cite{pazzaglia2021adaptive}. 
In this context, even a simple controller may be subject to computational overruns~\cite{akesson2020empirical} caused by, e.g., bursts of high priority interrupts, cache misses, variable execution times of ancillary functions, such as data filtering or coordinate transformations (e.g., Park-Clarke transforms in electric motors), or other complex interactions.
If such events are rare or temporary, choosing a longer period for a sampled controller may result in worse performance and stability margins for nominal conditions~\cite{Pazzaglia:2019}.}%

Since the fundamental properties of stability and performance of a controller depend on the actual pattern of control commands, it is necessary to precisely define what happens when a control deadline is missed.
This led to several suggestions in literature for \emph{deadline miss handling strategies} and \emph{actuator modes}.
The handling strategy manages the fate of the job that missed the deadline (and possibly the next ones), while the actuator mode deals with the loss of control signal and decides whether it should be held constant, or zeroed~\cite{schenato09}.\removed{\footnote{\removed{
The most common strategy applied by control engineers is \emph{hold}. However, \emph{zero} is sometimes the only viable option, e.g., when holding the previous control signal requires additional computation (like the translation to a different coordinate system) and there is no time to perform such computation.}}}
A few handling strategies \new{for periodic controllers} have been proposed in the past~\cite{Cervin:2005,Pazzaglia:2018, Pazzaglia:2019, Maggio:2020,pazzaglia2020generalized}, the most important being \emph{Kill}, \emph{Skip-Next}, and \emph{Queue} (also denoted as \emph{Continue}).
\new{Other strategies exist for server-based systems, e.g., the server approach~\cite{cervin2005control} or the continuous stream model~\cite{fontanelli2013continuous}.
However, they require elastic modifications of periods and deadlines, and are thus more suited for soft deadlines and a probabilistic framework.} 
\removed{In particular,}The results of~\cite{Cervin:2005, Pazzaglia:2019},\new{\cite{Cervin:2019}},\cite{Maggio:2020} suggest that the Queue strategy (i.e., letting each job executing until completion, queuing newly released jobs) may create chain effects of missed deadlines, negatively affecting stability and performance.
Thus, Kill and Skip-Next will be the focus of this paper.
The following definitions \removed{of deadline miss handling strategies}hold for a task $\tau$, with period $T$.

\begin{definition}[Kill strategy]%
    \label{def:kill}%
    \removed{The Kill strategy imposes the immediate termination of the job that misses the deadline.
    A killed job will never complete its execution, meaning that, for an arbitrary}%
    \new{The Kill strategy terminates the job that misses the deadline immediately.
    Formally, for the}~$i$-th job of $\tau$ either $f_i\leq d_i$ or $f_i=\infty$.
    If the $i$-th job is killed, the $(i+1)$-th job is immediately released, i.e., $a_{i+1} = a_{i} + T$.
\end{definition}

\begin{definition}[Skip-Next strategy]%
    \label{def:skip}%
    \removed{%
    The Skip-Next strategy imposes that a job that misses its deadline is allowed to continue during the following period. When an arbitrary $i$-th job of $\tau$, released at $a_i$ misses its deadline $d_i$, a new deadline $d^{\succ}_i = d_i + T$ is set for the job.
    The $i$-th job then becomes a job with activation time $a_i$ and deadline $d^{\succ}_i$.
    At the same time, the $(i+1)$-th job activation is set to $a_{i+1} = d^{\succ}_i$.
    The process can be repeated, extending the deadline until the job completion.
   }%
    \new{%
    The Skip-Next strategy allows the job that misses its deadline to continue during the following period. Formally, if the $i$-th job of $\tau$ misses its deadline $d_i$, a new deadline $d^{+}_i = d_i + T$ is set for the job.
    The $(i+1)$-th job activation is set to $a_{i+1} = d^{+}_i$.
   } %
    \end{definition}

\subsection{Stability analysis techniques \new{based on JSR}}
\label{sec:existing}
A control task that may experience deadline misses requires a proper analysis to check its asymptotic stability in all possible timing conditions. 
In this paragraph we introduce \removed{the}existing analytic techniques from the control literature, which can be adapted to deal with this problem.

\removed{%
The closest work to this paper is the one in~\cite{Maggio:2020}, which presents a stability analysis for the $\tau \vdash\overbar{\left<m\right>}$ weakly hard constraint.
However, its application for a more general weakly-hard constraint may result in overly pessimistic bounds.
More specifically,
}%
In~\cite{Maggio:2020}, the authors identify a set of subsequences of hit and missed deadlines, which can be arbitrarily combined to obtain all possible sequences in $\sset{}{\overbar{\left<m\right>}}$.
After assigning to each subsequence its corresponding dynamic matrix, the stability analysis of the resulting arbitrary switching system can then be obtained by leveraging the \emph{Joint Spectral Radius} (JSR)~\cite{Jungers2009}, which is briefly presented here.
%
Given $\ell\in\N\setminus \left\{ 0 \right\} $ and a set of
matrices $\Aa = \{A_1, \ldots, A_\ell \} \subseteq \R^{n\times n}$,
under the hypothesis of arbitrary switching \new{over any sequence $s=\{a_1,a_2,\dots\}$ of indices of matrices in $\Aa$}, the JSR of $\Aa$ is defined by:
\begin{equation}
  \label{jsr} \rho\left(\Aa\right)= \lim_{k\rightarrow\infty}
  \max_{s\in\{1,\ldots,\ell\}^k} \norm{A_{a_k}
  \cdots A_{a_2}A_{a_1}}^{\frac{1}{k}} \,.
\end{equation}
The number $\rho\left(\Aa\right)$ characterizes the maximal asymptotic
growth rate of products of matrices from $\Aa$
(thus  $\rho(\Aa)<1$ means that the system is
asymptotically stable), and is independent of
the norm $\norm{\cdot}$ used in \eqref{jsr}.
\removed{When $\Aa$ is a singleton, the joint spectral radius is equal to the spectral radius, which justifies that the JSR generalizes the spectral radius for the case of multiple matrices.}

The JSR was introduced in \cite{rota} and is applied in several important
contexts, including switched dynamical systems stability and
combinatorics\new{~\cite{Jungers2009}}. 
\removed{We refer to~\cite{Jungers2009} for more details about
theory and applications related to JSR.}Existing practical tools such
as the JSR Matlab toolbox~\cite{vankeerberghen2014jsr} allow one to
compute both upper and lower bounds on $\rho\left(\Aa\right)$, leveraging multiple algorithms.

The JSR approach can be generalized for the case \removed{of}\new{where the} switching
sequences \new{between the different dynamics of $\Aa$ are not arbitrary, but} constrained by a given graph $\GG{}$, with the so called
\emph{constrained joint spectral  radius} (CJSR)~\cite{dai2012gelfand}. Consider again a set of matrices
$\Aa$ and define \removed{(with a slight abuse of notation)}${S}_k(\GG{})$ as the set
of all possible switching sequences \new{$s$} of length $k$ that satisfy the
constraints of graph $\GG{}$. Then, the CJSR of $\Aa$ is defined by
\begin{equation}
\label{cjsr}
    \rho\left(\Aa,\GG{}\right)= \lim_{k\rightarrow\infty}
    \max_{s\in{S}_k(\GG{})} \norm{A_{a_k}
    \cdots A_{a_2}A_{a_1}}^{\frac{1}{k}} \,.
\end{equation}
In general, computing or approximating the
CJSR is not an easy task, and noticeably fewer works exist for CJSR with respect to JSR. In~\cite{philippe2016stability}, the authors propose a
multinorm-based method to approximate with arbitrary accuracy the
CJSR. An ad-hoc branch-and-bound algorithm is proposed
in~\cite{dercole2017simple}.
Another approach~\cite{kozyakin2014berger, wang2014stability, xu2020approximation} is based on the creation of an arbitrary switching system such that its JSR is equal to the CJSR of the original system, based on a Kronecker lifting method.
This will be also our approach, as detailed \removed{in later sections of the paper.}\new{later.} 

In~\cite{parrilo}, the authors propose \removed{a hierarchy of relaxations to
provide a sequence of} upper bounds for the JSR based
on positive polynomials which be can decomposed as \emph{sum of
squares} (SOS) of polynomials. This approximation method is particularly interesting in terms of performance and has been chosen as primary approach in our tests. 
%
It consists of considering a hierarchy of relaxations
indexed by \removed{$d\in\N^{>}$,}\new{$d\in\N\setminus \left\{ 0 \right\}$,} which in turn yields a sequence of upper
bounds for $\rho\left(\Aa\right)$:
\begin{align}
    \label{densesos}
    \rho_{\textrm{SOS},2d}\left(\Aa\right) &= \inf_{p\in\R\left[x\right]_{2d},\gamma}  \gamma \\
    &\textrm{ s.t.} 
    \begin{cases} 
        p\left(x\right)\in\sos_{2d},\\
        \gamma^{2d}p\left(x\right)-p\left(A_ix\right)\in\sos_{2d},\ 1 \leq i \leq \ell.
    \end{cases} \nonumber
\end{align}
%
The optimal value of this optimization problem can be computed by
combining a bisection procedure on $\gamma$ together with
\emph{semidefinite programming} (SDP)~\cite{wolkowicz2012handbook}.
Finding the coefficients of a
polynomial being SOS boils down to solving an SDP~\cite{re2,
parrilo2000structured, lasserre2001global}.
\new{%
A typical SDP solver (e.g.~Mosek) based on interior-point methods requires $O(n_{\text{sdp}}^3 + n_{\text{sdp}}^2 m_{\text{sdp}}^2 + m_{\text{sdp}}^3)$ arithmetic operations per iteration and $O(n_{\text{sdp}}^2 + n_{\text{sdp}} m_{\text{sdp}} + m_{\text{sdp}}^2)$ memory where $n_{\text{sdp}}$ is the size of PSD matrices and $m_{\text{sdp}}$ is the number of affine constraints. 
Here, the set $\mathcal{A}$ involves $m$ matrices of size $n$.
For the SDP relaxation given in Equation \eqref{densesos}, with $d = 1$, one has $n_{\text{sdp}} = O(n)$ and $m_{\text{sdp}} = O(m n^2)$.
Thus, in total the SDP solver requires $O(m^3 n^6)$ arithmetic operations per iteration and $O(m^2 n^4)$ memory.}%
\removed{%
SDP solvers minimize a linear objective function with linear matrix inequality constraints.
The value of an SDP problem can be approximated up to a given precision in polynomial time with respect to its input size, while relying on interior-point algorithms, implemented in modern solvers such as Mosek~\cite{andersen2000mosek}.
Finding the coefficients of a polynomial being SOS boils down to solving an SDP~\cite{re2, parrilo2000structured, lasserre2001global}.
The upper bound $\rho_{\textrm{SOS},2d}\left(\Aa\right)$ satisfies the following inequalities (Theorem 4.3 of~\cite{parrilo}).
\begin{theorem}[SOS relaxation]
    \label{sec2-thm2}
    Let $\Aa=\{A_1,\ldots,A_\ell\}\subseteq\R^{n\times n}$. 
    For any integer $d\ge1$, $\ell^{-\frac{1}{2d}}\rho_{\textrm{SOS},2d}\left(\Aa\right)\le\rho\left(\Aa\right)\le\rho_{\textrm{SOS},2d}\left(\Aa\right)$.
\end{theorem}
Theorem \ref{sec2-thm2} implies the convergence of $\{\rho_{\textrm{SOS},2d}(\Aa)\}_{d\ge1}$ to $\rho\left(\Aa\right)$ when $d$ goes to infinity.}%
\new{To reduce both the time and space complexity} associated with this procedure, a \emph{sparse} variant, called \texttt{SparseJSR}, has been proposed in \cite{sparsejsr} to exploit the sparsity of the input matrices from $\Aa$, based on the \emph{term sparsity} SOS (TSSOS) framework \cite{tssos}.
This allows the authors to provide SOS decompositions of polynomials with arbitrary sparse support.
By contrast, the procedure defined in~\eqref{densesos} will be denoted hereafter as \emph{dense}.

\removed{
Next, we briefly recall
some preliminary material from~\cite{sparsejsr} to explain
how the \texttt{SparseJSR} algorithm works.
The \emph{support} of $f$
is defined by $\supp\left(f\right)=\{\a\in\A\mid f_{\a}\ne0\}$. Let
$\R\left[\A\right] \subset \R\left[x\right]_{2d}$ be the set of
polynomials whose supports are contained in $\A$,  and $\sos\left[\A\right] \subset \sos_{2d}$
be the set of SOS polynomials whose supports are contained in $\A$.
The authors in~\cite{sparsejsr} restrict the support of the unknown polynomial $p$
from Equation~\eqref{densesos} to obtain a sparse SOS program,
%
choosing  $p_0(x)= x_1^{2d} + x_2^{2d} + \ldots + x_n^{2d}$ to be the
power sum of coordinates with a fixed degree $2 d$,
$\A_0=\supp\left(p_0\right)$, and
\begin{equation}\label{supp}
  \A=\A_0\cup\bigcup_{i=1}^\ell \supp\left(p_0\left(A_ix\right)\right).
\end{equation}
Let $\A_i= \A \cup \supp \left(p \left(A_ix\right) \right)$ for
$i=1, \ldots, \ell$. To take into account the sparsity of the matrices
from $\Aa$, $\sos_{2d}$ is replaced by
either by $\sos_{\A}$ or $\sos_{\A_i}$ in~\eqref{densesos}, obtaining:
\begin{align}
    \label{tssos}
    \rho_{\textrm{TSSOS},2d}\left(\Aa\right) &= \inf_{p\in\R\left[\A\right],\gamma}\gamma \\
    &\textrm{ s.t.} 
    \begin{cases}
        p\left(x\right)\in\sos_{\A},\\
        \gamma^{2d}p\left(x\right) - p\left(A_ix\right)\in\sos_{\A_i}, \ 1 \leq i \leq \ell. \nonumber
    \end{cases}
\end{align}
As for $\rho_{\textrm{SOS},2d}\left(\Aa\right)$, the optimal value
$\rho_{\textrm{TSSOS},2d}\left(\Aa\right)$ of \eqref{tssos} can be
obtained from a bisection procedure.
}
\new{Let $\rho_{\textrm{TSSOS},2d}\left(\Aa\right)$ be the upper bound provided by \texttt{SparseJSR}.
From Theorem 4.1 of~\cite{sparsejsr}, one has} 
$\rho\left(\Aa\right) \leq \rho_{\textrm{SOS},2d}\left(\Aa\right) \leq
\rho_{\textrm{TSSOS},2d}\left(\Aa\right)$, yielding a hierarchy of
upper bounds for the JSR $\rho\left(\Aa\right)$. By contrast with the
dense case, the sparse upper bound can be obtained significantly
faster if the matrices from $\Aa$ are sparse.
This is relevant for the
specific application we are considering, as the matrices that we have
to analyse (introduced in Section~\ref{sec:stability}) are sparse and
it is possible to exploit their structure to reduce computation times.
\removed{%
We remark that it is also possible to substitute SOS with other algorithms such as branch and bound or ellipsoid \cite{vankeerberghen2014jsr} to compute bounds on the value of the JSR.
}%
