\subsection{Control Systems}
\label{ssec:systemmodel}
In this paper, we consider a controllable and fully observable \emph{discrete-time} sampled linear time invariant system, expressed as:
%
\begin{equation}
    \label{eq:plant}
    \plant: \,\, \left\{
    \begin{aligned}
        x_{k+1} &= \Ap\,x_k + \Bp\,u_k \\
        y_{k} &= \Cp\,x_k + \Dp\,u_k
    \end{aligned}
    \right.
\end{equation}
%
Here $x_k\in\R^{n_x}$, $u_k\in\R^{n_u}$, and $y_k\in\R^{n_y}$ are the plant state, the control signal and the plant output, sampled at time $k\cdot \Ts$, $\Ts$ is the sampling period, and $k\in\N_{\geq}$.
The plant is controlled by a stabilising, LTI, one-step delay discrete-time controller
%
\begin{equation}
    \label{eq:controller}
    \ctrler: \,\, \left\{
    \begin{aligned}
        z_{k+1} &= \Ac\,z_k + \Bc\,\left(r_k - y_k\right) \\
        u_{k+1} &= \Cc\,z_k + \Dc\,\left(r_k - y_k\right).
    \end{aligned}
    \right.
\end{equation}
%
where $z_k\in\R^{n_z}$ is the controller's internal state and $r_k\in\R^{n_y}$ is the setpoint.
Without loss of generality, we consider $r_k = 0$.

\subsection{Real-Time Tasks That May Miss Deadlines}
\label{ssec:whalgebra}
%
The controller in~\eqref{eq:controller} is implemented as a real-time task $\tau$, and designed to be executed periodically with period $\Ts$ in a real-time embedded platform.
Under nominal conditions the task releases an instance (called \emph{job}) in each period, that should be completed before the release of the next instance.
We denote the sequence of activation instants for $\tau$ with $(a_i)_{i \in \N_{\geq}}$, such that, in nominal conditions, $a_{i+1} = a_i+\Ts$,  the sequence of completion instants $(f_i)_{i \in \N_{\geq}}$, and the sequence of job deadlines with $(d_i)_{i \in \N_{\geq}}$, such that $d_i = a_i + \Ts$ (also called \emph{implicit} deadline).
This requirement can be either satisfied or not, leading respectively to deadline hits and misses.
%
\begin{definition}[Deadline hit and miss]%
\label{def:hit}%
    The $i$-th job of a periodic task $\tau$ with period $\Ts$ hits its deadline when $f_i \leq d_i$ and misses its deadline when $f_i > d_i$.
\end{definition}
%
We refer to both deadline hits and misses using the term \emph{outcome} of a job.
Intuitively, each job's outcome is dependent on the characteristics of the remaining tasks executing in the real-time system and the chosen scheduling algorithm.
Given a taskset and a (worst-case) schedule, it is possible to bound the worst-case behaviour of the job outcomes~\cite{Bernat:2001, Ernst:2015}.
This bound is generally denoted using the \emph{weakly-hard model}~\cite{Bernat:2001}.\footnote{We emphasise that the counter incrementing the number of control periods since the start of time $k$ will be distinguishable from the weakly-hard constraint's window length $k$ based on context. Otherwise, a specification will be provided.}
%
\begin{definition}[Weakly-hard task models]%
\label{def:weakly-hard}%
    A task $\tau$ may satisfy any combination of these four basic weakly
    hard constraints:
    \begin{enumerate}[label=(\roman*)]
        \item \label{item:mk} $\tau \vdash \anymiss{}$: at most $x$ deadlines are missed, in any window of $k$ consecutive jobs;
        \item \label{item:hk} $\tau \vdash \anyhit{}$: at least $x$ deadlines are hit, in any window of $k$ consecutive jobs;
        \item \label{item:cons} $\tau \vdash \rowmiss{}$: at most $x$ \emph{consecutive} deadlines are missed, in any window of $k$ consecutive jobs; and
        \item $\tau \vdash \rowhit{}$: at least $x$ \emph{consecutive} deadlines are hit, in any window of $k$ consecutive jobs,
    \end{enumerate}
    with $x\in \N_{\geq}$, $k \in \N_{>}$, and $x\leq k$.
\end{definition}
%
A generic weakly-hard constraint is hereafter denoted with the symbol $\lambda$, while a set of $L$ constraints will be referred to as $\Lambda = \{ \lambda_1, \lambda_2, \dots, \lambda_L \}$.

We define a word $w= \seq{c_1, c_2, \dots, c_N}$ representing a sequence of $N$ consecutive outcomes.
Each element $c_i$ in the word is a character that belongs to the alphabet of job outcomes $\Sigma$, defined as follows.

\begin{definition}[Alphabet $\Sigma$ of job outcomes]%
    \label{def:basic-alphabet}%
    The alphabet $\Sigma$ includes all possible outcomes of a job.
    Commonly, $\Sigma = \{ \cM, \cH \}$ where $\cM$ and $\cH$ denotes respectively a deadline miss or hit.
\end{definition}

We use $w \vdash \lambda$ to denote that the word $w$ satisfies the constraint $\lambda$. 
Stating that $\tau \vdash \lambda$ means that all the possible sequences of outcomes (hits and misses) that $\tau$ can experience satisfy the corresponding constraint $\lambda$.
The set of such sequences naturally results from the definition of $\lambda$, and is formally defined as the \emph{satisfaction set} as follows~\cite{Bernat:2001}.

\begin{definition}[Satisfaction set $\sset{N}{\lambda}$]%
    \label{def:satisfaction}%
    We denote with $\sset{N}{\lambda}$ the set of sequences of length $N \geq 1$ that satisfy a constraint $\lambda$.
    Formally, $\sset{N}{\lambda} = \{ w \in \Sigma^N \mid w \vdash \lambda \}$.
\end{definition}
Taking the limit to infinity, the set $\sset{}{\lambda} \equiv \sset{\infty}{\lambda}$ contains all the sequences of infinite length that satisfy $\lambda$.
%
The notion of \emph{domination} between constraints then follows~\cite{Bernat:2001}.
\begin{definition}[Constraint domination]%
    \label{def:domination}%
    Constraint $\lambda_i$ \emph{dominates} $\lambda_j$ (formally, $\lambda_i \preceq \lambda_j$) if $\sset{}{\lambda_i} \subseteq \sset{}{\lambda_j}$. 
\end{definition}


\subsection{Control Tasks That May Miss Deadlines}
\label{sec:back_deadline_miss}
%While the considerations above apply to generic tasks,
When a control task $\tau$ is implemented on an embedded platform with limited computational power, alongside other applications, it is not uncommon for it to experience deadline misses, even in case of simple control designs (PID, LQG, etc)~\cite{akesson2020empirical,pazzaglia2021adaptive}.
In this context, even a simple controller may be subject to computational overruns~\cite{akesson2020empirical} caused by, e.g., bursts of high priority interrupts, cache misses, variable execution times of ancillary functions, such as data filtering or coordinate transformations (e.g., Park-Clarke transforms in electric motors), or other complex interactions.
If such events are rare or temporary, choosing a longer period for a sampled controller may result in worse performance and stability margins for nominal conditions~\cite{Pazzaglia:2019}.

Since the fundamental properties of stability and performance of a controller depend on the actual pattern of control commands, it is necessary to precisely define what happens when a control deadline is missed.
In particular, we need a \emph{deadline miss handling strategy} to decide the fate of the job that missed the deadline (and possibly the next ones), and an \emph{actuator mode} to deal with the loss of a new control signal, for example by \tH{}ing the previous value constant or \tZ{}ing it~\cite{schenato09}.
A few handling strategies for periodic controllers have been proposed in literature, the most interesting being \emph{\tK{}} and \emph{\tS{}}~\cite{Cervin:2005,Pazzaglia:2019,Maggio:2020}.

\begin{definition}[\tK{} strategy]%
    \label{def:kill}%
    Under the \tK{} strategy, a job that misses its deadline is terminated immediately. 
    Formally, for the~$i$-th job of $\tau$ either $f_i\leq d_i$ or $f_i=\infty$.
\end{definition}

\begin{definition}[\tS{} strategy]%
    \label{def:skip}%
    Under the \tS{} strategy, a job that misses its deadline is allowed to continue during the following period.
    Formally, if the $i$-th job of $\tau$ misses its deadline $d_i$, a new deadline $d^{+}_i = d_i + \Ts$ is set for the job, and $a_{i+1} = d^{+}_i$.
\end{definition}


\subsection{Stability Analysis Techniques Based on JSR}
\label{sec:existing}
A control task that may experience deadline misses requires a proper analysis to check its asymptotic stability under different timing conditions. 
In~\cite{Maggio:2020}, the authors identify a set of subsequences of hit and missed deadlines, which can be arbitrarily combined to obtain all possible sequences in $\sset{}{\rowmiss{}}$.
After assigning to each subsequence its corresponding dynamic matrix, the stability analysis of the resulting arbitrary switching system can then be obtained by leveraging the \emph{Joint Spectral Radius} (JSR)~\cite{Jungers2009}, which is briefly presented here.
%
Given $N\in\N_{>}$ and a set of matrices $\Aa = \{\clmat{}_1, \ldots, \clmat{}_N \} \subseteq \R^{n\times n}$, under the hypothesis of arbitrary switching over any sequence $w=\seq{c_1,c_2,\dots}$ of indices of matrices in $\Aa$, the JSR of $\Aa$ is defined by:
\begin{equation}
    \label{jsr}
    \rho\left(\Aa\right)= \lim_{\ell\rightarrow\infty} \max_{w\in\{1,\ldots,N\}^\ell} \norm{\clmat{}_{c_\ell} \cdots \clmat{}_{c_2}\clmat{}_{c_1}}^{1/\ell} \,.
\end{equation}
The number $\rho\left(\Aa\right)$ characterizes the maximal asymptotic growth rate of products of matrices from $\Aa$ (thus  $\rho(\Aa)<1$ means that the system is asymptotically stable), and is independent of the norm $\norm{\cdot}$ used in \eqref{jsr}.
%
The JSR was introduced in \cite{rota} and is applied in several important contexts, including switched dynamical systems stability and combinatorics~\cite{Jungers2009}.
Existing practical tools such as the \code{JSR toolbox}~\cite{vankeerberghen2014jsr} allow one to compute both upper and lower bounds on $\rho\left(\Aa\right)$.

When the switching sequences between the dynamics of $\Aa$ are not arbitrary, but constrained by a graph $\GG{}$, the so called \emph{constrained joint spectral radius} (CJSR)~\cite{dai2012gelfand} can be applied.
Considering $\sset{\ell}{\GG{}}$ as the set of all possible switching sequences $w$ of length $\ell$ that satisfy the constraints of a graph $\GG{}$, the CJSR of $\Aa$ is defined by
\begin{equation}
\label{cjsr}
    \rho\left(\Aa,\GG{}\right)= \lim_{\ell\rightarrow\infty} \max_{w\in\sset{\ell}{\GG{}}} \norm{\clmat{}_{c_\ell} \cdots \clmat{}_{c_2}\clmat{}_{c_1}}^{1/\ell} \,.
\end{equation}
In general, computing or approximating the CJSR is not an easy task, and noticeably fewer works exist for CJSR with respect to JSR.
In~\cite{philippe2016stability}, the authors propose a multinorm-based method to approximate the CJSR with arbitrary accuracy.
Other works~\cite{kozyakin2014berger, wang2014stability, xu2020approximation} propose the creation of an arbitrary switching system such that its JSR is equal to the CJSR of the original system, based on a Kronecker lifting method.
This will be also our approach, as detailed later.

In~\cite{parrilo}, the authors propose upper bounds for the JSR based on positive polynomials $p(x)$ which be can decomposed as \emph{sum of squares} (SOS) of polynomials.
%
It consists of considering a hierarchy of relaxations indexed by $d\in\N_{>}$, which in turn yields a sequence of upper bounds for $\rho\left(\Aa\right)$:
\begin{equation}
    \label{densesos}
    \begin{aligned}
        \rho_{\mathrm{SOS},2d}\funof{\Aa} =& \inf_{p\funof{x} \in \R_{2d}\left[x\right], \gamma}  \gamma \\
        \textrm{s.t.}\quad&
        \left\{\begin{matrix*}[l]
                p\funof{x} &\text{is SOS},\\
                \gamma^{2d} p\funof{x} - p\funof{\clmat_i x} &\text{is SOS},\; \clmat_i \in \Aa.
        \end{matrix*}\right.
    \end{aligned}
\end{equation}
%
The optimal value of this optimization problem can be computed by combining a bisection procedure on $\gamma$ together with \emph{semidefinite programming} (SDP)~\cite{wolkowicz2012handbook}.
Finding the coefficients of a polynomial being SOS boils down to solving an SDP~\cite{re2, parrilo2000structured, lasserre2001global}.
A typical SDP solver based on interior-point methods requires $O(n_{\text{sdp}}^3 + n_{\text{sdp}}^2 m_{\text{sdp}}^2 + m_{\text{sdp}}^3)$ arithmetic operations per iteration and $O(n_{\text{sdp}}^2 + n_{\text{sdp}} m_{\text{sdp}} + m_{\text{sdp}}^2)$ memory where $n_{\text{sdp}}$ is the size of PSD matrices and $m_{\text{sdp}}$ is the number of affine constraints.
Here, the set $\Aa$ involves $m$ matrices of size $n$.
For the SDP relaxation given in Equation \eqref{densesos}, with $d = 1$, one has $n_{\text{sdp}} = O(n)$ and $m_{\text{sdp}} = O(m n^2)$.
Thus, in total the SDP solver requires $O(m^3 n^6)$ arithmetic operations per iteration and $O(m^2 n^4)$ memory.

To reduce both the time and space complexity associated with this procedure, a tool, called \code{SparseJSR}~\cite{sparsejsr}, has been proposed to exploit the sparsity of the input matrices from $\Aa$, based on the \emph{term sparsity} SOS (TSSOS) framework~\cite{tssos}.
This allows the authors to provide SOS decompositions of polynomials with arbitrary sparse support.
By contrast, the procedure defined in~\eqref{densesos} will be denoted hereafter as \emph{dense}.

Let $\rho_{\textrm{TSSOS},2d}\left(\Aa\right)$ be the upper bound provided by \code{SparseJSR}.
From Theorem 4.1 of~\cite{sparsejsr}, one has $\rho\left(\Aa\right) \leq \rho_{\textrm{SOS},2d}\left(\Aa\right) \leq \rho_{\textrm{TSSOS},2d}\left(\Aa\right)$, yielding a hierarchy of upper bounds for the JSR $\rho\left(\Aa\right)$.
While providing a more conservative result, the sparse upper bound can be obtained significantly faster than the dense upper bound if the matrices from $\Aa$ are sparse~\cite{sparsejsr}, e.g., the matrices we analyse in Section~\ref{sec:stability}.
